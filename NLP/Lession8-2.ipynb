{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# <center>Python课程实践--深度学习</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课程内容\n",
    "\n",
    "* 1.循环神经网络RNN\n",
    "* 2.长短期记忆网络LSTM\n",
    "* 3.IMDB电影评论分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.循环神经网络RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 循环神经网络简介\n",
    "\n",
    "CNN等传统神经网络的局限在于：将固定大小的向量作为输入（比如一张图片），然后输出一个固定大小的向量（比如不同分类的概率）。不仅如此，CNN还按照固定的计算步骤（比如模型中层的数量）来实现这样的输入输出。这样的神经网络没有持久性：假设你希望对电影中每一帧的事件类型进行分类，传统的神经网络就没有办法使用电影中先前的事件推断后续的事件。\n",
    "\n",
    "RNN 解决了这个问题。RNN 是包含循环的网络，允许信息的持久化。在自然语言处理(NLP)领域，RNN已经可以做语音识别、机器翻译、生成手写字符，以及构建强大的语言模型 (Sutskever et al.)，(Graves)，(Mikolov et al.)（字符级别和单词级别的都有。在机器视觉领域，RNN也非常流行。包括帧级别的视频分类，图像描述，视频描述以及基于图像的Q&A等等。\n",
    "\n",
    "#### RNN结构\n",
    "\n",
    "神经网络的模块A正在读取某个输入xt，并输出一个值ht，循环可以使得信息从当前步传递到下一步，将这个循环展开，如下所示。链式的特征揭示了 RNN 本质上是与序列和列表相关的，它们是对于这类数据的最自然的神经网络架构。\n",
    "\n",
    "![image](images/2.png)\n",
    "\n",
    "#### 长期依赖问题\n",
    "\n",
    "RNN 的关键点之一就是他们可以用来连接先前的信息到当前的任务上，例如使用过去的视频段来推测对当前段的理解。如果 RNN 可以做到这个，他们就变得非常有用。但是真的可以么？答案是，还有很多依赖因素。有时候，我们仅仅需要知道先前的信息来执行当前的任务。例如，我们有一个语言模型用来基于先前的词来预测下一个词。如果我们试着预测 “the clouds are in the sky” 最后的词，我们并不需要任何其他的上下文 —— 因此下一个词很显然就应该是 sky。在这样的场景中，相关的信息和预测的词位置之间的间隔是非常小的，RNN 可以学会使用先前的信息。\n",
    "\n",
    "![image](images/3.png)\n",
    "\n",
    "但是同样会有一些更加复杂的场景。假设我们试着去预测“I grew up in France... I speak fluent French”最后的词。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的 France 的上下文的。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。不幸的是，在这个间隔不断增大时，RNN 会丧失学习到连接如此远的信息的能力。\n",
    "\n",
    "![image](images/4.png)\n",
    "\n",
    "在理论上，RNN 绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN 肯定不能够成功学习到这些知识。Bengio, et al. (1994)等人对该问题进行了深入的研究，他们发现一些使训练 RNN 变得非常困难的相当根本的原因。然而，幸运的是，LSTM 并没有这个问题！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 理解循环神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#只返回最后一个时间步的输出\n",
    "\n",
    "#构建一个序列的网络结构\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32)) #添加一层字嵌入式\n",
    "model.add(SimpleRNN(32)) #增加RNN\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, None, 32)          2080      \n",
      "=================================================================\n",
      "Total params: 34,080\n",
      "Trainable params: 34,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#返回完整的状态序列\n",
    "model =Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 32)          32000     \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_4 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, None, 32)          2080      \n",
      "_________________________________________________________________\n",
      "simple_rnn_6 (SimpleRNN)     (None, 32)                2080      \n",
      "=================================================================\n",
      "Total params: 40,320\n",
      "Trainable params: 40,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#将多个循环层逐个堆叠---需要所有中间层都返回完整的输出序列\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 在IMDB数据上应用SimpleRNN\n",
    "\n",
    "IMDB数据集包含来自互联网的50000条严重两极分化的评论，该数据被分为用于训练的25000条评论和用于测试的25000条评论，训练集和测试集都包含50%的正面评价和50%的负面评价。该数据集已经经过预处理：评论（单词序列）已经被转换为整数序列，其中每个整数代表字典中的某个单词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 17s 1us/step\n",
      "25000 train_sequences\n",
      "25000 test_sequences\n",
      "Pad sequences (samples x time)\n",
      "input_train.shape:  (25000, 500)\n",
      "input_test.shape:  (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 1000\n",
    "maxlen = 500\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(input_train), 'train_sequences')\n",
    "print(len(input_test), 'test_sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
    "print('input_train.shape: ',input_train.shape )\n",
    "print('input_test.shape: ', input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train\n",
    "y_train\n",
    "#数据集已经对单词进行了编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 19s 939us/step - loss: 0.6366 - acc: 0.6236 - val_loss: 0.5501 - val_acc: 0.7384\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 20s 995us/step - loss: 0.4727 - acc: 0.7836 - val_loss: 0.4131 - val_acc: 0.8232\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 20s 1ms/step - loss: 0.4276 - acc: 0.8108 - val_loss: 0.4322 - val_acc: 0.8038\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 21s 1ms/step - loss: 0.3847 - acc: 0.8374 - val_loss: 0.4426 - val_acc: 0.8174\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 20s 993us/step - loss: 0.3532 - acc: 0.8512 - val_loss: 0.4667 - val_acc: 0.8078\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense   #Dense就是常用的全连接层\n",
    "\n",
    "#构造序列的网络结构\n",
    "model = Sequential()\n",
    "\n",
    "#Embedding层只能作为模型的第一层\n",
    "#input_dim：大或等于0的整数，字典长度，即输入数据最大下标+1\n",
    "#output_dim：大于0的整数，代表全连接嵌入的维度\n",
    "model.add(Embedding(max_features, 32))\n",
    "\n",
    "model.add(SimpleRNN(32)) #必须也是32位\n",
    "model.add(Dense(1, activation='sigmoid')) #最后网络输出为 1个 变量，  用于分类  Sigmoid函数常被用作神经网络的阈值函数，将变量映射到0,1之间\n",
    "\n",
    "#编译模型\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "#训练模型\n",
    "history = model.fit(input_train, y_train,\n",
    "                   epochs=5,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#绘制训练的学习曲线\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 17s 690us/step\n",
      "[0.4595277501296997, 0.8057600259780884]\n"
     ]
    }
   ],
   "source": [
    "#验证模型\n",
    "#Returns the loss value & metrics values for the model in test mode\n",
    "\n",
    "results = model.evaluate(input_test, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 长短期记忆网络LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 长短期记忆网络简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Long Short Term 网络—— 一般就叫做 LSTM，是一种 RNN 特殊的类型，可以学习长期依赖信息。LSTM 由Hochreiter & Schmidhuber (1997)提出，并在近期被Alex Graves进行了改良和推广。在很多问题，LSTM 都取得相当巨大的成功，并得到了广泛的使用。LSTM 通过刻意的设计来避免长期依赖问题。记住长期的信息在实践中是 LSTM 的默认行为，而非需要付出很大代价才能获得的能力！\n",
    "\n",
    "所有 RNN 都具有一种重复神经网络模块的链式的形式。在标准的 RNN 中，这个重复的模块只有一个非常简单的结构，例如一个 tanh 层。\n",
    "\n",
    "![image](images/5.png)\n",
    "\n",
    "LSTM 同样是这样的结构，但是重复的模块拥有一个不同的结构。不同于 单一神经网络层，这里是有四个，以一种非常特殊的方式进行交互。\n",
    "\n",
    "![image](images/6.png)\n",
    "\n",
    "#### LSTM网络详解\n",
    "\n",
    "下面对LSTM网络进行详细说明，首先说明一下图中使用的图标，如下：\n",
    "\n",
    "![image](images/7.png)\n",
    "\n",
    "在上面的图例中，每一条黑线传输着一整个向量，从一个节点的输出到其他节点的输入。粉色的圈代表按位 pointwise 的操作，诸如向量的和，而黄色的矩阵就是学习到的神经网络层。合在一起的线表示向量的连接，分开的线表示内容被复制，然后分发到不同的位置。\n",
    "\n",
    "LSTM 的关键就是细胞状态cell state，水平线在图上方贯穿运行，也就是贯穿每个重复结构的上面这条flow。细胞状态类似于传送带，直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。这条flow其实就承载着之前所有状态的信息，每当flow流经一个重复结构A的时候，都会有相应的操作来决定舍弃什么旧的信息以及添加什么新的信息。\n",
    "\n",
    "![image](images/8.png)\n",
    "\n",
    "LSTM 有通过精心设计对信息增减进行控制的结构，称作为“门”。门是一种让信息选择式通过的方法。他们包含一个 sigmoid 神经网络层和一个按位的乘法操作。Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。0 代表“不许任何量通过”，1 就指“允许任意量通过”！\n",
    "\n",
    "![image](images/9.png)\n",
    "\n",
    "LSTM 拥有三个门，来保护和控制细胞状态，分别是遗忘门 (forget gate)、输入门 (input gate)、输出门 (output gate)。下面对这三个门进行详细讲解\n",
    "\n",
    "#### 遗忘门 (forget gate)\n",
    "\n",
    "遗忘门决定了要从cell state中舍弃什么信息。其通过输入上一状态的输出ht-1、当前状态输入信息Xt到一个Sigmoid函数中，产生一个介于0到1之间的数值，与cell state相乘之后来确定舍弃（保留）多少信息。0 表示“完全舍弃”，1 表示“完全保留”。\n",
    "\n",
    "![image](images/10.png)\n",
    "\n",
    "上式中，<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;W_{f}\" title=\"W_{f}\" />可以写为：<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;[W_{f}]\\begin{bmatrix}&space;h_{t-1}\\\\&space;x_{t}&space;\\end{bmatrix}=\\begin{bmatrix}&space;W_{fh}&space;&&space;W_{fx}&space;\\end{bmatrix}\\begin{bmatrix}&space;h_{t-1}\\\\&space;x_{t}&space;\\end{bmatrix}=W_{fh}h_{t-1}&plus;W_{fx}x_{t}\" title=\"[W_{f}]\\begin{bmatrix} h_{t-1}\\\\ x_{t} \\end{bmatrix}=\\begin{bmatrix} W_{fh} & W_{fx} \\end{bmatrix}\\begin{bmatrix} h_{t-1}\\\\ x_{t} \\end{bmatrix}=W_{fh}h_{t-1}+W_{fx}x_{t}\" />\n",
    "\n",
    "\n",
    "#### 输入门 (input gate)\n",
    "\n",
    "输入门决定了要往cell state中保存什么新的信息。其通过输入上一状态的输出、当前状态输入信息到一个Sigmoid函数中，产生一个介于0到1之间的数值来确定我们需要保留多少的新信息。同时，一个tanh层会通过上一状态的输出、当前状态输入信息来得到一个将要加入到cell state中的“候选新信息”。\n",
    "\n",
    "![image](images/11.png)\n",
    "\n",
    "现在计算当前时刻的单元状态。它是由上一次的单元状态按元素乘以遗忘门，丢弃掉我们确定需要丢弃的信息；然后把当前输入的单元状态按元素乘以输入门，将两个积加和，这就是新的候选值：\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/svg.latex?\\dpi{300}&space;C_{t}=f_{t}*C_{t-1}&plus;i_{t}*\\widetilde{C_{t}}\" title=\"C_{t}=f_{t}*C_{t-1}+i_{t}*\\widetilde{C_{t}}\" />\n",
    "\n",
    "![image](images/12.png)\n",
    "\n",
    "#### 输出门 (output gate)\n",
    "\n",
    "输出门决定了要从cell state中输出什么信息。这个输出将会基于我们的细胞状态，但是也是一个过滤后的版本，会先有一个Sigmoid函数产生一个介于0到1之间的数值来确定我们需要输出多少cell state中的信息。cell state的信息再与相乘时首先会经过一个tanh层进行“激活”（非线性变换）。得到的就是这个LSTM block的输出信息。\n",
    "\n",
    "![image](images/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 理解长短期记忆网络\n",
    "\n",
    "循环神经网络(RNN)的变种是长短期模型网络(LSTM)。它使用广泛，因为它的架构克服了困扰着所有周期性的神经网络梯度消失和梯度爆炸的问题，允许创建非常大的、非常深的网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习字母表\n",
    "\n",
    "学习字母表是一个简单的序列预测问题。也就是说，根据字母表的字母，可以预测字母表的下一个字母。\n",
    "\n",
    "这是一个简单的序列预测问题，一旦被理解，就可以被推广到其他的序列预测问题，如时间序列预测和序列分类。\n",
    "\n",
    "![](images/LSTM.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 单字符——单字符的映射的简单LSTM\n",
    "学习如何根据一个字符的上下文来预测字母表中的下一个字符。\n",
    "\n",
    "定义一个LSTM网络，它有32个单元，一个输出层，其中有一个softmax的激活函数来进行预测。由于这是一个多类分类问题，所以我们可以使用在Keras中使用对数损失函数(称为“分类交叉熵”(categorical_crossentropy))，并使用ADAM优化函数对网络进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n"
     ]
    }
   ],
   "source": [
    "# 对随机数生成器选定随机数种子，以确保每次执行代码时结果都是相同的。\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "# 为了便于阅读，我们用大写字母来定义字母表。\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "# 神经网络是对数字建模，因此我们需要将字母表中的字母映射到整数值（把字母映射为数字）。我们可以很容易地通过创建字母索引的字典(map)到字符。\n",
    "# 我们还可以创建一个反向查找，以便将预测转换回字符，以便稍后使用。\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# 造这样一个数据集，用一个字母，来预测下一个字母是什么\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "# 需要将NumPy数组重新构造为LSTM网络所期望的格式，即[samples示例, time steps时间步数, features特征]。\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0]],\n",
       "\n",
       "       [[ 1]],\n",
       "\n",
       "       [[ 2]],\n",
       "\n",
       "       [[ 3]],\n",
       "\n",
       "       [[ 4]],\n",
       "\n",
       "       [[ 5]],\n",
       "\n",
       "       [[ 6]],\n",
       "\n",
       "       [[ 7]],\n",
       "\n",
       "       [[ 8]],\n",
       "\n",
       "       [[ 9]],\n",
       "\n",
       "       [[10]],\n",
       "\n",
       "       [[11]],\n",
       "\n",
       "       [[12]],\n",
       "\n",
       "       [[13]],\n",
       "\n",
       "       [[14]],\n",
       "\n",
       "       [[15]],\n",
       "\n",
       "       [[16]],\n",
       "\n",
       "       [[17]],\n",
       "\n",
       "       [[18]],\n",
       "\n",
       "       [[19]],\n",
       "\n",
       "       [[20]],\n",
       "\n",
       "       [[21]],\n",
       "\n",
       "       [[22]],\n",
       "\n",
       "       [[23]],\n",
       "\n",
       "       [[24]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 1, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "# 需要把我们的整数值归一化到0～1的区间上，这是LSTM网络使用的s形激活函数（sigmoid）的范围\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "# 把这个问题看作是一个序列分类任务，其中26个字母代表一个不同的类。\n",
    "# 用keras的内置的 to_categorical()函数把输出output(y)进行 one－hot编码(one-hot指n维单位向量a=(0,…,0,1,0,…,0))作为输出层的结果。\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 0s - loss: 3.2661 - accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2582 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2551 - accuracy: 0.0400\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2524 - accuracy: 0.0400\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2495 - accuracy: 0.0400\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2470 - accuracy: 0.0400\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2438 - accuracy: 0.0400\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2410 - accuracy: 0.0400\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.2376 - accuracy: 0.0400\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.2345 - accuracy: 0.0400\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.2308 - accuracy: 0.0400\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.2271 - accuracy: 0.0400\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.2230 - accuracy: 0.0400\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.2195 - accuracy: 0.0400\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.2150 - accuracy: 0.0400\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.2104 - accuracy: 0.0400\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.2050 - accuracy: 0.0400\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.1998 - accuracy: 0.0400\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.1947 - accuracy: 0.0400\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.1885 - accuracy: 0.0400\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.1825 - accuracy: 0.0400\n",
      "Epoch 22/500\n",
      " - 0s - loss: 3.1755 - accuracy: 0.0400\n",
      "Epoch 23/500\n",
      " - 0s - loss: 3.1686 - accuracy: 0.0400\n",
      "Epoch 24/500\n",
      " - 0s - loss: 3.1621 - accuracy: 0.0400\n",
      "Epoch 25/500\n",
      " - 0s - loss: 3.1544 - accuracy: 0.0400\n",
      "Epoch 26/500\n",
      " - 0s - loss: 3.1462 - accuracy: 0.0400\n",
      "Epoch 27/500\n",
      " - 0s - loss: 3.1398 - accuracy: 0.0400\n",
      "Epoch 28/500\n",
      " - 0s - loss: 3.1301 - accuracy: 0.0800\n",
      "Epoch 29/500\n",
      " - 0s - loss: 3.1215 - accuracy: 0.1200\n",
      "Epoch 30/500\n",
      " - 0s - loss: 3.1125 - accuracy: 0.0800\n",
      "Epoch 31/500\n",
      " - 0s - loss: 3.1037 - accuracy: 0.1200\n",
      "Epoch 32/500\n",
      " - 0s - loss: 3.0929 - accuracy: 0.0800\n",
      "Epoch 33/500\n",
      " - 0s - loss: 3.0832 - accuracy: 0.0800\n",
      "Epoch 34/500\n",
      " - 0s - loss: 3.0733 - accuracy: 0.0400\n",
      "Epoch 35/500\n",
      " - 0s - loss: 3.0621 - accuracy: 0.0400\n",
      "Epoch 36/500\n",
      " - 0s - loss: 3.0510 - accuracy: 0.0800\n",
      "Epoch 37/500\n",
      " - 0s - loss: 3.0394 - accuracy: 0.0800\n",
      "Epoch 38/500\n",
      " - 0s - loss: 3.0285 - accuracy: 0.0400\n",
      "Epoch 39/500\n",
      " - 0s - loss: 3.0176 - accuracy: 0.0800\n",
      "Epoch 40/500\n",
      " - 0s - loss: 3.0055 - accuracy: 0.0800\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.9923 - accuracy: 0.0400\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.9788 - accuracy: 0.0800\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.9658 - accuracy: 0.0400\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.9532 - accuracy: 0.0400\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.9390 - accuracy: 0.0400\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.9262 - accuracy: 0.0400\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.9137 - accuracy: 0.0800\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.8998 - accuracy: 0.0800\n",
      "Epoch 49/500\n",
      " - 0s - loss: 2.8868 - accuracy: 0.0800\n",
      "Epoch 50/500\n",
      " - 0s - loss: 2.8740 - accuracy: 0.0800\n",
      "Epoch 51/500\n",
      " - 0s - loss: 2.8614 - accuracy: 0.0800\n",
      "Epoch 52/500\n",
      " - 0s - loss: 2.8486 - accuracy: 0.0800\n",
      "Epoch 53/500\n",
      " - 0s - loss: 2.8366 - accuracy: 0.0800\n",
      "Epoch 54/500\n",
      " - 0s - loss: 2.8240 - accuracy: 0.0400\n",
      "Epoch 55/500\n",
      " - 0s - loss: 2.8112 - accuracy: 0.0800\n",
      "Epoch 56/500\n",
      " - 0s - loss: 2.8017 - accuracy: 0.0400\n",
      "Epoch 57/500\n",
      " - 0s - loss: 2.7897 - accuracy: 0.0800\n",
      "Epoch 58/500\n",
      " - 0s - loss: 2.7786 - accuracy: 0.0800\n",
      "Epoch 59/500\n",
      " - 0s - loss: 2.7672 - accuracy: 0.0800\n",
      "Epoch 60/500\n",
      " - 0s - loss: 2.7576 - accuracy: 0.0800\n",
      "Epoch 61/500\n",
      " - 0s - loss: 2.7474 - accuracy: 0.0800\n",
      "Epoch 62/500\n",
      " - 0s - loss: 2.7376 - accuracy: 0.0800\n",
      "Epoch 63/500\n",
      " - 0s - loss: 2.7287 - accuracy: 0.0800\n",
      "Epoch 64/500\n",
      " - 0s - loss: 2.7193 - accuracy: 0.0800\n",
      "Epoch 65/500\n",
      " - 0s - loss: 2.7100 - accuracy: 0.0800\n",
      "Epoch 66/500\n",
      " - 0s - loss: 2.7023 - accuracy: 0.0800\n",
      "Epoch 67/500\n",
      " - 0s - loss: 2.6938 - accuracy: 0.0800\n",
      "Epoch 68/500\n",
      " - 0s - loss: 2.6857 - accuracy: 0.1200\n",
      "Epoch 69/500\n",
      " - 0s - loss: 2.6780 - accuracy: 0.1600\n",
      "Epoch 70/500\n",
      " - 0s - loss: 2.6700 - accuracy: 0.1200\n",
      "Epoch 71/500\n",
      " - 0s - loss: 2.6632 - accuracy: 0.0800\n",
      "Epoch 72/500\n",
      " - 0s - loss: 2.6557 - accuracy: 0.1200\n",
      "Epoch 73/500\n",
      " - 0s - loss: 2.6494 - accuracy: 0.1200\n",
      "Epoch 74/500\n",
      " - 0s - loss: 2.6432 - accuracy: 0.1200\n",
      "Epoch 75/500\n",
      " - 0s - loss: 2.6363 - accuracy: 0.1200\n",
      "Epoch 76/500\n",
      " - 0s - loss: 2.6309 - accuracy: 0.1200\n",
      "Epoch 77/500\n",
      " - 0s - loss: 2.6239 - accuracy: 0.1200\n",
      "Epoch 78/500\n",
      " - 0s - loss: 2.6167 - accuracy: 0.0800\n",
      "Epoch 79/500\n",
      " - 0s - loss: 2.6107 - accuracy: 0.1200\n",
      "Epoch 80/500\n",
      " - 0s - loss: 2.6052 - accuracy: 0.1200\n",
      "Epoch 81/500\n",
      " - 0s - loss: 2.5993 - accuracy: 0.1200\n",
      "Epoch 82/500\n",
      " - 0s - loss: 2.5933 - accuracy: 0.0800\n",
      "Epoch 83/500\n",
      " - 0s - loss: 2.5888 - accuracy: 0.1200\n",
      "Epoch 84/500\n",
      " - 0s - loss: 2.5828 - accuracy: 0.1200\n",
      "Epoch 85/500\n",
      " - 0s - loss: 2.5777 - accuracy: 0.1200\n",
      "Epoch 86/500\n",
      " - 0s - loss: 2.5733 - accuracy: 0.1200\n",
      "Epoch 87/500\n",
      " - 0s - loss: 2.5671 - accuracy: 0.1200\n",
      "Epoch 88/500\n",
      " - 0s - loss: 2.5608 - accuracy: 0.1200\n",
      "Epoch 89/500\n",
      " - 0s - loss: 2.5550 - accuracy: 0.1200\n",
      "Epoch 90/500\n",
      " - 0s - loss: 2.5513 - accuracy: 0.1200\n",
      "Epoch 91/500\n",
      " - 0s - loss: 2.5467 - accuracy: 0.1200\n",
      "Epoch 92/500\n",
      " - 0s - loss: 2.5415 - accuracy: 0.1200\n",
      "Epoch 93/500\n",
      " - 0s - loss: 2.5360 - accuracy: 0.1200\n",
      "Epoch 94/500\n",
      " - 0s - loss: 2.5313 - accuracy: 0.1600\n",
      "Epoch 95/500\n",
      " - 0s - loss: 2.5263 - accuracy: 0.1600\n",
      "Epoch 96/500\n",
      " - 0s - loss: 2.5210 - accuracy: 0.1600\n",
      "Epoch 97/500\n",
      " - 0s - loss: 2.5162 - accuracy: 0.1600\n",
      "Epoch 98/500\n",
      " - 0s - loss: 2.5109 - accuracy: 0.1600\n",
      "Epoch 99/500\n",
      " - 0s - loss: 2.5055 - accuracy: 0.1600\n",
      "Epoch 100/500\n",
      " - 0s - loss: 2.5018 - accuracy: 0.1200\n",
      "Epoch 101/500\n",
      " - 0s - loss: 2.4955 - accuracy: 0.1200\n",
      "Epoch 102/500\n",
      " - 0s - loss: 2.4915 - accuracy: 0.1600\n",
      "Epoch 103/500\n",
      " - 0s - loss: 2.4868 - accuracy: 0.1600\n",
      "Epoch 104/500\n",
      " - 0s - loss: 2.4817 - accuracy: 0.1600\n",
      "Epoch 105/500\n",
      " - 0s - loss: 2.4771 - accuracy: 0.1600\n",
      "Epoch 106/500\n",
      " - 0s - loss: 2.4727 - accuracy: 0.1600\n",
      "Epoch 107/500\n",
      " - 0s - loss: 2.4679 - accuracy: 0.2000\n",
      "Epoch 108/500\n",
      " - 0s - loss: 2.4620 - accuracy: 0.1600\n",
      "Epoch 109/500\n",
      " - 0s - loss: 2.4577 - accuracy: 0.2000\n",
      "Epoch 110/500\n",
      " - 0s - loss: 2.4519 - accuracy: 0.1600\n",
      "Epoch 111/500\n",
      " - 0s - loss: 2.4494 - accuracy: 0.2000\n",
      "Epoch 112/500\n",
      " - 0s - loss: 2.4443 - accuracy: 0.2000\n",
      "Epoch 113/500\n",
      " - 0s - loss: 2.4394 - accuracy: 0.2000\n",
      "Epoch 114/500\n",
      " - 0s - loss: 2.4337 - accuracy: 0.2400\n",
      "Epoch 115/500\n",
      " - 0s - loss: 2.4290 - accuracy: 0.2000\n",
      "Epoch 116/500\n",
      " - 0s - loss: 2.4251 - accuracy: 0.2000\n",
      "Epoch 117/500\n",
      " - 0s - loss: 2.4222 - accuracy: 0.2400\n",
      "Epoch 118/500\n",
      " - 0s - loss: 2.4154 - accuracy: 0.2400\n",
      "Epoch 119/500\n",
      " - 0s - loss: 2.4123 - accuracy: 0.2000\n",
      "Epoch 120/500\n",
      " - 0s - loss: 2.4080 - accuracy: 0.2400\n",
      "Epoch 121/500\n",
      " - 0s - loss: 2.4039 - accuracy: 0.2000\n",
      "Epoch 122/500\n",
      " - 0s - loss: 2.3990 - accuracy: 0.2000\n",
      "Epoch 123/500\n",
      " - 0s - loss: 2.3950 - accuracy: 0.2400\n",
      "Epoch 124/500\n",
      " - 0s - loss: 2.3895 - accuracy: 0.2400\n",
      "Epoch 125/500\n",
      " - 0s - loss: 2.3876 - accuracy: 0.2400\n",
      "Epoch 126/500\n",
      " - 0s - loss: 2.3820 - accuracy: 0.2400\n",
      "Epoch 127/500\n",
      " - 0s - loss: 2.3773 - accuracy: 0.2000\n",
      "Epoch 128/500\n",
      " - 0s - loss: 2.3751 - accuracy: 0.2400\n",
      "Epoch 129/500\n",
      " - 0s - loss: 2.3697 - accuracy: 0.2400\n",
      "Epoch 130/500\n",
      " - 0s - loss: 2.3659 - accuracy: 0.2400\n",
      "Epoch 131/500\n",
      " - 0s - loss: 2.3626 - accuracy: 0.2000\n",
      "Epoch 132/500\n",
      " - 0s - loss: 2.3577 - accuracy: 0.2400\n",
      "Epoch 133/500\n",
      " - 0s - loss: 2.3540 - accuracy: 0.2800\n",
      "Epoch 134/500\n",
      " - 0s - loss: 2.3501 - accuracy: 0.2000\n",
      "Epoch 135/500\n",
      " - 0s - loss: 2.3452 - accuracy: 0.2000\n",
      "Epoch 136/500\n",
      " - 0s - loss: 2.3429 - accuracy: 0.1600\n",
      "Epoch 137/500\n",
      " - 0s - loss: 2.3381 - accuracy: 0.2000\n",
      "Epoch 138/500\n",
      " - 0s - loss: 2.3352 - accuracy: 0.1600\n",
      "Epoch 139/500\n",
      " - 0s - loss: 2.3304 - accuracy: 0.2400\n",
      "Epoch 140/500\n",
      " - 0s - loss: 2.3273 - accuracy: 0.2400\n",
      "Epoch 141/500\n",
      " - 0s - loss: 2.3235 - accuracy: 0.2000\n",
      "Epoch 142/500\n",
      " - 0s - loss: 2.3196 - accuracy: 0.2000\n",
      "Epoch 143/500\n",
      " - 0s - loss: 2.3152 - accuracy: 0.2000\n",
      "Epoch 144/500\n",
      " - 0s - loss: 2.3128 - accuracy: 0.2000\n",
      "Epoch 145/500\n",
      " - 0s - loss: 2.3087 - accuracy: 0.2000\n",
      "Epoch 146/500\n",
      " - 0s - loss: 2.3070 - accuracy: 0.2400\n",
      "Epoch 147/500\n",
      " - 0s - loss: 2.3022 - accuracy: 0.2000\n",
      "Epoch 148/500\n",
      " - 0s - loss: 2.2973 - accuracy: 0.2000\n",
      "Epoch 149/500\n",
      " - 0s - loss: 2.2960 - accuracy: 0.2400\n",
      "Epoch 150/500\n",
      " - 0s - loss: 2.2911 - accuracy: 0.1600\n",
      "Epoch 151/500\n",
      " - 0s - loss: 2.2885 - accuracy: 0.1600\n",
      "Epoch 152/500\n",
      " - 0s - loss: 2.2857 - accuracy: 0.1600\n",
      "Epoch 153/500\n",
      " - 0s - loss: 2.2811 - accuracy: 0.2000\n",
      "Epoch 154/500\n",
      " - 0s - loss: 2.2783 - accuracy: 0.2800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      " - 0s - loss: 2.2736 - accuracy: 0.2400\n",
      "Epoch 156/500\n",
      " - 0s - loss: 2.2729 - accuracy: 0.2000\n",
      "Epoch 157/500\n",
      " - 0s - loss: 2.2681 - accuracy: 0.2400\n",
      "Epoch 158/500\n",
      " - 0s - loss: 2.2640 - accuracy: 0.2400\n",
      "Epoch 159/500\n",
      " - 0s - loss: 2.2635 - accuracy: 0.2800\n",
      "Epoch 160/500\n",
      " - 0s - loss: 2.2596 - accuracy: 0.2400\n",
      "Epoch 161/500\n",
      " - 0s - loss: 2.2551 - accuracy: 0.2400\n",
      "Epoch 162/500\n",
      " - 0s - loss: 2.2534 - accuracy: 0.2400\n",
      "Epoch 163/500\n",
      " - 0s - loss: 2.2498 - accuracy: 0.3200\n",
      "Epoch 164/500\n",
      " - 0s - loss: 2.2469 - accuracy: 0.2400\n",
      "Epoch 165/500\n",
      " - 0s - loss: 2.2428 - accuracy: 0.2400\n",
      "Epoch 166/500\n",
      " - 0s - loss: 2.2394 - accuracy: 0.2400\n",
      "Epoch 167/500\n",
      " - 0s - loss: 2.2369 - accuracy: 0.2400\n",
      "Epoch 168/500\n",
      " - 0s - loss: 2.2337 - accuracy: 0.2000\n",
      "Epoch 169/500\n",
      " - 0s - loss: 2.2320 - accuracy: 0.2400\n",
      "Epoch 170/500\n",
      " - 0s - loss: 2.2282 - accuracy: 0.2400\n",
      "Epoch 171/500\n",
      " - 0s - loss: 2.2261 - accuracy: 0.2800\n",
      "Epoch 172/500\n",
      " - 0s - loss: 2.2220 - accuracy: 0.3200\n",
      "Epoch 173/500\n",
      " - 0s - loss: 2.2195 - accuracy: 0.3600\n",
      "Epoch 174/500\n",
      " - 0s - loss: 2.2161 - accuracy: 0.2800\n",
      "Epoch 175/500\n",
      " - 0s - loss: 2.2138 - accuracy: 0.2400\n",
      "Epoch 176/500\n",
      " - 0s - loss: 2.2101 - accuracy: 0.2800\n",
      "Epoch 177/500\n",
      " - 0s - loss: 2.2073 - accuracy: 0.2400\n",
      "Epoch 178/500\n",
      " - 0s - loss: 2.2045 - accuracy: 0.3200\n",
      "Epoch 179/500\n",
      " - 0s - loss: 2.2017 - accuracy: 0.3200\n",
      "Epoch 180/500\n",
      " - 0s - loss: 2.1992 - accuracy: 0.2000\n",
      "Epoch 181/500\n",
      " - 0s - loss: 2.1953 - accuracy: 0.2800\n",
      "Epoch 182/500\n",
      " - 0s - loss: 2.1941 - accuracy: 0.3200\n",
      "Epoch 183/500\n",
      " - 0s - loss: 2.1912 - accuracy: 0.3600\n",
      "Epoch 184/500\n",
      " - 0s - loss: 2.1877 - accuracy: 0.3200\n",
      "Epoch 185/500\n",
      " - 0s - loss: 2.1840 - accuracy: 0.2800\n",
      "Epoch 186/500\n",
      " - 0s - loss: 2.1826 - accuracy: 0.3200\n",
      "Epoch 187/500\n",
      " - 0s - loss: 2.1799 - accuracy: 0.2800\n",
      "Epoch 188/500\n",
      " - 0s - loss: 2.1758 - accuracy: 0.3600\n",
      "Epoch 189/500\n",
      " - 0s - loss: 2.1742 - accuracy: 0.3200\n",
      "Epoch 190/500\n",
      " - 0s - loss: 2.1722 - accuracy: 0.3600\n",
      "Epoch 191/500\n",
      " - 0s - loss: 2.1691 - accuracy: 0.2800\n",
      "Epoch 192/500\n",
      " - 0s - loss: 2.1677 - accuracy: 0.3600\n",
      "Epoch 193/500\n",
      " - 0s - loss: 2.1642 - accuracy: 0.4000\n",
      "Epoch 194/500\n",
      " - 0s - loss: 2.1609 - accuracy: 0.3200\n",
      "Epoch 195/500\n",
      " - 0s - loss: 2.1580 - accuracy: 0.3600\n",
      "Epoch 196/500\n",
      " - 0s - loss: 2.1567 - accuracy: 0.3200\n",
      "Epoch 197/500\n",
      " - 0s - loss: 2.1550 - accuracy: 0.3200\n",
      "Epoch 198/500\n",
      " - 0s - loss: 2.1510 - accuracy: 0.2800\n",
      "Epoch 199/500\n",
      " - 0s - loss: 2.1482 - accuracy: 0.3200\n",
      "Epoch 200/500\n",
      " - 0s - loss: 2.1447 - accuracy: 0.3200\n",
      "Epoch 201/500\n",
      " - 0s - loss: 2.1434 - accuracy: 0.2800\n",
      "Epoch 202/500\n",
      " - 0s - loss: 2.1410 - accuracy: 0.2800\n",
      "Epoch 203/500\n",
      " - 0s - loss: 2.1380 - accuracy: 0.3200\n",
      "Epoch 204/500\n",
      " - 0s - loss: 2.1367 - accuracy: 0.3200\n",
      "Epoch 205/500\n",
      " - 0s - loss: 2.1341 - accuracy: 0.3600\n",
      "Epoch 206/500\n",
      " - 0s - loss: 2.1315 - accuracy: 0.3200\n",
      "Epoch 207/500\n",
      " - 0s - loss: 2.1276 - accuracy: 0.3600\n",
      "Epoch 208/500\n",
      " - 0s - loss: 2.1254 - accuracy: 0.2800\n",
      "Epoch 209/500\n",
      " - 0s - loss: 2.1244 - accuracy: 0.3600\n",
      "Epoch 210/500\n",
      " - 0s - loss: 2.1216 - accuracy: 0.3600\n",
      "Epoch 211/500\n",
      " - 0s - loss: 2.1187 - accuracy: 0.3200\n",
      "Epoch 212/500\n",
      " - 0s - loss: 2.1158 - accuracy: 0.3200\n",
      "Epoch 213/500\n",
      " - 0s - loss: 2.1155 - accuracy: 0.3200\n",
      "Epoch 214/500\n",
      " - 0s - loss: 2.1124 - accuracy: 0.3200\n",
      "Epoch 215/500\n",
      " - 0s - loss: 2.1092 - accuracy: 0.3200\n",
      "Epoch 216/500\n",
      " - 0s - loss: 2.1076 - accuracy: 0.4000\n",
      "Epoch 217/500\n",
      " - 0s - loss: 2.1060 - accuracy: 0.4000\n",
      "Epoch 218/500\n",
      " - 0s - loss: 2.1030 - accuracy: 0.4000\n",
      "Epoch 219/500\n",
      " - 0s - loss: 2.1005 - accuracy: 0.3200\n",
      "Epoch 220/500\n",
      " - 0s - loss: 2.0975 - accuracy: 0.3600\n",
      "Epoch 221/500\n",
      " - 0s - loss: 2.0963 - accuracy: 0.3200\n",
      "Epoch 222/500\n",
      " - 0s - loss: 2.0943 - accuracy: 0.3600\n",
      "Epoch 223/500\n",
      " - 0s - loss: 2.0906 - accuracy: 0.2800\n",
      "Epoch 224/500\n",
      " - 0s - loss: 2.0877 - accuracy: 0.4000\n",
      "Epoch 225/500\n",
      " - 0s - loss: 2.0872 - accuracy: 0.4400\n",
      "Epoch 226/500\n",
      " - 0s - loss: 2.0870 - accuracy: 0.4400\n",
      "Epoch 227/500\n",
      " - 0s - loss: 2.0841 - accuracy: 0.3600\n",
      "Epoch 228/500\n",
      " - 0s - loss: 2.0777 - accuracy: 0.4000\n",
      "Epoch 229/500\n",
      " - 0s - loss: 2.0780 - accuracy: 0.4400\n",
      "Epoch 230/500\n",
      " - 0s - loss: 2.0757 - accuracy: 0.4400\n",
      "Epoch 231/500\n",
      " - 0s - loss: 2.0736 - accuracy: 0.4400\n",
      "Epoch 232/500\n",
      " - 0s - loss: 2.0712 - accuracy: 0.4400\n",
      "Epoch 233/500\n",
      " - 0s - loss: 2.0702 - accuracy: 0.4000\n",
      "Epoch 234/500\n",
      " - 0s - loss: 2.0672 - accuracy: 0.4800\n",
      "Epoch 235/500\n",
      " - 0s - loss: 2.0657 - accuracy: 0.4800\n",
      "Epoch 236/500\n",
      " - 0s - loss: 2.0643 - accuracy: 0.4800\n",
      "Epoch 237/500\n",
      " - 0s - loss: 2.0595 - accuracy: 0.4000\n",
      "Epoch 238/500\n",
      " - 0s - loss: 2.0574 - accuracy: 0.4400\n",
      "Epoch 239/500\n",
      " - 0s - loss: 2.0569 - accuracy: 0.4000\n",
      "Epoch 240/500\n",
      " - 0s - loss: 2.0538 - accuracy: 0.3600\n",
      "Epoch 241/500\n",
      " - 0s - loss: 2.0508 - accuracy: 0.4400\n",
      "Epoch 242/500\n",
      " - 0s - loss: 2.0509 - accuracy: 0.4000\n",
      "Epoch 243/500\n",
      " - 0s - loss: 2.0493 - accuracy: 0.4400\n",
      "Epoch 244/500\n",
      " - 0s - loss: 2.0451 - accuracy: 0.3600\n",
      "Epoch 245/500\n",
      " - 0s - loss: 2.0438 - accuracy: 0.4800\n",
      "Epoch 246/500\n",
      " - 0s - loss: 2.0430 - accuracy: 0.4400\n",
      "Epoch 247/500\n",
      " - 0s - loss: 2.0404 - accuracy: 0.4800\n",
      "Epoch 248/500\n",
      " - 0s - loss: 2.0389 - accuracy: 0.4800\n",
      "Epoch 249/500\n",
      " - 0s - loss: 2.0357 - accuracy: 0.3600\n",
      "Epoch 250/500\n",
      " - 0s - loss: 2.0329 - accuracy: 0.4800\n",
      "Epoch 251/500\n",
      " - 0s - loss: 2.0323 - accuracy: 0.4400\n",
      "Epoch 252/500\n",
      " - 0s - loss: 2.0313 - accuracy: 0.3600\n",
      "Epoch 253/500\n",
      " - 0s - loss: 2.0275 - accuracy: 0.4400\n",
      "Epoch 254/500\n",
      " - 0s - loss: 2.0262 - accuracy: 0.4800\n",
      "Epoch 255/500\n",
      " - 0s - loss: 2.0237 - accuracy: 0.4800\n",
      "Epoch 256/500\n",
      " - 0s - loss: 2.0216 - accuracy: 0.5200\n",
      "Epoch 257/500\n",
      " - 0s - loss: 2.0203 - accuracy: 0.5200\n",
      "Epoch 258/500\n",
      " - 0s - loss: 2.0172 - accuracy: 0.5600\n",
      "Epoch 259/500\n",
      " - 0s - loss: 2.0174 - accuracy: 0.4800\n",
      "Epoch 260/500\n",
      " - 0s - loss: 2.0138 - accuracy: 0.4000\n",
      "Epoch 261/500\n",
      " - 0s - loss: 2.0126 - accuracy: 0.5600\n",
      "Epoch 262/500\n",
      " - 0s - loss: 2.0112 - accuracy: 0.3200\n",
      "Epoch 263/500\n",
      " - 0s - loss: 2.0103 - accuracy: 0.5200\n",
      "Epoch 264/500\n",
      " - 0s - loss: 2.0080 - accuracy: 0.4400\n",
      "Epoch 265/500\n",
      " - 0s - loss: 2.0045 - accuracy: 0.5600\n",
      "Epoch 266/500\n",
      " - 0s - loss: 2.0033 - accuracy: 0.5200\n",
      "Epoch 267/500\n",
      " - 0s - loss: 2.0021 - accuracy: 0.4800\n",
      "Epoch 268/500\n",
      " - 0s - loss: 1.9978 - accuracy: 0.5600\n",
      "Epoch 269/500\n",
      " - 0s - loss: 1.9982 - accuracy: 0.5200\n",
      "Epoch 270/500\n",
      " - 0s - loss: 1.9951 - accuracy: 0.5200\n",
      "Epoch 271/500\n",
      " - 0s - loss: 1.9932 - accuracy: 0.5200\n",
      "Epoch 272/500\n",
      " - 0s - loss: 1.9904 - accuracy: 0.6000\n",
      "Epoch 273/500\n",
      " - 0s - loss: 1.9893 - accuracy: 0.5600\n",
      "Epoch 274/500\n",
      " - 0s - loss: 1.9879 - accuracy: 0.4400\n",
      "Epoch 275/500\n",
      " - 0s - loss: 1.9855 - accuracy: 0.4800\n",
      "Epoch 276/500\n",
      " - 0s - loss: 1.9827 - accuracy: 0.4800\n",
      "Epoch 277/500\n",
      " - 0s - loss: 1.9819 - accuracy: 0.5200\n",
      "Epoch 278/500\n",
      " - 0s - loss: 1.9821 - accuracy: 0.5200\n",
      "Epoch 279/500\n",
      " - 0s - loss: 1.9778 - accuracy: 0.5600\n",
      "Epoch 280/500\n",
      " - 0s - loss: 1.9774 - accuracy: 0.4000\n",
      "Epoch 281/500\n",
      " - 0s - loss: 1.9767 - accuracy: 0.4800\n",
      "Epoch 282/500\n",
      " - 0s - loss: 1.9743 - accuracy: 0.4800\n",
      "Epoch 283/500\n",
      " - 0s - loss: 1.9718 - accuracy: 0.4800\n",
      "Epoch 284/500\n",
      " - 0s - loss: 1.9713 - accuracy: 0.5600\n",
      "Epoch 285/500\n",
      " - 0s - loss: 1.9685 - accuracy: 0.4800\n",
      "Epoch 286/500\n",
      " - 0s - loss: 1.9661 - accuracy: 0.5200\n",
      "Epoch 287/500\n",
      " - 0s - loss: 1.9633 - accuracy: 0.4800\n",
      "Epoch 288/500\n",
      " - 0s - loss: 1.9625 - accuracy: 0.5600\n",
      "Epoch 289/500\n",
      " - 0s - loss: 1.9611 - accuracy: 0.4800\n",
      "Epoch 290/500\n",
      " - 0s - loss: 1.9599 - accuracy: 0.5200\n",
      "Epoch 291/500\n",
      " - 0s - loss: 1.9576 - accuracy: 0.5600\n",
      "Epoch 292/500\n",
      " - 0s - loss: 1.9550 - accuracy: 0.6000\n",
      "Epoch 293/500\n",
      " - 0s - loss: 1.9565 - accuracy: 0.6000\n",
      "Epoch 294/500\n",
      " - 0s - loss: 1.9516 - accuracy: 0.5200\n",
      "Epoch 295/500\n",
      " - 0s - loss: 1.9516 - accuracy: 0.5600\n",
      "Epoch 296/500\n",
      " - 0s - loss: 1.9483 - accuracy: 0.5600\n",
      "Epoch 297/500\n",
      " - 0s - loss: 1.9471 - accuracy: 0.4400\n",
      "Epoch 298/500\n",
      " - 0s - loss: 1.9444 - accuracy: 0.5600\n",
      "Epoch 299/500\n",
      " - 0s - loss: 1.9455 - accuracy: 0.6000\n",
      "Epoch 300/500\n",
      " - 0s - loss: 1.9416 - accuracy: 0.5200\n",
      "Epoch 301/500\n",
      " - 0s - loss: 1.9405 - accuracy: 0.4800\n",
      "Epoch 302/500\n",
      " - 0s - loss: 1.9395 - accuracy: 0.5600\n",
      "Epoch 303/500\n",
      " - 0s - loss: 1.9378 - accuracy: 0.4800\n",
      "Epoch 304/500\n",
      " - 0s - loss: 1.9341 - accuracy: 0.5200\n",
      "Epoch 305/500\n",
      " - 0s - loss: 1.9354 - accuracy: 0.6000\n",
      "Epoch 306/500\n",
      " - 0s - loss: 1.9325 - accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      " - 0s - loss: 1.9321 - accuracy: 0.5200\n",
      "Epoch 308/500\n",
      " - 0s - loss: 1.9298 - accuracy: 0.5200\n",
      "Epoch 309/500\n",
      " - 0s - loss: 1.9274 - accuracy: 0.5600\n",
      "Epoch 310/500\n",
      " - 0s - loss: 1.9272 - accuracy: 0.5600\n",
      "Epoch 311/500\n",
      " - 0s - loss: 1.9252 - accuracy: 0.6000\n",
      "Epoch 312/500\n",
      " - 0s - loss: 1.9249 - accuracy: 0.5200\n",
      "Epoch 313/500\n",
      " - 0s - loss: 1.9216 - accuracy: 0.6000\n",
      "Epoch 314/500\n",
      " - 0s - loss: 1.9210 - accuracy: 0.4800\n",
      "Epoch 315/500\n",
      " - 0s - loss: 1.9183 - accuracy: 0.5600\n",
      "Epoch 316/500\n",
      " - 0s - loss: 1.9170 - accuracy: 0.6000\n",
      "Epoch 317/500\n",
      " - 0s - loss: 1.9156 - accuracy: 0.5600\n",
      "Epoch 318/500\n",
      " - 0s - loss: 1.9133 - accuracy: 0.6800\n",
      "Epoch 319/500\n",
      " - 0s - loss: 1.9114 - accuracy: 0.6400\n",
      "Epoch 320/500\n",
      " - 0s - loss: 1.9106 - accuracy: 0.5600\n",
      "Epoch 321/500\n",
      " - 0s - loss: 1.9094 - accuracy: 0.5200\n",
      "Epoch 322/500\n",
      " - 0s - loss: 1.9066 - accuracy: 0.6000\n",
      "Epoch 323/500\n",
      " - 0s - loss: 1.9056 - accuracy: 0.5600\n",
      "Epoch 324/500\n",
      " - 0s - loss: 1.9040 - accuracy: 0.5600\n",
      "Epoch 325/500\n",
      " - 0s - loss: 1.9036 - accuracy: 0.6000\n",
      "Epoch 326/500\n",
      " - 0s - loss: 1.8986 - accuracy: 0.6400\n",
      "Epoch 327/500\n",
      " - 0s - loss: 1.8988 - accuracy: 0.6000\n",
      "Epoch 328/500\n",
      " - 0s - loss: 1.8970 - accuracy: 0.6000\n",
      "Epoch 329/500\n",
      " - 0s - loss: 1.8946 - accuracy: 0.6800\n",
      "Epoch 330/500\n",
      " - 0s - loss: 1.8941 - accuracy: 0.6000\n",
      "Epoch 331/500\n",
      " - 0s - loss: 1.8939 - accuracy: 0.6000\n",
      "Epoch 332/500\n",
      " - 0s - loss: 1.8902 - accuracy: 0.6800\n",
      "Epoch 333/500\n",
      " - 0s - loss: 1.8878 - accuracy: 0.5600\n",
      "Epoch 334/500\n",
      " - 0s - loss: 1.8871 - accuracy: 0.5600\n",
      "Epoch 335/500\n",
      " - 0s - loss: 1.8864 - accuracy: 0.5600\n",
      "Epoch 336/500\n",
      " - 0s - loss: 1.8872 - accuracy: 0.5600\n",
      "Epoch 337/500\n",
      " - 0s - loss: 1.8836 - accuracy: 0.6800\n",
      "Epoch 338/500\n",
      " - 0s - loss: 1.8836 - accuracy: 0.6000\n",
      "Epoch 339/500\n",
      " - 0s - loss: 1.8830 - accuracy: 0.6000\n",
      "Epoch 340/500\n",
      " - 0s - loss: 1.8781 - accuracy: 0.6400\n",
      "Epoch 341/500\n",
      " - 0s - loss: 1.8793 - accuracy: 0.6800\n",
      "Epoch 342/500\n",
      " - 0s - loss: 1.8775 - accuracy: 0.5600\n",
      "Epoch 343/500\n",
      " - 0s - loss: 1.8763 - accuracy: 0.5200\n",
      "Epoch 344/500\n",
      " - 0s - loss: 1.8731 - accuracy: 0.5200\n",
      "Epoch 345/500\n",
      " - 0s - loss: 1.8723 - accuracy: 0.5600\n",
      "Epoch 346/500\n",
      " - 0s - loss: 1.8704 - accuracy: 0.6400\n",
      "Epoch 347/500\n",
      " - 0s - loss: 1.8690 - accuracy: 0.6000\n",
      "Epoch 348/500\n",
      " - 0s - loss: 1.8686 - accuracy: 0.5600\n",
      "Epoch 349/500\n",
      " - 0s - loss: 1.8670 - accuracy: 0.5200\n",
      "Epoch 350/500\n",
      " - 0s - loss: 1.8658 - accuracy: 0.6400\n",
      "Epoch 351/500\n",
      " - 0s - loss: 1.8639 - accuracy: 0.6400\n",
      "Epoch 352/500\n",
      " - 0s - loss: 1.8615 - accuracy: 0.5200\n",
      "Epoch 353/500\n",
      " - 0s - loss: 1.8630 - accuracy: 0.6000\n",
      "Epoch 354/500\n",
      " - 0s - loss: 1.8599 - accuracy: 0.6400\n",
      "Epoch 355/500\n",
      " - 0s - loss: 1.8580 - accuracy: 0.6400\n",
      "Epoch 356/500\n",
      " - 0s - loss: 1.8560 - accuracy: 0.6000\n",
      "Epoch 357/500\n",
      " - 0s - loss: 1.8566 - accuracy: 0.5600\n",
      "Epoch 358/500\n",
      " - 0s - loss: 1.8548 - accuracy: 0.6400\n",
      "Epoch 359/500\n",
      " - 0s - loss: 1.8529 - accuracy: 0.6400\n",
      "Epoch 360/500\n",
      " - 0s - loss: 1.8534 - accuracy: 0.6800\n",
      "Epoch 361/500\n",
      " - 0s - loss: 1.8489 - accuracy: 0.6400\n",
      "Epoch 362/500\n",
      " - 0s - loss: 1.8476 - accuracy: 0.6000\n",
      "Epoch 363/500\n",
      " - 0s - loss: 1.8489 - accuracy: 0.6800\n",
      "Epoch 364/500\n",
      " - 0s - loss: 1.8456 - accuracy: 0.7200\n",
      "Epoch 365/500\n",
      " - 0s - loss: 1.8451 - accuracy: 0.6400\n",
      "Epoch 366/500\n",
      " - 0s - loss: 1.8449 - accuracy: 0.6400\n",
      "Epoch 367/500\n",
      " - 0s - loss: 1.8424 - accuracy: 0.6400\n",
      "Epoch 368/500\n",
      " - 0s - loss: 1.8397 - accuracy: 0.6000\n",
      "Epoch 369/500\n",
      " - 0s - loss: 1.8376 - accuracy: 0.6800\n",
      "Epoch 370/500\n",
      " - 0s - loss: 1.8386 - accuracy: 0.6000\n",
      "Epoch 371/500\n",
      " - 0s - loss: 1.8373 - accuracy: 0.6400\n",
      "Epoch 372/500\n",
      " - 0s - loss: 1.8344 - accuracy: 0.6400\n",
      "Epoch 373/500\n",
      " - 0s - loss: 1.8357 - accuracy: 0.6800\n",
      "Epoch 374/500\n",
      " - 0s - loss: 1.8327 - accuracy: 0.6800\n",
      "Epoch 375/500\n",
      " - 0s - loss: 1.8318 - accuracy: 0.6800\n",
      "Epoch 376/500\n",
      " - 0s - loss: 1.8297 - accuracy: 0.6800\n",
      "Epoch 377/500\n",
      " - 0s - loss: 1.8290 - accuracy: 0.6400\n",
      "Epoch 378/500\n",
      " - 0s - loss: 1.8300 - accuracy: 0.6400\n",
      "Epoch 379/500\n",
      " - 0s - loss: 1.8265 - accuracy: 0.6800\n",
      "Epoch 380/500\n",
      " - 0s - loss: 1.8251 - accuracy: 0.6400\n",
      "Epoch 381/500\n",
      " - 0s - loss: 1.8239 - accuracy: 0.6800\n",
      "Epoch 382/500\n",
      " - 0s - loss: 1.8229 - accuracy: 0.6400\n",
      "Epoch 383/500\n",
      " - 0s - loss: 1.8197 - accuracy: 0.6800\n",
      "Epoch 384/500\n",
      " - 0s - loss: 1.8188 - accuracy: 0.6400\n",
      "Epoch 385/500\n",
      " - 0s - loss: 1.8205 - accuracy: 0.6800\n",
      "Epoch 386/500\n",
      " - 0s - loss: 1.8153 - accuracy: 0.6800\n",
      "Epoch 387/500\n",
      " - 0s - loss: 1.8144 - accuracy: 0.6000\n",
      "Epoch 388/500\n",
      " - 0s - loss: 1.8138 - accuracy: 0.7200\n",
      "Epoch 389/500\n",
      " - 0s - loss: 1.8149 - accuracy: 0.6400\n",
      "Epoch 390/500\n",
      " - 0s - loss: 1.8104 - accuracy: 0.6000\n",
      "Epoch 391/500\n",
      " - 0s - loss: 1.8117 - accuracy: 0.6000\n",
      "Epoch 392/500\n",
      " - 0s - loss: 1.8092 - accuracy: 0.6800\n",
      "Epoch 393/500\n",
      " - 0s - loss: 1.8085 - accuracy: 0.6000\n",
      "Epoch 394/500\n",
      " - 0s - loss: 1.8064 - accuracy: 0.5600\n",
      "Epoch 395/500\n",
      " - 0s - loss: 1.8056 - accuracy: 0.6800\n",
      "Epoch 396/500\n",
      " - 0s - loss: 1.8056 - accuracy: 0.6400\n",
      "Epoch 397/500\n",
      " - 0s - loss: 1.8027 - accuracy: 0.6800\n",
      "Epoch 398/500\n",
      " - 0s - loss: 1.8023 - accuracy: 0.6800\n",
      "Epoch 399/500\n",
      " - 0s - loss: 1.8020 - accuracy: 0.8000\n",
      "Epoch 400/500\n",
      " - 0s - loss: 1.8012 - accuracy: 0.5600\n",
      "Epoch 401/500\n",
      " - 0s - loss: 1.8009 - accuracy: 0.6800\n",
      "Epoch 402/500\n",
      " - 0s - loss: 1.7960 - accuracy: 0.6400\n",
      "Epoch 403/500\n",
      " - 0s - loss: 1.7978 - accuracy: 0.6000\n",
      "Epoch 404/500\n",
      " - 0s - loss: 1.7970 - accuracy: 0.6800\n",
      "Epoch 405/500\n",
      " - 0s - loss: 1.7946 - accuracy: 0.7200\n",
      "Epoch 406/500\n",
      " - 0s - loss: 1.7929 - accuracy: 0.7200\n",
      "Epoch 407/500\n",
      " - 0s - loss: 1.7924 - accuracy: 0.6800\n",
      "Epoch 408/500\n",
      " - 0s - loss: 1.7917 - accuracy: 0.7200\n",
      "Epoch 409/500\n",
      " - 0s - loss: 1.7900 - accuracy: 0.7600\n",
      "Epoch 410/500\n",
      " - 0s - loss: 1.7907 - accuracy: 0.6000\n",
      "Epoch 411/500\n",
      " - 0s - loss: 1.7870 - accuracy: 0.6000\n",
      "Epoch 412/500\n",
      " - 0s - loss: 1.7853 - accuracy: 0.7200\n",
      "Epoch 413/500\n",
      " - 0s - loss: 1.7842 - accuracy: 0.6800\n",
      "Epoch 414/500\n",
      " - 0s - loss: 1.7834 - accuracy: 0.7200\n",
      "Epoch 415/500\n",
      " - 0s - loss: 1.7826 - accuracy: 0.6800\n",
      "Epoch 416/500\n",
      " - 0s - loss: 1.7808 - accuracy: 0.6800\n",
      "Epoch 417/500\n",
      " - 0s - loss: 1.7814 - accuracy: 0.6400\n",
      "Epoch 418/500\n",
      " - 0s - loss: 1.7804 - accuracy: 0.7200\n",
      "Epoch 419/500\n",
      " - 0s - loss: 1.7769 - accuracy: 0.7200\n",
      "Epoch 420/500\n",
      " - 0s - loss: 1.7756 - accuracy: 0.6800\n",
      "Epoch 421/500\n",
      " - 0s - loss: 1.7762 - accuracy: 0.6400\n",
      "Epoch 422/500\n",
      " - 0s - loss: 1.7760 - accuracy: 0.6800\n",
      "Epoch 423/500\n",
      " - 0s - loss: 1.7747 - accuracy: 0.7200\n",
      "Epoch 424/500\n",
      " - 0s - loss: 1.7735 - accuracy: 0.6800\n",
      "Epoch 425/500\n",
      " - 0s - loss: 1.7700 - accuracy: 0.7200\n",
      "Epoch 426/500\n",
      " - 0s - loss: 1.7694 - accuracy: 0.7200\n",
      "Epoch 427/500\n",
      " - 0s - loss: 1.7688 - accuracy: 0.7600\n",
      "Epoch 428/500\n",
      " - 0s - loss: 1.7670 - accuracy: 0.7200\n",
      "Epoch 429/500\n",
      " - 0s - loss: 1.7650 - accuracy: 0.7200\n",
      "Epoch 430/500\n",
      " - 0s - loss: 1.7639 - accuracy: 0.7600\n",
      "Epoch 431/500\n",
      " - 0s - loss: 1.7641 - accuracy: 0.7200\n",
      "Epoch 432/500\n",
      " - 0s - loss: 1.7638 - accuracy: 0.7600\n",
      "Epoch 433/500\n",
      " - 0s - loss: 1.7609 - accuracy: 0.7200\n",
      "Epoch 434/500\n",
      " - 0s - loss: 1.7612 - accuracy: 0.7600\n",
      "Epoch 435/500\n",
      " - 0s - loss: 1.7582 - accuracy: 0.7600\n",
      "Epoch 436/500\n",
      " - 0s - loss: 1.7584 - accuracy: 0.8000\n",
      "Epoch 437/500\n",
      " - 0s - loss: 1.7566 - accuracy: 0.6400\n",
      "Epoch 438/500\n",
      " - 0s - loss: 1.7579 - accuracy: 0.6400\n",
      "Epoch 439/500\n",
      " - 0s - loss: 1.7555 - accuracy: 0.6400\n",
      "Epoch 440/500\n",
      " - 0s - loss: 1.7520 - accuracy: 0.7600\n",
      "Epoch 441/500\n",
      " - 0s - loss: 1.7534 - accuracy: 0.7200\n",
      "Epoch 442/500\n",
      " - 0s - loss: 1.7534 - accuracy: 0.7200\n",
      "Epoch 443/500\n",
      " - 0s - loss: 1.7515 - accuracy: 0.7200\n",
      "Epoch 444/500\n",
      " - 0s - loss: 1.7518 - accuracy: 0.6800\n",
      "Epoch 445/500\n",
      " - 0s - loss: 1.7503 - accuracy: 0.7600\n",
      "Epoch 446/500\n",
      " - 0s - loss: 1.7443 - accuracy: 0.6800\n",
      "Epoch 447/500\n",
      " - 0s - loss: 1.7478 - accuracy: 0.7200\n",
      "Epoch 448/500\n",
      " - 0s - loss: 1.7454 - accuracy: 0.6400\n",
      "Epoch 449/500\n",
      " - 0s - loss: 1.7438 - accuracy: 0.6800\n",
      "Epoch 450/500\n",
      " - 0s - loss: 1.7424 - accuracy: 0.7600\n",
      "Epoch 451/500\n",
      " - 0s - loss: 1.7424 - accuracy: 0.7600\n",
      "Epoch 452/500\n",
      " - 0s - loss: 1.7411 - accuracy: 0.7600\n",
      "Epoch 453/500\n",
      " - 0s - loss: 1.7390 - accuracy: 0.7200\n",
      "Epoch 454/500\n",
      " - 0s - loss: 1.7399 - accuracy: 0.8000\n",
      "Epoch 455/500\n",
      " - 0s - loss: 1.7389 - accuracy: 0.8000\n",
      "Epoch 456/500\n",
      " - 0s - loss: 1.7352 - accuracy: 0.7200\n",
      "Epoch 457/500\n",
      " - 0s - loss: 1.7358 - accuracy: 0.8400\n",
      "Epoch 458/500\n",
      " - 0s - loss: 1.7358 - accuracy: 0.7600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/500\n",
      " - 0s - loss: 1.7355 - accuracy: 0.6800\n",
      "Epoch 460/500\n",
      " - 0s - loss: 1.7324 - accuracy: 0.7600\n",
      "Epoch 461/500\n",
      " - 0s - loss: 1.7316 - accuracy: 0.7200\n",
      "Epoch 462/500\n",
      " - 0s - loss: 1.7302 - accuracy: 0.7600\n",
      "Epoch 463/500\n",
      " - 0s - loss: 1.7296 - accuracy: 0.7600\n",
      "Epoch 464/500\n",
      " - 0s - loss: 1.7279 - accuracy: 0.7200\n",
      "Epoch 465/500\n",
      " - 0s - loss: 1.7297 - accuracy: 0.7600\n",
      "Epoch 466/500\n",
      " - 0s - loss: 1.7273 - accuracy: 0.6800\n",
      "Epoch 467/500\n",
      " - 0s - loss: 1.7244 - accuracy: 0.7600\n",
      "Epoch 468/500\n",
      " - 0s - loss: 1.7240 - accuracy: 0.8000\n",
      "Epoch 469/500\n",
      " - 0s - loss: 1.7233 - accuracy: 0.8000\n",
      "Epoch 470/500\n",
      " - 0s - loss: 1.7214 - accuracy: 0.6800\n",
      "Epoch 471/500\n",
      " - 0s - loss: 1.7234 - accuracy: 0.7600\n",
      "Epoch 472/500\n",
      " - 0s - loss: 1.7212 - accuracy: 0.8000\n",
      "Epoch 473/500\n",
      " - 0s - loss: 1.7185 - accuracy: 0.6800\n",
      "Epoch 474/500\n",
      " - 0s - loss: 1.7202 - accuracy: 0.7200\n",
      "Epoch 475/500\n",
      " - 0s - loss: 1.7172 - accuracy: 0.7600\n",
      "Epoch 476/500\n",
      " - 0s - loss: 1.7203 - accuracy: 0.7200\n",
      "Epoch 477/500\n",
      " - 0s - loss: 1.7168 - accuracy: 0.7600\n",
      "Epoch 478/500\n",
      " - 0s - loss: 1.7144 - accuracy: 0.7600\n",
      "Epoch 479/500\n",
      " - 0s - loss: 1.7110 - accuracy: 0.8000\n",
      "Epoch 480/500\n",
      " - 0s - loss: 1.7111 - accuracy: 0.8000\n",
      "Epoch 481/500\n",
      " - 0s - loss: 1.7120 - accuracy: 0.8000\n",
      "Epoch 482/500\n",
      " - 0s - loss: 1.7104 - accuracy: 0.7600\n",
      "Epoch 483/500\n",
      " - 0s - loss: 1.7119 - accuracy: 0.7200\n",
      "Epoch 484/500\n",
      " - 0s - loss: 1.7073 - accuracy: 0.7200\n",
      "Epoch 485/500\n",
      " - 0s - loss: 1.7092 - accuracy: 0.7200\n",
      "Epoch 486/500\n",
      " - 0s - loss: 1.7071 - accuracy: 0.7600\n",
      "Epoch 487/500\n",
      " - 0s - loss: 1.7065 - accuracy: 0.8000\n",
      "Epoch 488/500\n",
      " - 0s - loss: 1.7058 - accuracy: 0.7600\n",
      "Epoch 489/500\n",
      " - 0s - loss: 1.7046 - accuracy: 0.7600\n",
      "Epoch 490/500\n",
      " - 0s - loss: 1.7025 - accuracy: 0.7200\n",
      "Epoch 491/500\n",
      " - 0s - loss: 1.7012 - accuracy: 0.7600\n",
      "Epoch 492/500\n",
      " - 0s - loss: 1.7000 - accuracy: 0.8400\n",
      "Epoch 493/500\n",
      " - 0s - loss: 1.6989 - accuracy: 0.7600\n",
      "Epoch 494/500\n",
      " - 0s - loss: 1.6994 - accuracy: 0.8400\n",
      "Epoch 495/500\n",
      " - 0s - loss: 1.6961 - accuracy: 0.7600\n",
      "Epoch 496/500\n",
      " - 0s - loss: 1.6972 - accuracy: 0.8000\n",
      "Epoch 497/500\n",
      " - 0s - loss: 1.6964 - accuracy: 0.6800\n",
      "Epoch 498/500\n",
      " - 0s - loss: 1.6962 - accuracy: 0.7600\n",
      "Epoch 499/500\n",
      " - 0s - loss: 1.6973 - accuracy: 0.8000\n",
      "Epoch 500/500\n",
      " - 0s - loss: 1.6932 - accuracy: 0.8000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16021fdbfd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the model\n",
    "# 定义一个LSTM网络，它有32个单元，一个输出层，其中有一个softmax的激活函数来进行预测。\n",
    "# 由于这是一个多类分类问题，所以我们可以使用在Keras中使用对数损失函数(称为“分类交叉熵”(categorical_crossentropy))，\n",
    "# 并使用ADAM优化函数对网络进行优化。\n",
    "# 该模型以500批次(epochs)，每批次数据输入大小(batch)为1的形式训练\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 88.00%\n"
     ]
    }
   ],
   "source": [
    "# summarize performance of the model\n",
    "# 对整个训练集的性能进行评估和总结\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A'] -> B\n",
      "['B'] -> B\n",
      "['C'] -> D\n",
      "['D'] -> E\n",
      "['E'] -> F\n",
      "['F'] -> G\n",
      "['G'] -> H\n",
      "['H'] -> I\n",
      "['I'] -> J\n",
      "['J'] -> K\n",
      "['K'] -> L\n",
      "['L'] -> M\n",
      "['M'] -> N\n",
      "['N'] -> O\n",
      "['O'] -> P\n",
      "['P'] -> Q\n",
      "['Q'] -> R\n",
      "['R'] -> S\n",
      "['S'] -> T\n",
      "['T'] -> U\n",
      "['U'] -> V\n",
      "['V'] -> W\n",
      "['W'] -> Y\n",
      "['X'] -> Z\n",
      "['Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# demonstrate some model predictions\n",
    "# 通过网络重新运行训练数据，并生成预测，将输入和输出对转换回原来的字符格式，以获得关于网络如何了解问题的视觉效果。\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原因是可怜的lstm单元根本没有可以利用的上下文章信息。 \n",
    "每个输入输出模式都以随机的顺序显示在网络中，并且网络的状态在每个模式之后被重置(每个批处理的每个批次包含一个模式)。\n",
    "\n",
    "这是对LSTM网络架构的滥用，因为我们把它当作了一个标准的多层感知器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三字符特征——单字符的映射的简单LSTM\n",
    "\n",
    "在多层感知器中添加更多上下文最流行的方法是特征窗口方法(Feature Window method)。\n",
    "\n",
    "即序列中的前面步骤的输出被作为附加的输入特性提供给网络。我们可以用相同的技巧，为LSTM网络提供更多的上下文。\n",
    "\n",
    "**将序列长度从1增加到3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive LSTM to learn three-char window to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n",
      "Epoch 1/500\n",
      " - 0s - loss: 3.2741 - accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2612 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2538 - accuracy: 0.0435\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2474 - accuracy: 0.0435\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2410 - accuracy: 0.0435\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2340 - accuracy: 0.0435\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2279 - accuracy: 0.0435\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2213 - accuracy: 0.0435\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.2139 - accuracy: 0.0435\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.2070 - accuracy: 0.0435\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.1994 - accuracy: 0.0435\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.1916 - accuracy: 0.0435\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.1839 - accuracy: 0.0435\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.1748 - accuracy: 0.0435\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.1663 - accuracy: 0.0435\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.1572 - accuracy: 0.0435\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.1483 - accuracy: 0.0435\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.1397 - accuracy: 0.0435\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.1296 - accuracy: 0.0435\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.1208 - accuracy: 0.0435\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.1126 - accuracy: 0.0435\n",
      "Epoch 22/500\n",
      " - 0s - loss: 3.1037 - accuracy: 0.0435\n",
      "Epoch 23/500\n",
      " - 0s - loss: 3.0945 - accuracy: 0.0435\n",
      "Epoch 24/500\n",
      " - 0s - loss: 3.0859 - accuracy: 0.0435\n",
      "Epoch 25/500\n",
      " - 0s - loss: 3.0784 - accuracy: 0.0435\n",
      "Epoch 26/500\n",
      " - 0s - loss: 3.0707 - accuracy: 0.0435\n",
      "Epoch 27/500\n",
      " - 0s - loss: 3.0624 - accuracy: 0.0435\n",
      "Epoch 28/500\n",
      " - 0s - loss: 3.0552 - accuracy: 0.0000e+00\n",
      "Epoch 29/500\n",
      " - 0s - loss: 3.0491 - accuracy: 0.0000e+00\n",
      "Epoch 30/500\n",
      " - 0s - loss: 3.0405 - accuracy: 0.0435\n",
      "Epoch 31/500\n",
      " - 0s - loss: 3.0346 - accuracy: 0.0435\n",
      "Epoch 32/500\n",
      " - 0s - loss: 3.0273 - accuracy: 0.0435\n",
      "Epoch 33/500\n",
      " - 0s - loss: 3.0193 - accuracy: 0.0435\n",
      "Epoch 34/500\n",
      " - 0s - loss: 3.0128 - accuracy: 0.0435\n",
      "Epoch 35/500\n",
      " - 0s - loss: 3.0049 - accuracy: 0.0435\n",
      "Epoch 36/500\n",
      " - 0s - loss: 2.9996 - accuracy: 0.0435\n",
      "Epoch 37/500\n",
      " - 0s - loss: 2.9902 - accuracy: 0.0870\n",
      "Epoch 38/500\n",
      " - 0s - loss: 2.9848 - accuracy: 0.0435\n",
      "Epoch 39/500\n",
      " - 0s - loss: 2.9746 - accuracy: 0.0435\n",
      "Epoch 40/500\n",
      " - 0s - loss: 2.9678 - accuracy: 0.0870\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.9599 - accuracy: 0.0870\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.9522 - accuracy: 0.0870\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.9433 - accuracy: 0.0435\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.9352 - accuracy: 0.0435\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.9262 - accuracy: 0.0435\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.9185 - accuracy: 0.0870\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.9091 - accuracy: 0.0435\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.9002 - accuracy: 0.0870\n",
      "Epoch 49/500\n",
      " - 0s - loss: 2.8904 - accuracy: 0.0870\n",
      "Epoch 50/500\n",
      " - 0s - loss: 2.8806 - accuracy: 0.0870\n",
      "Epoch 51/500\n",
      " - 0s - loss: 2.8715 - accuracy: 0.0435\n",
      "Epoch 52/500\n",
      " - 0s - loss: 2.8609 - accuracy: 0.0435\n",
      "Epoch 53/500\n",
      " - 0s - loss: 2.8532 - accuracy: 0.1304\n",
      "Epoch 54/500\n",
      " - 0s - loss: 2.8405 - accuracy: 0.1304\n",
      "Epoch 55/500\n",
      " - 0s - loss: 2.8316 - accuracy: 0.0870\n",
      "Epoch 56/500\n",
      " - 0s - loss: 2.8210 - accuracy: 0.0870\n",
      "Epoch 57/500\n",
      " - 0s - loss: 2.8110 - accuracy: 0.1304\n",
      "Epoch 58/500\n",
      " - 0s - loss: 2.8002 - accuracy: 0.0870\n",
      "Epoch 59/500\n",
      " - 0s - loss: 2.7898 - accuracy: 0.1304\n",
      "Epoch 60/500\n",
      " - 0s - loss: 2.7791 - accuracy: 0.0870\n",
      "Epoch 61/500\n",
      " - 0s - loss: 2.7695 - accuracy: 0.1304\n",
      "Epoch 62/500\n",
      " - 0s - loss: 2.7582 - accuracy: 0.0870\n",
      "Epoch 63/500\n",
      " - 0s - loss: 2.7500 - accuracy: 0.0435\n",
      "Epoch 64/500\n",
      " - 0s - loss: 2.7391 - accuracy: 0.1304\n",
      "Epoch 65/500\n",
      " - 0s - loss: 2.7295 - accuracy: 0.1304\n",
      "Epoch 66/500\n",
      " - 0s - loss: 2.7194 - accuracy: 0.0870\n",
      "Epoch 67/500\n",
      " - 0s - loss: 2.7112 - accuracy: 0.0435\n",
      "Epoch 68/500\n",
      " - 0s - loss: 2.7020 - accuracy: 0.1304\n",
      "Epoch 69/500\n",
      " - 0s - loss: 2.6900 - accuracy: 0.1304\n",
      "Epoch 70/500\n",
      " - 0s - loss: 2.6822 - accuracy: 0.0870\n",
      "Epoch 71/500\n",
      " - 0s - loss: 2.6720 - accuracy: 0.0870\n",
      "Epoch 72/500\n",
      " - 0s - loss: 2.6640 - accuracy: 0.0870\n",
      "Epoch 73/500\n",
      " - 0s - loss: 2.6559 - accuracy: 0.0870\n",
      "Epoch 74/500\n",
      " - 0s - loss: 2.6466 - accuracy: 0.0870\n",
      "Epoch 75/500\n",
      " - 0s - loss: 2.6382 - accuracy: 0.0870\n",
      "Epoch 76/500\n",
      " - 0s - loss: 2.6304 - accuracy: 0.0870\n",
      "Epoch 77/500\n",
      " - 0s - loss: 2.6237 - accuracy: 0.0870\n",
      "Epoch 78/500\n",
      " - 0s - loss: 2.6153 - accuracy: 0.0870\n",
      "Epoch 79/500\n",
      " - 0s - loss: 2.6073 - accuracy: 0.0870\n",
      "Epoch 80/500\n",
      " - 0s - loss: 2.6003 - accuracy: 0.0870\n",
      "Epoch 81/500\n",
      " - 0s - loss: 2.5932 - accuracy: 0.0870\n",
      "Epoch 82/500\n",
      " - 0s - loss: 2.5852 - accuracy: 0.0435\n",
      "Epoch 83/500\n",
      " - 0s - loss: 2.5791 - accuracy: 0.1304\n",
      "Epoch 84/500\n",
      " - 0s - loss: 2.5737 - accuracy: 0.0870\n",
      "Epoch 85/500\n",
      " - 0s - loss: 2.5642 - accuracy: 0.1304\n",
      "Epoch 86/500\n",
      " - 0s - loss: 2.5570 - accuracy: 0.0435\n",
      "Epoch 87/500\n",
      " - 0s - loss: 2.5534 - accuracy: 0.0870\n",
      "Epoch 88/500\n",
      " - 0s - loss: 2.5455 - accuracy: 0.0435\n",
      "Epoch 89/500\n",
      " - 0s - loss: 2.5392 - accuracy: 0.0870\n",
      "Epoch 90/500\n",
      " - 0s - loss: 2.5326 - accuracy: 0.1304\n",
      "Epoch 91/500\n",
      " - 0s - loss: 2.5273 - accuracy: 0.0435\n",
      "Epoch 92/500\n",
      " - 0s - loss: 2.5205 - accuracy: 0.1739\n",
      "Epoch 93/500\n",
      " - 0s - loss: 2.5149 - accuracy: 0.1739\n",
      "Epoch 94/500\n",
      " - 0s - loss: 2.5095 - accuracy: 0.1739\n",
      "Epoch 95/500\n",
      " - 0s - loss: 2.5042 - accuracy: 0.1304\n",
      "Epoch 96/500\n",
      " - 0s - loss: 2.4987 - accuracy: 0.1739\n",
      "Epoch 97/500\n",
      " - 0s - loss: 2.4914 - accuracy: 0.1739\n",
      "Epoch 98/500\n",
      " - 0s - loss: 2.4853 - accuracy: 0.1739\n",
      "Epoch 99/500\n",
      " - 0s - loss: 2.4802 - accuracy: 0.1304\n",
      "Epoch 100/500\n",
      " - 0s - loss: 2.4758 - accuracy: 0.1739\n",
      "Epoch 101/500\n",
      " - 0s - loss: 2.4707 - accuracy: 0.1739\n",
      "Epoch 102/500\n",
      " - 0s - loss: 2.4642 - accuracy: 0.1304\n",
      "Epoch 103/500\n",
      " - 0s - loss: 2.4584 - accuracy: 0.1304\n",
      "Epoch 104/500\n",
      " - 0s - loss: 2.4522 - accuracy: 0.1304\n",
      "Epoch 105/500\n",
      " - 0s - loss: 2.4490 - accuracy: 0.1304\n",
      "Epoch 106/500\n",
      " - 0s - loss: 2.4443 - accuracy: 0.1739\n",
      "Epoch 107/500\n",
      " - 0s - loss: 2.4389 - accuracy: 0.1739\n",
      "Epoch 108/500\n",
      " - 0s - loss: 2.4329 - accuracy: 0.1739\n",
      "Epoch 109/500\n",
      " - 0s - loss: 2.4272 - accuracy: 0.1304\n",
      "Epoch 110/500\n",
      " - 0s - loss: 2.4231 - accuracy: 0.1739\n",
      "Epoch 111/500\n",
      " - 0s - loss: 2.4181 - accuracy: 0.1739\n",
      "Epoch 112/500\n",
      " - 0s - loss: 2.4125 - accuracy: 0.1739\n",
      "Epoch 113/500\n",
      " - 0s - loss: 2.4098 - accuracy: 0.1304\n",
      "Epoch 114/500\n",
      " - 0s - loss: 2.4024 - accuracy: 0.1739\n",
      "Epoch 115/500\n",
      " - 0s - loss: 2.3991 - accuracy: 0.1739\n",
      "Epoch 116/500\n",
      " - 0s - loss: 2.3950 - accuracy: 0.1304\n",
      "Epoch 117/500\n",
      " - 0s - loss: 2.3891 - accuracy: 0.1739\n",
      "Epoch 118/500\n",
      " - 0s - loss: 2.3844 - accuracy: 0.1739\n",
      "Epoch 119/500\n",
      " - 0s - loss: 2.3798 - accuracy: 0.1739\n",
      "Epoch 120/500\n",
      " - 0s - loss: 2.3742 - accuracy: 0.1304\n",
      "Epoch 121/500\n",
      " - 0s - loss: 2.3704 - accuracy: 0.1739\n",
      "Epoch 122/500\n",
      " - 0s - loss: 2.3658 - accuracy: 0.1304\n",
      "Epoch 123/500\n",
      " - 0s - loss: 2.3611 - accuracy: 0.1739\n",
      "Epoch 124/500\n",
      " - 0s - loss: 2.3571 - accuracy: 0.1739\n",
      "Epoch 125/500\n",
      " - 0s - loss: 2.3517 - accuracy: 0.1739\n",
      "Epoch 126/500\n",
      " - 0s - loss: 2.3470 - accuracy: 0.1739\n",
      "Epoch 127/500\n",
      " - 0s - loss: 2.3426 - accuracy: 0.1739\n",
      "Epoch 128/500\n",
      " - 0s - loss: 2.3378 - accuracy: 0.1739\n",
      "Epoch 129/500\n",
      " - 0s - loss: 2.3366 - accuracy: 0.1739\n",
      "Epoch 130/500\n",
      " - 0s - loss: 2.3304 - accuracy: 0.1739\n",
      "Epoch 131/500\n",
      " - 0s - loss: 2.3263 - accuracy: 0.1739\n",
      "Epoch 132/500\n",
      " - 0s - loss: 2.3224 - accuracy: 0.1739\n",
      "Epoch 133/500\n",
      " - 0s - loss: 2.3170 - accuracy: 0.1739\n",
      "Epoch 134/500\n",
      " - 0s - loss: 2.3141 - accuracy: 0.1739\n",
      "Epoch 135/500\n",
      " - 0s - loss: 2.3097 - accuracy: 0.1739\n",
      "Epoch 136/500\n",
      " - 0s - loss: 2.3052 - accuracy: 0.1304\n",
      "Epoch 137/500\n",
      " - 0s - loss: 2.3000 - accuracy: 0.1739\n",
      "Epoch 138/500\n",
      " - 0s - loss: 2.2964 - accuracy: 0.1739\n",
      "Epoch 139/500\n",
      " - 0s - loss: 2.2939 - accuracy: 0.1739\n",
      "Epoch 140/500\n",
      " - 0s - loss: 2.2864 - accuracy: 0.1739\n",
      "Epoch 141/500\n",
      " - 0s - loss: 2.2857 - accuracy: 0.1739\n",
      "Epoch 142/500\n",
      " - 0s - loss: 2.2814 - accuracy: 0.1304\n",
      "Epoch 143/500\n",
      " - 0s - loss: 2.2754 - accuracy: 0.1304\n",
      "Epoch 144/500\n",
      " - 0s - loss: 2.2719 - accuracy: 0.1739\n",
      "Epoch 145/500\n",
      " - 0s - loss: 2.2679 - accuracy: 0.1739\n",
      "Epoch 146/500\n",
      " - 0s - loss: 2.2637 - accuracy: 0.1739\n",
      "Epoch 147/500\n",
      " - 0s - loss: 2.2606 - accuracy: 0.1304\n",
      "Epoch 148/500\n",
      " - 0s - loss: 2.2567 - accuracy: 0.1739\n",
      "Epoch 149/500\n",
      " - 0s - loss: 2.2531 - accuracy: 0.1739\n",
      "Epoch 150/500\n",
      " - 0s - loss: 2.2500 - accuracy: 0.1304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 151/500\n",
      " - 0s - loss: 2.2457 - accuracy: 0.1739\n",
      "Epoch 152/500\n",
      " - 0s - loss: 2.2424 - accuracy: 0.2174\n",
      "Epoch 153/500\n",
      " - 0s - loss: 2.2370 - accuracy: 0.1304\n",
      "Epoch 154/500\n",
      " - 0s - loss: 2.2346 - accuracy: 0.2174\n",
      "Epoch 155/500\n",
      " - 0s - loss: 2.2300 - accuracy: 0.1739\n",
      "Epoch 156/500\n",
      " - 0s - loss: 2.2278 - accuracy: 0.2174\n",
      "Epoch 157/500\n",
      " - 0s - loss: 2.2237 - accuracy: 0.1739\n",
      "Epoch 158/500\n",
      " - 0s - loss: 2.2192 - accuracy: 0.2174\n",
      "Epoch 159/500\n",
      " - 0s - loss: 2.2159 - accuracy: 0.2609\n",
      "Epoch 160/500\n",
      " - 0s - loss: 2.2129 - accuracy: 0.2174\n",
      "Epoch 161/500\n",
      " - 0s - loss: 2.2109 - accuracy: 0.1739\n",
      "Epoch 162/500\n",
      " - 0s - loss: 2.2061 - accuracy: 0.2609\n",
      "Epoch 163/500\n",
      " - 0s - loss: 2.2012 - accuracy: 0.2609\n",
      "Epoch 164/500\n",
      " - 0s - loss: 2.1991 - accuracy: 0.2174\n",
      "Epoch 165/500\n",
      " - 0s - loss: 2.1960 - accuracy: 0.3043\n",
      "Epoch 166/500\n",
      " - 0s - loss: 2.1931 - accuracy: 0.2174\n",
      "Epoch 167/500\n",
      " - 0s - loss: 2.1890 - accuracy: 0.1739\n",
      "Epoch 168/500\n",
      " - 0s - loss: 2.1858 - accuracy: 0.2609\n",
      "Epoch 169/500\n",
      " - 0s - loss: 2.1824 - accuracy: 0.1739\n",
      "Epoch 170/500\n",
      " - 0s - loss: 2.1802 - accuracy: 0.2174\n",
      "Epoch 171/500\n",
      " - 0s - loss: 2.1748 - accuracy: 0.2174\n",
      "Epoch 172/500\n",
      " - 0s - loss: 2.1719 - accuracy: 0.2609\n",
      "Epoch 173/500\n",
      " - 0s - loss: 2.1690 - accuracy: 0.3478\n",
      "Epoch 174/500\n",
      " - 0s - loss: 2.1674 - accuracy: 0.2609\n",
      "Epoch 175/500\n",
      " - 0s - loss: 2.1628 - accuracy: 0.2609\n",
      "Epoch 176/500\n",
      " - 0s - loss: 2.1590 - accuracy: 0.2609\n",
      "Epoch 177/500\n",
      " - 0s - loss: 2.1559 - accuracy: 0.2609\n",
      "Epoch 178/500\n",
      " - 0s - loss: 2.1549 - accuracy: 0.2609\n",
      "Epoch 179/500\n",
      " - 0s - loss: 2.1492 - accuracy: 0.2609\n",
      "Epoch 180/500\n",
      " - 0s - loss: 2.1473 - accuracy: 0.3043\n",
      "Epoch 181/500\n",
      " - 0s - loss: 2.1451 - accuracy: 0.3043\n",
      "Epoch 182/500\n",
      " - 0s - loss: 2.1410 - accuracy: 0.3478\n",
      "Epoch 183/500\n",
      " - 0s - loss: 2.1383 - accuracy: 0.3478\n",
      "Epoch 184/500\n",
      " - 0s - loss: 2.1351 - accuracy: 0.3043\n",
      "Epoch 185/500\n",
      " - 0s - loss: 2.1318 - accuracy: 0.3478\n",
      "Epoch 186/500\n",
      " - 0s - loss: 2.1299 - accuracy: 0.3043\n",
      "Epoch 187/500\n",
      " - 0s - loss: 2.1259 - accuracy: 0.2609\n",
      "Epoch 188/500\n",
      " - 0s - loss: 2.1244 - accuracy: 0.3043\n",
      "Epoch 189/500\n",
      " - 0s - loss: 2.1212 - accuracy: 0.3913\n",
      "Epoch 190/500\n",
      " - 0s - loss: 2.1170 - accuracy: 0.3913\n",
      "Epoch 191/500\n",
      " - 0s - loss: 2.1153 - accuracy: 0.2609\n",
      "Epoch 192/500\n",
      " - 0s - loss: 2.1107 - accuracy: 0.3043\n",
      "Epoch 193/500\n",
      " - 0s - loss: 2.1104 - accuracy: 0.3043\n",
      "Epoch 194/500\n",
      " - 0s - loss: 2.1072 - accuracy: 0.2174\n",
      "Epoch 195/500\n",
      " - 0s - loss: 2.1034 - accuracy: 0.3043\n",
      "Epoch 196/500\n",
      " - 0s - loss: 2.1024 - accuracy: 0.3043\n",
      "Epoch 197/500\n",
      " - 0s - loss: 2.0975 - accuracy: 0.3043\n",
      "Epoch 198/500\n",
      " - 0s - loss: 2.0945 - accuracy: 0.3913\n",
      "Epoch 199/500\n",
      " - 0s - loss: 2.0920 - accuracy: 0.3913\n",
      "Epoch 200/500\n",
      " - 0s - loss: 2.0896 - accuracy: 0.3478\n",
      "Epoch 201/500\n",
      " - 0s - loss: 2.0866 - accuracy: 0.3913\n",
      "Epoch 202/500\n",
      " - 0s - loss: 2.0841 - accuracy: 0.3478\n",
      "Epoch 203/500\n",
      " - 0s - loss: 2.0831 - accuracy: 0.3913\n",
      "Epoch 204/500\n",
      " - 0s - loss: 2.0793 - accuracy: 0.3478\n",
      "Epoch 205/500\n",
      " - 0s - loss: 2.0763 - accuracy: 0.3913\n",
      "Epoch 206/500\n",
      " - 0s - loss: 2.0741 - accuracy: 0.3043\n",
      "Epoch 207/500\n",
      " - 0s - loss: 2.0704 - accuracy: 0.3478\n",
      "Epoch 208/500\n",
      " - 0s - loss: 2.0676 - accuracy: 0.3043\n",
      "Epoch 209/500\n",
      " - 0s - loss: 2.0668 - accuracy: 0.3043\n",
      "Epoch 210/500\n",
      " - 0s - loss: 2.0628 - accuracy: 0.3043\n",
      "Epoch 211/500\n",
      " - 0s - loss: 2.0615 - accuracy: 0.4783\n",
      "Epoch 212/500\n",
      " - 0s - loss: 2.0586 - accuracy: 0.3478\n",
      "Epoch 213/500\n",
      " - 0s - loss: 2.0551 - accuracy: 0.3913\n",
      "Epoch 214/500\n",
      " - 0s - loss: 2.0525 - accuracy: 0.4348\n",
      "Epoch 215/500\n",
      " - 0s - loss: 2.0514 - accuracy: 0.3913\n",
      "Epoch 216/500\n",
      " - 0s - loss: 2.0494 - accuracy: 0.3913\n",
      "Epoch 217/500\n",
      " - 0s - loss: 2.0457 - accuracy: 0.3478\n",
      "Epoch 218/500\n",
      " - 0s - loss: 2.0450 - accuracy: 0.4348\n",
      "Epoch 219/500\n",
      " - 0s - loss: 2.0391 - accuracy: 0.4348\n",
      "Epoch 220/500\n",
      " - 0s - loss: 2.0382 - accuracy: 0.3043\n",
      "Epoch 221/500\n",
      " - 0s - loss: 2.0362 - accuracy: 0.2609\n",
      "Epoch 222/500\n",
      " - 0s - loss: 2.0320 - accuracy: 0.4348\n",
      "Epoch 223/500\n",
      " - 0s - loss: 2.0307 - accuracy: 0.3478\n",
      "Epoch 224/500\n",
      " - 0s - loss: 2.0280 - accuracy: 0.3913\n",
      "Epoch 225/500\n",
      " - 0s - loss: 2.0267 - accuracy: 0.3478\n",
      "Epoch 226/500\n",
      " - 0s - loss: 2.0250 - accuracy: 0.4348\n",
      "Epoch 227/500\n",
      " - 0s - loss: 2.0228 - accuracy: 0.4783\n",
      "Epoch 228/500\n",
      " - 0s - loss: 2.0201 - accuracy: 0.3913\n",
      "Epoch 229/500\n",
      " - 0s - loss: 2.0174 - accuracy: 0.3913\n",
      "Epoch 230/500\n",
      " - 0s - loss: 2.0135 - accuracy: 0.3478\n",
      "Epoch 231/500\n",
      " - 0s - loss: 2.0122 - accuracy: 0.3478\n",
      "Epoch 232/500\n",
      " - 0s - loss: 2.0102 - accuracy: 0.3913\n",
      "Epoch 233/500\n",
      " - 0s - loss: 2.0084 - accuracy: 0.3913\n",
      "Epoch 234/500\n",
      " - 0s - loss: 2.0055 - accuracy: 0.4348\n",
      "Epoch 235/500\n",
      " - 0s - loss: 2.0037 - accuracy: 0.4783\n",
      "Epoch 236/500\n",
      " - 0s - loss: 2.0019 - accuracy: 0.4348\n",
      "Epoch 237/500\n",
      " - 0s - loss: 1.9994 - accuracy: 0.3913\n",
      "Epoch 238/500\n",
      " - 0s - loss: 1.9952 - accuracy: 0.4348\n",
      "Epoch 239/500\n",
      " - 0s - loss: 1.9948 - accuracy: 0.3913\n",
      "Epoch 240/500\n",
      " - 0s - loss: 1.9926 - accuracy: 0.4348\n",
      "Epoch 241/500\n",
      " - 0s - loss: 1.9902 - accuracy: 0.3913\n",
      "Epoch 242/500\n",
      " - 0s - loss: 1.9877 - accuracy: 0.3478\n",
      "Epoch 243/500\n",
      " - 0s - loss: 1.9850 - accuracy: 0.3478\n",
      "Epoch 244/500\n",
      " - 0s - loss: 1.9820 - accuracy: 0.3478\n",
      "Epoch 245/500\n",
      " - 0s - loss: 1.9804 - accuracy: 0.4348\n",
      "Epoch 246/500\n",
      " - 0s - loss: 1.9800 - accuracy: 0.5217\n",
      "Epoch 247/500\n",
      " - 0s - loss: 1.9773 - accuracy: 0.5217\n",
      "Epoch 248/500\n",
      " - 0s - loss: 1.9754 - accuracy: 0.5217\n",
      "Epoch 249/500\n",
      " - 0s - loss: 1.9726 - accuracy: 0.5217\n",
      "Epoch 250/500\n",
      " - 0s - loss: 1.9731 - accuracy: 0.4348\n",
      "Epoch 251/500\n",
      " - 0s - loss: 1.9677 - accuracy: 0.5217\n",
      "Epoch 252/500\n",
      " - 0s - loss: 1.9678 - accuracy: 0.3913\n",
      "Epoch 253/500\n",
      " - 0s - loss: 1.9646 - accuracy: 0.4348\n",
      "Epoch 254/500\n",
      " - 0s - loss: 1.9634 - accuracy: 0.5652\n",
      "Epoch 255/500\n",
      " - 0s - loss: 1.9600 - accuracy: 0.5217\n",
      "Epoch 256/500\n",
      " - 0s - loss: 1.9588 - accuracy: 0.4348\n",
      "Epoch 257/500\n",
      " - 0s - loss: 1.9572 - accuracy: 0.4783\n",
      "Epoch 258/500\n",
      " - 0s - loss: 1.9528 - accuracy: 0.5217\n",
      "Epoch 259/500\n",
      " - 0s - loss: 1.9535 - accuracy: 0.4348\n",
      "Epoch 260/500\n",
      " - 0s - loss: 1.9474 - accuracy: 0.5652\n",
      "Epoch 261/500\n",
      " - 0s - loss: 1.9473 - accuracy: 0.5652\n",
      "Epoch 262/500\n",
      " - 0s - loss: 1.9443 - accuracy: 0.4348\n",
      "Epoch 263/500\n",
      " - 0s - loss: 1.9432 - accuracy: 0.4783\n",
      "Epoch 264/500\n",
      " - 0s - loss: 1.9453 - accuracy: 0.4348\n",
      "Epoch 265/500\n",
      " - 0s - loss: 1.9401 - accuracy: 0.4783\n",
      "Epoch 266/500\n",
      " - 0s - loss: 1.9372 - accuracy: 0.4783\n",
      "Epoch 267/500\n",
      " - 0s - loss: 1.9367 - accuracy: 0.5217\n",
      "Epoch 268/500\n",
      " - 0s - loss: 1.9319 - accuracy: 0.4783\n",
      "Epoch 269/500\n",
      " - 0s - loss: 1.9325 - accuracy: 0.3913\n",
      "Epoch 270/500\n",
      " - 0s - loss: 1.9304 - accuracy: 0.5217\n",
      "Epoch 271/500\n",
      " - 0s - loss: 1.9303 - accuracy: 0.4783\n",
      "Epoch 272/500\n",
      " - 0s - loss: 1.9250 - accuracy: 0.4783\n",
      "Epoch 273/500\n",
      " - 0s - loss: 1.9255 - accuracy: 0.4348\n",
      "Epoch 274/500\n",
      " - 0s - loss: 1.9245 - accuracy: 0.5652\n",
      "Epoch 275/500\n",
      " - 0s - loss: 1.9203 - accuracy: 0.5652\n",
      "Epoch 276/500\n",
      " - 0s - loss: 1.9185 - accuracy: 0.4783\n",
      "Epoch 277/500\n",
      " - 0s - loss: 1.9146 - accuracy: 0.3913\n",
      "Epoch 278/500\n",
      " - 0s - loss: 1.9149 - accuracy: 0.5217\n",
      "Epoch 279/500\n",
      " - 0s - loss: 1.9144 - accuracy: 0.5217\n",
      "Epoch 280/500\n",
      " - 0s - loss: 1.9126 - accuracy: 0.5217\n",
      "Epoch 281/500\n",
      " - 0s - loss: 1.9081 - accuracy: 0.6087\n",
      "Epoch 282/500\n",
      " - 0s - loss: 1.9098 - accuracy: 0.5217\n",
      "Epoch 283/500\n",
      " - 0s - loss: 1.9082 - accuracy: 0.4783\n",
      "Epoch 284/500\n",
      " - 0s - loss: 1.9042 - accuracy: 0.4783\n",
      "Epoch 285/500\n",
      " - 0s - loss: 1.9029 - accuracy: 0.5652\n",
      "Epoch 286/500\n",
      " - 0s - loss: 1.9013 - accuracy: 0.4348\n",
      "Epoch 287/500\n",
      " - 0s - loss: 1.8994 - accuracy: 0.6087\n",
      "Epoch 288/500\n",
      " - 0s - loss: 1.8969 - accuracy: 0.5652\n",
      "Epoch 289/500\n",
      " - 0s - loss: 1.8922 - accuracy: 0.5217\n",
      "Epoch 290/500\n",
      " - 0s - loss: 1.8926 - accuracy: 0.6087\n",
      "Epoch 291/500\n",
      " - 0s - loss: 1.8899 - accuracy: 0.5652\n",
      "Epoch 292/500\n",
      " - 0s - loss: 1.8884 - accuracy: 0.5652\n",
      "Epoch 293/500\n",
      " - 0s - loss: 1.8881 - accuracy: 0.5217\n",
      "Epoch 294/500\n",
      " - 0s - loss: 1.8855 - accuracy: 0.5652\n",
      "Epoch 295/500\n",
      " - 0s - loss: 1.8836 - accuracy: 0.5217\n",
      "Epoch 296/500\n",
      " - 0s - loss: 1.8817 - accuracy: 0.6087\n",
      "Epoch 297/500\n",
      " - 0s - loss: 1.8810 - accuracy: 0.5652\n",
      "Epoch 298/500\n",
      " - 0s - loss: 1.8803 - accuracy: 0.5652\n",
      "Epoch 299/500\n",
      " - 0s - loss: 1.8768 - accuracy: 0.6087\n",
      "Epoch 300/500\n",
      " - 0s - loss: 1.8743 - accuracy: 0.6087\n",
      "Epoch 301/500\n",
      " - 0s - loss: 1.8726 - accuracy: 0.5217\n",
      "Epoch 302/500\n",
      " - 0s - loss: 1.8709 - accuracy: 0.5652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 303/500\n",
      " - 0s - loss: 1.8703 - accuracy: 0.5652\n",
      "Epoch 304/500\n",
      " - 0s - loss: 1.8710 - accuracy: 0.4783\n",
      "Epoch 305/500\n",
      " - 0s - loss: 1.8684 - accuracy: 0.5652\n",
      "Epoch 306/500\n",
      " - 0s - loss: 1.8637 - accuracy: 0.4783\n",
      "Epoch 307/500\n",
      " - 0s - loss: 1.8627 - accuracy: 0.6087\n",
      "Epoch 308/500\n",
      " - 0s - loss: 1.8614 - accuracy: 0.6522\n",
      "Epoch 309/500\n",
      " - 0s - loss: 1.8609 - accuracy: 0.5652\n",
      "Epoch 310/500\n",
      " - 0s - loss: 1.8578 - accuracy: 0.5652\n",
      "Epoch 311/500\n",
      " - 0s - loss: 1.8565 - accuracy: 0.6522\n",
      "Epoch 312/500\n",
      " - 0s - loss: 1.8527 - accuracy: 0.6087\n",
      "Epoch 313/500\n",
      " - 0s - loss: 1.8527 - accuracy: 0.5217\n",
      "Epoch 314/500\n",
      " - 0s - loss: 1.8508 - accuracy: 0.6087\n",
      "Epoch 315/500\n",
      " - 0s - loss: 1.8496 - accuracy: 0.6087\n",
      "Epoch 316/500\n",
      " - 0s - loss: 1.8481 - accuracy: 0.6087\n",
      "Epoch 317/500\n",
      " - 0s - loss: 1.8448 - accuracy: 0.6087\n",
      "Epoch 318/500\n",
      " - 0s - loss: 1.8442 - accuracy: 0.6087\n",
      "Epoch 319/500\n",
      " - 0s - loss: 1.8421 - accuracy: 0.6957\n",
      "Epoch 320/500\n",
      " - 0s - loss: 1.8415 - accuracy: 0.6087\n",
      "Epoch 321/500\n",
      " - 0s - loss: 1.8396 - accuracy: 0.6087\n",
      "Epoch 322/500\n",
      " - 0s - loss: 1.8396 - accuracy: 0.4783\n",
      "Epoch 323/500\n",
      " - 0s - loss: 1.8349 - accuracy: 0.5652\n",
      "Epoch 324/500\n",
      " - 0s - loss: 1.8361 - accuracy: 0.6957\n",
      "Epoch 325/500\n",
      " - 0s - loss: 1.8338 - accuracy: 0.6522\n",
      "Epoch 326/500\n",
      " - 0s - loss: 1.8320 - accuracy: 0.6522\n",
      "Epoch 327/500\n",
      " - 0s - loss: 1.8336 - accuracy: 0.6087\n",
      "Epoch 328/500\n",
      " - 0s - loss: 1.8289 - accuracy: 0.6522\n",
      "Epoch 329/500\n",
      " - 0s - loss: 1.8266 - accuracy: 0.6087\n",
      "Epoch 330/500\n",
      " - 0s - loss: 1.8252 - accuracy: 0.6522\n",
      "Epoch 331/500\n",
      " - 0s - loss: 1.8275 - accuracy: 0.6087\n",
      "Epoch 332/500\n",
      " - 0s - loss: 1.8227 - accuracy: 0.6087\n",
      "Epoch 333/500\n",
      " - 0s - loss: 1.8224 - accuracy: 0.6522\n",
      "Epoch 334/500\n",
      " - 0s - loss: 1.8189 - accuracy: 0.5652\n",
      "Epoch 335/500\n",
      " - 0s - loss: 1.8199 - accuracy: 0.6522\n",
      "Epoch 336/500\n",
      " - 0s - loss: 1.8154 - accuracy: 0.6087\n",
      "Epoch 337/500\n",
      " - 0s - loss: 1.8139 - accuracy: 0.6957\n",
      "Epoch 338/500\n",
      " - 0s - loss: 1.8132 - accuracy: 0.5652\n",
      "Epoch 339/500\n",
      " - 0s - loss: 1.8113 - accuracy: 0.6957\n",
      "Epoch 340/500\n",
      " - 0s - loss: 1.8104 - accuracy: 0.6087\n",
      "Epoch 341/500\n",
      " - 0s - loss: 1.8090 - accuracy: 0.6087\n",
      "Epoch 342/500\n",
      " - 0s - loss: 1.8092 - accuracy: 0.6957\n",
      "Epoch 343/500\n",
      " - 0s - loss: 1.8059 - accuracy: 0.6522\n",
      "Epoch 344/500\n",
      " - 0s - loss: 1.8027 - accuracy: 0.6957\n",
      "Epoch 345/500\n",
      " - 0s - loss: 1.8018 - accuracy: 0.6522\n",
      "Epoch 346/500\n",
      " - 0s - loss: 1.8008 - accuracy: 0.6957\n",
      "Epoch 347/500\n",
      " - 0s - loss: 1.8009 - accuracy: 0.6087\n",
      "Epoch 348/500\n",
      " - 0s - loss: 1.7972 - accuracy: 0.6522\n",
      "Epoch 349/500\n",
      " - 0s - loss: 1.7968 - accuracy: 0.6957\n",
      "Epoch 350/500\n",
      " - 0s - loss: 1.7952 - accuracy: 0.6957\n",
      "Epoch 351/500\n",
      " - 0s - loss: 1.7921 - accuracy: 0.6087\n",
      "Epoch 352/500\n",
      " - 0s - loss: 1.7933 - accuracy: 0.6522\n",
      "Epoch 353/500\n",
      " - 0s - loss: 1.7909 - accuracy: 0.6087\n",
      "Epoch 354/500\n",
      " - 0s - loss: 1.7887 - accuracy: 0.6522\n",
      "Epoch 355/500\n",
      " - 0s - loss: 1.7908 - accuracy: 0.6087\n",
      "Epoch 356/500\n",
      " - 0s - loss: 1.7857 - accuracy: 0.6087\n",
      "Epoch 357/500\n",
      " - 0s - loss: 1.7848 - accuracy: 0.5652\n",
      "Epoch 358/500\n",
      " - 0s - loss: 1.7858 - accuracy: 0.6522\n",
      "Epoch 359/500\n",
      " - 0s - loss: 1.7825 - accuracy: 0.6522\n",
      "Epoch 360/500\n",
      " - 0s - loss: 1.7787 - accuracy: 0.6957\n",
      "Epoch 361/500\n",
      " - 0s - loss: 1.7787 - accuracy: 0.6957\n",
      "Epoch 362/500\n",
      " - 0s - loss: 1.7767 - accuracy: 0.6522\n",
      "Epoch 363/500\n",
      " - 0s - loss: 1.7771 - accuracy: 0.6087\n",
      "Epoch 364/500\n",
      " - 0s - loss: 1.7754 - accuracy: 0.6522\n",
      "Epoch 365/500\n",
      " - 0s - loss: 1.7745 - accuracy: 0.6087\n",
      "Epoch 366/500\n",
      " - 0s - loss: 1.7737 - accuracy: 0.6522\n",
      "Epoch 367/500\n",
      " - 0s - loss: 1.7714 - accuracy: 0.6087\n",
      "Epoch 368/500\n",
      " - 0s - loss: 1.7690 - accuracy: 0.6957\n",
      "Epoch 369/500\n",
      " - 0s - loss: 1.7672 - accuracy: 0.7391\n",
      "Epoch 370/500\n",
      " - 0s - loss: 1.7675 - accuracy: 0.6522\n",
      "Epoch 371/500\n",
      " - 0s - loss: 1.7651 - accuracy: 0.6957\n",
      "Epoch 372/500\n",
      " - 0s - loss: 1.7643 - accuracy: 0.6957\n",
      "Epoch 373/500\n",
      " - 0s - loss: 1.7616 - accuracy: 0.6522\n",
      "Epoch 374/500\n",
      " - 0s - loss: 1.7596 - accuracy: 0.6522\n",
      "Epoch 375/500\n",
      " - 0s - loss: 1.7608 - accuracy: 0.6957\n",
      "Epoch 376/500\n",
      " - 0s - loss: 1.7575 - accuracy: 0.6087\n",
      "Epoch 377/500\n",
      " - 0s - loss: 1.7560 - accuracy: 0.7391\n",
      "Epoch 378/500\n",
      " - 0s - loss: 1.7569 - accuracy: 0.6957\n",
      "Epoch 379/500\n",
      " - 0s - loss: 1.7523 - accuracy: 0.6087\n",
      "Epoch 380/500\n",
      " - 0s - loss: 1.7536 - accuracy: 0.6522\n",
      "Epoch 381/500\n",
      " - 0s - loss: 1.7529 - accuracy: 0.6957\n",
      "Epoch 382/500\n",
      " - 0s - loss: 1.7516 - accuracy: 0.7391\n",
      "Epoch 383/500\n",
      " - 0s - loss: 1.7473 - accuracy: 0.6957\n",
      "Epoch 384/500\n",
      " - 0s - loss: 1.7468 - accuracy: 0.6522\n",
      "Epoch 385/500\n",
      " - 0s - loss: 1.7489 - accuracy: 0.6957\n",
      "Epoch 386/500\n",
      " - 0s - loss: 1.7448 - accuracy: 0.6957\n",
      "Epoch 387/500\n",
      " - 0s - loss: 1.7421 - accuracy: 0.6957\n",
      "Epoch 388/500\n",
      " - 0s - loss: 1.7413 - accuracy: 0.7826\n",
      "Epoch 389/500\n",
      " - 0s - loss: 1.7424 - accuracy: 0.6957\n",
      "Epoch 390/500\n",
      " - 0s - loss: 1.7396 - accuracy: 0.6522\n",
      "Epoch 391/500\n",
      " - 0s - loss: 1.7376 - accuracy: 0.6522\n",
      "Epoch 392/500\n",
      " - 0s - loss: 1.7354 - accuracy: 0.7391\n",
      "Epoch 393/500\n",
      " - 0s - loss: 1.7367 - accuracy: 0.6957\n",
      "Epoch 394/500\n",
      " - 0s - loss: 1.7333 - accuracy: 0.6522\n",
      "Epoch 395/500\n",
      " - 0s - loss: 1.7342 - accuracy: 0.6522\n",
      "Epoch 396/500\n",
      " - 0s - loss: 1.7316 - accuracy: 0.6957\n",
      "Epoch 397/500\n",
      " - 0s - loss: 1.7321 - accuracy: 0.6957\n",
      "Epoch 398/500\n",
      " - 0s - loss: 1.7309 - accuracy: 0.6957\n",
      "Epoch 399/500\n",
      " - 0s - loss: 1.7286 - accuracy: 0.7826\n",
      "Epoch 400/500\n",
      " - 0s - loss: 1.7278 - accuracy: 0.6957\n",
      "Epoch 401/500\n",
      " - 0s - loss: 1.7257 - accuracy: 0.6957\n",
      "Epoch 402/500\n",
      " - 0s - loss: 1.7227 - accuracy: 0.6957\n",
      "Epoch 403/500\n",
      " - 0s - loss: 1.7230 - accuracy: 0.6522\n",
      "Epoch 404/500\n",
      " - 0s - loss: 1.7234 - accuracy: 0.7391\n",
      "Epoch 405/500\n",
      " - 0s - loss: 1.7217 - accuracy: 0.6957\n",
      "Epoch 406/500\n",
      " - 0s - loss: 1.7203 - accuracy: 0.7391\n",
      "Epoch 407/500\n",
      " - 0s - loss: 1.7174 - accuracy: 0.7391\n",
      "Epoch 408/500\n",
      " - 0s - loss: 1.7182 - accuracy: 0.7391\n",
      "Epoch 409/500\n",
      " - 0s - loss: 1.7158 - accuracy: 0.8261\n",
      "Epoch 410/500\n",
      " - 0s - loss: 1.7159 - accuracy: 0.7391\n",
      "Epoch 411/500\n",
      " - 0s - loss: 1.7143 - accuracy: 0.7826\n",
      "Epoch 412/500\n",
      " - 0s - loss: 1.7144 - accuracy: 0.6522\n",
      "Epoch 413/500\n",
      " - 0s - loss: 1.7125 - accuracy: 0.6522\n",
      "Epoch 414/500\n",
      " - 0s - loss: 1.7100 - accuracy: 0.6957\n",
      "Epoch 415/500\n",
      " - 0s - loss: 1.7085 - accuracy: 0.6957\n",
      "Epoch 416/500\n",
      " - 0s - loss: 1.7061 - accuracy: 0.7391\n",
      "Epoch 417/500\n",
      " - 0s - loss: 1.7039 - accuracy: 0.6957\n",
      "Epoch 418/500\n",
      " - 0s - loss: 1.7031 - accuracy: 0.6957\n",
      "Epoch 419/500\n",
      " - 0s - loss: 1.7030 - accuracy: 0.6957\n",
      "Epoch 420/500\n",
      " - 0s - loss: 1.7034 - accuracy: 0.7391\n",
      "Epoch 421/500\n",
      " - 0s - loss: 1.7026 - accuracy: 0.7391\n",
      "Epoch 422/500\n",
      " - 0s - loss: 1.6977 - accuracy: 0.8261\n",
      "Epoch 423/500\n",
      " - 0s - loss: 1.6975 - accuracy: 0.8261\n",
      "Epoch 424/500\n",
      " - 0s - loss: 1.6983 - accuracy: 0.7391\n",
      "Epoch 425/500\n",
      " - 0s - loss: 1.6989 - accuracy: 0.7391\n",
      "Epoch 426/500\n",
      " - 0s - loss: 1.6950 - accuracy: 0.7391\n",
      "Epoch 427/500\n",
      " - 0s - loss: 1.6922 - accuracy: 0.7826\n",
      "Epoch 428/500\n",
      " - 0s - loss: 1.6920 - accuracy: 0.7826\n",
      "Epoch 429/500\n",
      " - 0s - loss: 1.6921 - accuracy: 0.7391\n",
      "Epoch 430/500\n",
      " - 0s - loss: 1.6911 - accuracy: 0.6522\n",
      "Epoch 431/500\n",
      " - 0s - loss: 1.6892 - accuracy: 0.7391\n",
      "Epoch 432/500\n",
      " - 0s - loss: 1.6893 - accuracy: 0.7391\n",
      "Epoch 433/500\n",
      " - 0s - loss: 1.6850 - accuracy: 0.8261\n",
      "Epoch 434/500\n",
      " - 0s - loss: 1.6844 - accuracy: 0.7826\n",
      "Epoch 435/500\n",
      " - 0s - loss: 1.6836 - accuracy: 0.8261\n",
      "Epoch 436/500\n",
      " - 0s - loss: 1.6826 - accuracy: 0.7826\n",
      "Epoch 437/500\n",
      " - 0s - loss: 1.6838 - accuracy: 0.7826\n",
      "Epoch 438/500\n",
      " - 0s - loss: 1.6822 - accuracy: 0.7391\n",
      "Epoch 439/500\n",
      " - 0s - loss: 1.6793 - accuracy: 0.7826\n",
      "Epoch 440/500\n",
      " - 0s - loss: 1.6777 - accuracy: 0.8261\n",
      "Epoch 441/500\n",
      " - 0s - loss: 1.6790 - accuracy: 0.8261\n",
      "Epoch 442/500\n",
      " - 0s - loss: 1.6769 - accuracy: 0.7826\n",
      "Epoch 443/500\n",
      " - 0s - loss: 1.6718 - accuracy: 0.7826\n",
      "Epoch 444/500\n",
      " - 0s - loss: 1.6760 - accuracy: 0.7391\n",
      "Epoch 445/500\n",
      " - 0s - loss: 1.6726 - accuracy: 0.7826\n",
      "Epoch 446/500\n",
      " - 0s - loss: 1.6715 - accuracy: 0.7826\n",
      "Epoch 447/500\n",
      " - 0s - loss: 1.6710 - accuracy: 0.7391\n",
      "Epoch 448/500\n",
      " - 0s - loss: 1.6704 - accuracy: 0.7826\n",
      "Epoch 449/500\n",
      " - 0s - loss: 1.6700 - accuracy: 0.7826\n",
      "Epoch 450/500\n",
      " - 0s - loss: 1.6665 - accuracy: 0.8261\n",
      "Epoch 451/500\n",
      " - 0s - loss: 1.6657 - accuracy: 0.8696\n",
      "Epoch 452/500\n",
      " - 0s - loss: 1.6650 - accuracy: 0.7826\n",
      "Epoch 453/500\n",
      " - 0s - loss: 1.6651 - accuracy: 0.7826\n",
      "Epoch 454/500\n",
      " - 0s - loss: 1.6638 - accuracy: 0.6957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 455/500\n",
      " - 0s - loss: 1.6597 - accuracy: 0.7391\n",
      "Epoch 456/500\n",
      " - 0s - loss: 1.6633 - accuracy: 0.7826\n",
      "Epoch 457/500\n",
      " - 0s - loss: 1.6567 - accuracy: 0.7391\n",
      "Epoch 458/500\n",
      " - 0s - loss: 1.6598 - accuracy: 0.6957\n",
      "Epoch 459/500\n",
      " - 0s - loss: 1.6575 - accuracy: 0.8261\n",
      "Epoch 460/500\n",
      " - 0s - loss: 1.6587 - accuracy: 0.7391\n",
      "Epoch 461/500\n",
      " - 0s - loss: 1.6570 - accuracy: 0.7826\n",
      "Epoch 462/500\n",
      " - 0s - loss: 1.6546 - accuracy: 0.8261\n",
      "Epoch 463/500\n",
      " - 0s - loss: 1.6519 - accuracy: 0.7391\n",
      "Epoch 464/500\n",
      " - 0s - loss: 1.6493 - accuracy: 0.7391\n",
      "Epoch 465/500\n",
      " - 0s - loss: 1.6507 - accuracy: 0.8261\n",
      "Epoch 466/500\n",
      " - 0s - loss: 1.6528 - accuracy: 0.6957\n",
      "Epoch 467/500\n",
      " - 0s - loss: 1.6484 - accuracy: 0.8261\n",
      "Epoch 468/500\n",
      " - 0s - loss: 1.6474 - accuracy: 0.8696\n",
      "Epoch 469/500\n",
      " - 0s - loss: 1.6454 - accuracy: 0.8261\n",
      "Epoch 470/500\n",
      " - 0s - loss: 1.6441 - accuracy: 0.8261\n",
      "Epoch 471/500\n",
      " - 0s - loss: 1.6454 - accuracy: 0.7826\n",
      "Epoch 472/500\n",
      " - 0s - loss: 1.6414 - accuracy: 0.7391\n",
      "Epoch 473/500\n",
      " - 0s - loss: 1.6424 - accuracy: 0.8261\n",
      "Epoch 474/500\n",
      " - 0s - loss: 1.6411 - accuracy: 0.7826\n",
      "Epoch 475/500\n",
      " - 0s - loss: 1.6379 - accuracy: 0.7826\n",
      "Epoch 476/500\n",
      " - 0s - loss: 1.6362 - accuracy: 0.7391\n",
      "Epoch 477/500\n",
      " - 0s - loss: 1.6366 - accuracy: 0.8261\n",
      "Epoch 478/500\n",
      " - 0s - loss: 1.6358 - accuracy: 0.8696\n",
      "Epoch 479/500\n",
      " - 0s - loss: 1.6336 - accuracy: 0.8261\n",
      "Epoch 480/500\n",
      " - 0s - loss: 1.6358 - accuracy: 0.8261\n",
      "Epoch 481/500\n",
      " - 0s - loss: 1.6351 - accuracy: 0.7826\n",
      "Epoch 482/500\n",
      " - 0s - loss: 1.6325 - accuracy: 0.8261\n",
      "Epoch 483/500\n",
      " - 0s - loss: 1.6331 - accuracy: 0.7826\n",
      "Epoch 484/500\n",
      " - 0s - loss: 1.6314 - accuracy: 0.7826\n",
      "Epoch 485/500\n",
      " - 0s - loss: 1.6297 - accuracy: 0.7826\n",
      "Epoch 486/500\n",
      " - 0s - loss: 1.6269 - accuracy: 0.7826\n",
      "Epoch 487/500\n",
      " - 0s - loss: 1.6285 - accuracy: 0.8261\n",
      "Epoch 488/500\n",
      " - 0s - loss: 1.6268 - accuracy: 0.6957\n",
      "Epoch 489/500\n",
      " - 0s - loss: 1.6237 - accuracy: 0.8261\n",
      "Epoch 490/500\n",
      " - 0s - loss: 1.6241 - accuracy: 0.8261\n",
      "Epoch 491/500\n",
      " - 0s - loss: 1.6236 - accuracy: 0.7826\n",
      "Epoch 492/500\n",
      " - 0s - loss: 1.6216 - accuracy: 0.8696\n",
      "Epoch 493/500\n",
      " - 0s - loss: 1.6229 - accuracy: 0.7826\n",
      "Epoch 494/500\n",
      " - 0s - loss: 1.6211 - accuracy: 0.7826\n",
      "Epoch 495/500\n",
      " - 0s - loss: 1.6158 - accuracy: 0.8261\n",
      "Epoch 496/500\n",
      " - 0s - loss: 1.6182 - accuracy: 0.7826\n",
      "Epoch 497/500\n",
      " - 0s - loss: 1.6160 - accuracy: 0.7391\n",
      "Epoch 498/500\n",
      " - 0s - loss: 1.6178 - accuracy: 0.7826\n",
      "Epoch 499/500\n",
      " - 0s - loss: 1.6155 - accuracy: 0.8261\n",
      "Epoch 500/500\n",
      " - 0s - loss: 1.6114 - accuracy: 0.7391\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16023df9fd0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "# 在这里，我们将序列长度从1增加到3，例如: 我们把输入从一个字符升到三个字符。\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "# 将序列中的每个元素作为网络的一个新输入特性提供给它。这需要修改输入序列在数据准备步骤中的reshape:\n",
    "X = numpy.reshape(dataX, (len(dataX), 1, seq_length))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.61%\n",
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> T\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> X\n",
      "['U', 'V', 'W'] -> Z\n",
      "['V', 'W', 'X'] -> Z\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, 1, len(pattern)))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现有了一点点的提升，但是这一点点的提升未必是真的，梯度下降算法本来就是具有随机性的。\n",
    "\n",
    "也就是说我们再一次的错误使用了lstm循环神经网络。 \n",
    "我们确实给了上下文，但是并不是合适的方式， \n",
    "实际上，字母序列ABC才是一个特征的timesteps，而不是单独的一个特征的timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM的正确打开方式**\n",
    "\n",
    "利用lstm的关键是以时间序列(time steps)的方法来提供上下文，而不是像其他网络结构(CNN)一样，通过windowed features的方式。\n",
    "\n",
    "timesteps这个参数，我们设置了3，而不是前面的1。\n",
    "\n",
    "不同之处是，对输入数据的reshape是将输入序列作为一个特性的time step序列，而不是多个特性的单一time step。 \n",
    "也就是说我们把ABC 看成独立的一个特征组成的多个时间序列，而不是把ABC看成一个多个特征组成一个时间序列。 \n",
    "\n",
    "在训练 ABC——D的时候，BCD，CDE，都可以发挥作用。而最开始那种使用方法，只是利用了ABC——D这样一个训练样本。\n",
    "```py\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "```\n",
    "timesteps这个参数，我们设置了3，而不是前面的1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABC -> D\n",
      "BCD -> E\n",
      "CDE -> F\n",
      "DEF -> G\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "GHI -> J\n",
      "HIJ -> K\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "KLM -> N\n",
      "LMN -> O\n",
      "MNO -> P\n",
      "NOP -> Q\n",
      "OPQ -> R\n",
      "PQR -> S\n",
      "QRS -> T\n",
      "RST -> U\n",
      "STU -> V\n",
      "TUV -> W\n",
      "UVW -> X\n",
      "VWX -> Y\n",
      "WXY -> Z\n"
     ]
    }
   ],
   "source": [
    "# Naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23, 3, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 0s - loss: 3.2730 - accuracy: 0.0000e+00\n",
      "Epoch 2/500\n",
      " - 0s - loss: 3.2569 - accuracy: 0.0000e+00\n",
      "Epoch 3/500\n",
      " - 0s - loss: 3.2478 - accuracy: 0.0000e+00\n",
      "Epoch 4/500\n",
      " - 0s - loss: 3.2397 - accuracy: 0.0000e+00\n",
      "Epoch 5/500\n",
      " - 0s - loss: 3.2322 - accuracy: 0.0435\n",
      "Epoch 6/500\n",
      " - 0s - loss: 3.2233 - accuracy: 0.0435\n",
      "Epoch 7/500\n",
      " - 0s - loss: 3.2154 - accuracy: 0.0435\n",
      "Epoch 8/500\n",
      " - 0s - loss: 3.2065 - accuracy: 0.0435\n",
      "Epoch 9/500\n",
      " - 0s - loss: 3.1968 - accuracy: 0.0435\n",
      "Epoch 10/500\n",
      " - 0s - loss: 3.1865 - accuracy: 0.0000e+00\n",
      "Epoch 11/500\n",
      " - 0s - loss: 3.1754 - accuracy: 0.0435\n",
      "Epoch 12/500\n",
      " - 0s - loss: 3.1638 - accuracy: 0.0000e+00\n",
      "Epoch 13/500\n",
      " - 0s - loss: 3.1502 - accuracy: 0.0435\n",
      "Epoch 14/500\n",
      " - 0s - loss: 3.1373 - accuracy: 0.0435\n",
      "Epoch 15/500\n",
      " - 0s - loss: 3.1197 - accuracy: 0.0435\n",
      "Epoch 16/500\n",
      " - 0s - loss: 3.1031 - accuracy: 0.0435\n",
      "Epoch 17/500\n",
      " - 0s - loss: 3.0872 - accuracy: 0.0435\n",
      "Epoch 18/500\n",
      " - 0s - loss: 3.0720 - accuracy: 0.0435\n",
      "Epoch 19/500\n",
      " - 0s - loss: 3.0506 - accuracy: 0.0435\n",
      "Epoch 20/500\n",
      " - 0s - loss: 3.0317 - accuracy: 0.0435\n",
      "Epoch 21/500\n",
      " - 0s - loss: 3.0136 - accuracy: 0.0000e+00\n",
      "Epoch 22/500\n",
      " - 0s - loss: 2.9920 - accuracy: 0.0435\n",
      "Epoch 23/500\n",
      " - 0s - loss: 2.9622 - accuracy: 0.0435\n",
      "Epoch 24/500\n",
      " - 0s - loss: 2.9339 - accuracy: 0.0870\n",
      "Epoch 25/500\n",
      " - 0s - loss: 2.9051 - accuracy: 0.0870\n",
      "Epoch 26/500\n",
      " - 0s - loss: 2.8723 - accuracy: 0.0870\n",
      "Epoch 27/500\n",
      " - 0s - loss: 2.8345 - accuracy: 0.0870\n",
      "Epoch 28/500\n",
      " - 0s - loss: 2.7925 - accuracy: 0.0870\n",
      "Epoch 29/500\n",
      " - 0s - loss: 2.7526 - accuracy: 0.0435\n",
      "Epoch 30/500\n",
      " - 0s - loss: 2.7051 - accuracy: 0.0870\n",
      "Epoch 31/500\n",
      " - 0s - loss: 2.6634 - accuracy: 0.0870\n",
      "Epoch 32/500\n",
      " - 0s - loss: 2.6179 - accuracy: 0.0870\n",
      "Epoch 33/500\n",
      " - 0s - loss: 2.5790 - accuracy: 0.1304\n",
      "Epoch 34/500\n",
      " - 0s - loss: 2.5419 - accuracy: 0.1304\n",
      "Epoch 35/500\n",
      " - 0s - loss: 2.5035 - accuracy: 0.1304\n",
      "Epoch 36/500\n",
      " - 0s - loss: 2.4755 - accuracy: 0.0870\n",
      "Epoch 37/500\n",
      " - 0s - loss: 2.4326 - accuracy: 0.1304\n",
      "Epoch 38/500\n",
      " - 0s - loss: 2.4045 - accuracy: 0.1304\n",
      "Epoch 39/500\n",
      " - 0s - loss: 2.3620 - accuracy: 0.1739\n",
      "Epoch 40/500\n",
      " - 0s - loss: 2.3354 - accuracy: 0.1304\n",
      "Epoch 41/500\n",
      " - 0s - loss: 2.3057 - accuracy: 0.1739\n",
      "Epoch 42/500\n",
      " - 0s - loss: 2.2728 - accuracy: 0.1304\n",
      "Epoch 43/500\n",
      " - 0s - loss: 2.2526 - accuracy: 0.1739\n",
      "Epoch 44/500\n",
      " - 0s - loss: 2.2155 - accuracy: 0.2174\n",
      "Epoch 45/500\n",
      " - 0s - loss: 2.1874 - accuracy: 0.2174\n",
      "Epoch 46/500\n",
      " - 0s - loss: 2.1667 - accuracy: 0.2174\n",
      "Epoch 47/500\n",
      " - 0s - loss: 2.1470 - accuracy: 0.2609\n",
      "Epoch 48/500\n",
      " - 0s - loss: 2.1208 - accuracy: 0.2609\n",
      "Epoch 49/500\n",
      " - 0s - loss: 2.0982 - accuracy: 0.2174\n",
      "Epoch 50/500\n",
      " - 0s - loss: 2.0748 - accuracy: 0.2609\n",
      "Epoch 51/500\n",
      " - 0s - loss: 2.0526 - accuracy: 0.2609\n",
      "Epoch 52/500\n",
      " - 0s - loss: 2.0333 - accuracy: 0.2609\n",
      "Epoch 53/500\n",
      " - 0s - loss: 2.0063 - accuracy: 0.3043\n",
      "Epoch 54/500\n",
      " - 0s - loss: 1.9996 - accuracy: 0.3043\n",
      "Epoch 55/500\n",
      " - 0s - loss: 1.9867 - accuracy: 0.2174\n",
      "Epoch 56/500\n",
      " - 0s - loss: 1.9592 - accuracy: 0.3043\n",
      "Epoch 57/500\n",
      " - 0s - loss: 1.9461 - accuracy: 0.2609\n",
      "Epoch 58/500\n",
      " - 0s - loss: 1.9382 - accuracy: 0.3043\n",
      "Epoch 59/500\n",
      " - 0s - loss: 1.9221 - accuracy: 0.2609\n",
      "Epoch 60/500\n",
      " - 0s - loss: 1.9051 - accuracy: 0.2609\n",
      "Epoch 61/500\n",
      " - 0s - loss: 1.8889 - accuracy: 0.3913\n",
      "Epoch 62/500\n",
      " - 0s - loss: 1.8704 - accuracy: 0.3913\n",
      "Epoch 63/500\n",
      " - 0s - loss: 1.8629 - accuracy: 0.3478\n",
      "Epoch 64/500\n",
      " - 0s - loss: 1.8474 - accuracy: 0.4348\n",
      "Epoch 65/500\n",
      " - 0s - loss: 1.8329 - accuracy: 0.3913\n",
      "Epoch 66/500\n",
      " - 0s - loss: 1.8241 - accuracy: 0.3913\n",
      "Epoch 67/500\n",
      " - 0s - loss: 1.8169 - accuracy: 0.3478\n",
      "Epoch 68/500\n",
      " - 0s - loss: 1.7992 - accuracy: 0.3478\n",
      "Epoch 69/500\n",
      " - 0s - loss: 1.7836 - accuracy: 0.3913\n",
      "Epoch 70/500\n",
      " - 0s - loss: 1.7762 - accuracy: 0.4783\n",
      "Epoch 71/500\n",
      " - 0s - loss: 1.7615 - accuracy: 0.4783\n",
      "Epoch 72/500\n",
      " - 0s - loss: 1.7513 - accuracy: 0.4348\n",
      "Epoch 73/500\n",
      " - 0s - loss: 1.7474 - accuracy: 0.3913\n",
      "Epoch 74/500\n",
      " - 0s - loss: 1.7305 - accuracy: 0.5217\n",
      "Epoch 75/500\n",
      " - 0s - loss: 1.7178 - accuracy: 0.5217\n",
      "Epoch 76/500\n",
      " - 0s - loss: 1.7098 - accuracy: 0.4348\n",
      "Epoch 77/500\n",
      " - 0s - loss: 1.7069 - accuracy: 0.4348\n",
      "Epoch 78/500\n",
      " - 0s - loss: 1.6919 - accuracy: 0.5217\n",
      "Epoch 79/500\n",
      " - 0s - loss: 1.6799 - accuracy: 0.5217\n",
      "Epoch 80/500\n",
      " - 0s - loss: 1.6744 - accuracy: 0.5217\n",
      "Epoch 81/500\n",
      " - 0s - loss: 1.6672 - accuracy: 0.7391\n",
      "Epoch 82/500\n",
      " - 0s - loss: 1.6510 - accuracy: 0.5652\n",
      "Epoch 83/500\n",
      " - 0s - loss: 1.6459 - accuracy: 0.6522\n",
      "Epoch 84/500\n",
      " - 0s - loss: 1.6423 - accuracy: 0.6087\n",
      "Epoch 85/500\n",
      " - 0s - loss: 1.6267 - accuracy: 0.6087\n",
      "Epoch 86/500\n",
      " - 0s - loss: 1.6150 - accuracy: 0.6087\n",
      "Epoch 87/500\n",
      " - 0s - loss: 1.6081 - accuracy: 0.5652\n",
      "Epoch 88/500\n",
      " - 0s - loss: 1.6022 - accuracy: 0.5217\n",
      "Epoch 89/500\n",
      " - 0s - loss: 1.5892 - accuracy: 0.6957\n",
      "Epoch 90/500\n",
      " - 0s - loss: 1.5763 - accuracy: 0.6522\n",
      "Epoch 91/500\n",
      " - 0s - loss: 1.5780 - accuracy: 0.6522\n",
      "Epoch 92/500\n",
      " - 0s - loss: 1.5705 - accuracy: 0.6087\n",
      "Epoch 93/500\n",
      " - 0s - loss: 1.5675 - accuracy: 0.6087\n",
      "Epoch 94/500\n",
      " - 0s - loss: 1.5485 - accuracy: 0.6087\n",
      "Epoch 95/500\n",
      " - 0s - loss: 1.5440 - accuracy: 0.6957\n",
      "Epoch 96/500\n",
      " - 0s - loss: 1.5353 - accuracy: 0.6957\n",
      "Epoch 97/500\n",
      " - 0s - loss: 1.5266 - accuracy: 0.6957\n",
      "Epoch 98/500\n",
      " - 0s - loss: 1.5137 - accuracy: 0.6522\n",
      "Epoch 99/500\n",
      " - 0s - loss: 1.5092 - accuracy: 0.6957\n",
      "Epoch 100/500\n",
      " - 0s - loss: 1.5072 - accuracy: 0.6957\n",
      "Epoch 101/500\n",
      " - 0s - loss: 1.4972 - accuracy: 0.6957\n",
      "Epoch 102/500\n",
      " - 0s - loss: 1.4826 - accuracy: 0.6957\n",
      "Epoch 103/500\n",
      " - 0s - loss: 1.4776 - accuracy: 0.6957\n",
      "Epoch 104/500\n",
      " - 0s - loss: 1.4709 - accuracy: 0.6522\n",
      "Epoch 105/500\n",
      " - 0s - loss: 1.4698 - accuracy: 0.6522\n",
      "Epoch 106/500\n",
      " - 0s - loss: 1.4651 - accuracy: 0.6957\n",
      "Epoch 107/500\n",
      " - 0s - loss: 1.4605 - accuracy: 0.6957\n",
      "Epoch 108/500\n",
      " - 0s - loss: 1.4521 - accuracy: 0.6522\n",
      "Epoch 109/500\n",
      " - 0s - loss: 1.4335 - accuracy: 0.6957\n",
      "Epoch 110/500\n",
      " - 0s - loss: 1.4236 - accuracy: 0.7391\n",
      "Epoch 111/500\n",
      " - 0s - loss: 1.4175 - accuracy: 0.7391\n",
      "Epoch 112/500\n",
      " - 0s - loss: 1.4114 - accuracy: 0.8261\n",
      "Epoch 113/500\n",
      " - 0s - loss: 1.4165 - accuracy: 0.7391\n",
      "Epoch 114/500\n",
      " - 0s - loss: 1.3936 - accuracy: 0.7826\n",
      "Epoch 115/500\n",
      " - 0s - loss: 1.3971 - accuracy: 0.7391\n",
      "Epoch 116/500\n",
      " - 0s - loss: 1.3896 - accuracy: 0.7826\n",
      "Epoch 117/500\n",
      " - 0s - loss: 1.3774 - accuracy: 0.7826\n",
      "Epoch 118/500\n",
      " - 0s - loss: 1.3803 - accuracy: 0.7391\n",
      "Epoch 119/500\n",
      " - 0s - loss: 1.3624 - accuracy: 0.7826\n",
      "Epoch 120/500\n",
      " - 0s - loss: 1.3562 - accuracy: 0.7391\n",
      "Epoch 121/500\n",
      " - 0s - loss: 1.3549 - accuracy: 0.7391\n",
      "Epoch 122/500\n",
      " - 0s - loss: 1.3406 - accuracy: 0.7826\n",
      "Epoch 123/500\n",
      " - 0s - loss: 1.3328 - accuracy: 0.8261\n",
      "Epoch 124/500\n",
      " - 0s - loss: 1.3288 - accuracy: 0.7826\n",
      "Epoch 125/500\n",
      " - 0s - loss: 1.3277 - accuracy: 0.7391\n",
      "Epoch 126/500\n",
      " - 0s - loss: 1.3197 - accuracy: 0.8261\n",
      "Epoch 127/500\n",
      " - 0s - loss: 1.3132 - accuracy: 0.8261\n",
      "Epoch 128/500\n",
      " - 0s - loss: 1.3033 - accuracy: 0.7826\n",
      "Epoch 129/500\n",
      " - 0s - loss: 1.3020 - accuracy: 0.8261\n",
      "Epoch 130/500\n",
      " - 0s - loss: 1.2913 - accuracy: 0.8261\n",
      "Epoch 131/500\n",
      " - 0s - loss: 1.2912 - accuracy: 0.7826\n",
      "Epoch 132/500\n",
      " - 0s - loss: 1.2788 - accuracy: 0.8696\n",
      "Epoch 133/500\n",
      " - 0s - loss: 1.2814 - accuracy: 0.8696\n",
      "Epoch 134/500\n",
      " - 0s - loss: 1.2745 - accuracy: 0.9130\n",
      "Epoch 135/500\n",
      " - 0s - loss: 1.2704 - accuracy: 0.8261\n",
      "Epoch 136/500\n",
      " - 0s - loss: 1.2558 - accuracy: 0.8261\n",
      "Epoch 137/500\n",
      " - 0s - loss: 1.2503 - accuracy: 0.8261\n",
      "Epoch 138/500\n",
      " - 0s - loss: 1.2408 - accuracy: 0.8261\n",
      "Epoch 139/500\n",
      " - 0s - loss: 1.2458 - accuracy: 0.8696\n",
      "Epoch 140/500\n",
      " - 0s - loss: 1.2335 - accuracy: 0.8696\n",
      "Epoch 141/500\n",
      " - 0s - loss: 1.2320 - accuracy: 0.8261\n",
      "Epoch 142/500\n",
      " - 0s - loss: 1.2313 - accuracy: 0.8696\n",
      "Epoch 143/500\n",
      " - 0s - loss: 1.2108 - accuracy: 0.8261\n",
      "Epoch 144/500\n",
      " - 0s - loss: 1.2076 - accuracy: 0.8696\n",
      "Epoch 145/500\n",
      " - 0s - loss: 1.2088 - accuracy: 0.8696\n",
      "Epoch 146/500\n",
      " - 0s - loss: 1.1974 - accuracy: 0.9130\n",
      "Epoch 147/500\n",
      " - 0s - loss: 1.1956 - accuracy: 0.8261\n",
      "Epoch 148/500\n",
      " - 0s - loss: 1.1884 - accuracy: 0.8696\n",
      "Epoch 149/500\n",
      " - 0s - loss: 1.1859 - accuracy: 0.8696\n",
      "Epoch 150/500\n",
      " - 0s - loss: 1.1796 - accuracy: 0.8696\n",
      "Epoch 151/500\n",
      " - 0s - loss: 1.1756 - accuracy: 0.9130\n",
      "Epoch 152/500\n",
      " - 0s - loss: 1.1672 - accuracy: 0.8696\n",
      "Epoch 153/500\n",
      " - 0s - loss: 1.1602 - accuracy: 0.8261\n",
      "Epoch 154/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 1.1555 - accuracy: 0.9130\n",
      "Epoch 155/500\n",
      " - 0s - loss: 1.1553 - accuracy: 0.8261\n",
      "Epoch 156/500\n",
      " - 0s - loss: 1.1500 - accuracy: 0.8261\n",
      "Epoch 157/500\n",
      " - 0s - loss: 1.1399 - accuracy: 0.7826\n",
      "Epoch 158/500\n",
      " - 0s - loss: 1.1305 - accuracy: 0.8696\n",
      "Epoch 159/500\n",
      " - 0s - loss: 1.1288 - accuracy: 0.9130\n",
      "Epoch 160/500\n",
      " - 0s - loss: 1.1231 - accuracy: 0.9130\n",
      "Epoch 161/500\n",
      " - 0s - loss: 1.1162 - accuracy: 0.8696\n",
      "Epoch 162/500\n",
      " - 0s - loss: 1.1128 - accuracy: 0.8696\n",
      "Epoch 163/500\n",
      " - 0s - loss: 1.1015 - accuracy: 0.9130\n",
      "Epoch 164/500\n",
      " - 0s - loss: 1.0986 - accuracy: 0.9130\n",
      "Epoch 165/500\n",
      " - 0s - loss: 1.0938 - accuracy: 0.9130\n",
      "Epoch 166/500\n",
      " - 0s - loss: 1.0863 - accuracy: 0.9130\n",
      "Epoch 167/500\n",
      " - 0s - loss: 1.0905 - accuracy: 0.9130\n",
      "Epoch 168/500\n",
      " - 0s - loss: 1.0774 - accuracy: 0.9130\n",
      "Epoch 169/500\n",
      " - 0s - loss: 1.0720 - accuracy: 0.9130\n",
      "Epoch 170/500\n",
      " - 0s - loss: 1.0756 - accuracy: 0.8696\n",
      "Epoch 171/500\n",
      " - 0s - loss: 1.0636 - accuracy: 0.9130\n",
      "Epoch 172/500\n",
      " - 0s - loss: 1.0604 - accuracy: 0.9130\n",
      "Epoch 173/500\n",
      " - 0s - loss: 1.0479 - accuracy: 0.8696\n",
      "Epoch 174/500\n",
      " - 0s - loss: 1.0577 - accuracy: 0.8696\n",
      "Epoch 175/500\n",
      " - 0s - loss: 1.0446 - accuracy: 0.9130\n",
      "Epoch 176/500\n",
      " - 0s - loss: 1.0408 - accuracy: 0.8696\n",
      "Epoch 177/500\n",
      " - 0s - loss: 1.0350 - accuracy: 0.9130\n",
      "Epoch 178/500\n",
      " - 0s - loss: 1.0381 - accuracy: 0.8696\n",
      "Epoch 179/500\n",
      " - 0s - loss: 1.0240 - accuracy: 0.8696\n",
      "Epoch 180/500\n",
      " - 0s - loss: 1.0196 - accuracy: 0.8696\n",
      "Epoch 181/500\n",
      " - 0s - loss: 1.0207 - accuracy: 0.8261\n",
      "Epoch 182/500\n",
      " - 0s - loss: 1.0083 - accuracy: 0.8696\n",
      "Epoch 183/500\n",
      " - 0s - loss: 1.0085 - accuracy: 0.9130\n",
      "Epoch 184/500\n",
      " - 0s - loss: 1.0041 - accuracy: 0.9130\n",
      "Epoch 185/500\n",
      " - 0s - loss: 0.9977 - accuracy: 0.9130\n",
      "Epoch 186/500\n",
      " - 0s - loss: 0.9930 - accuracy: 0.8696\n",
      "Epoch 187/500\n",
      " - 0s - loss: 0.9927 - accuracy: 0.8696\n",
      "Epoch 188/500\n",
      " - 0s - loss: 0.9868 - accuracy: 0.9130\n",
      "Epoch 189/500\n",
      " - 0s - loss: 0.9805 - accuracy: 0.9130\n",
      "Epoch 190/500\n",
      " - 0s - loss: 0.9752 - accuracy: 0.9130\n",
      "Epoch 191/500\n",
      " - 0s - loss: 0.9705 - accuracy: 0.8696\n",
      "Epoch 192/500\n",
      " - 0s - loss: 0.9657 - accuracy: 0.9130\n",
      "Epoch 193/500\n",
      " - 0s - loss: 0.9664 - accuracy: 0.8696\n",
      "Epoch 194/500\n",
      " - 0s - loss: 0.9568 - accuracy: 0.8696\n",
      "Epoch 195/500\n",
      " - 0s - loss: 0.9514 - accuracy: 0.9130\n",
      "Epoch 196/500\n",
      " - 0s - loss: 0.9564 - accuracy: 0.8696\n",
      "Epoch 197/500\n",
      " - 0s - loss: 0.9431 - accuracy: 0.8696\n",
      "Epoch 198/500\n",
      " - 0s - loss: 0.9443 - accuracy: 0.8696\n",
      "Epoch 199/500\n",
      " - 0s - loss: 0.9366 - accuracy: 0.8696\n",
      "Epoch 200/500\n",
      " - 0s - loss: 0.9321 - accuracy: 0.9130\n",
      "Epoch 201/500\n",
      " - 0s - loss: 0.9240 - accuracy: 0.9130\n",
      "Epoch 202/500\n",
      " - 0s - loss: 0.9250 - accuracy: 0.8696\n",
      "Epoch 203/500\n",
      " - 0s - loss: 0.9245 - accuracy: 0.9130\n",
      "Epoch 204/500\n",
      " - 0s - loss: 0.9130 - accuracy: 0.8696\n",
      "Epoch 205/500\n",
      " - 0s - loss: 0.9231 - accuracy: 0.8696\n",
      "Epoch 206/500\n",
      " - 0s - loss: 0.9070 - accuracy: 0.9130\n",
      "Epoch 207/500\n",
      " - 0s - loss: 0.9000 - accuracy: 0.9130\n",
      "Epoch 208/500\n",
      " - 0s - loss: 0.9006 - accuracy: 0.8696\n",
      "Epoch 209/500\n",
      " - 0s - loss: 0.8987 - accuracy: 0.8696\n",
      "Epoch 210/500\n",
      " - 0s - loss: 0.8917 - accuracy: 0.8696\n",
      "Epoch 211/500\n",
      " - 0s - loss: 0.8870 - accuracy: 0.9565\n",
      "Epoch 212/500\n",
      " - 0s - loss: 0.8799 - accuracy: 0.9130\n",
      "Epoch 213/500\n",
      " - 0s - loss: 0.8740 - accuracy: 0.9565\n",
      "Epoch 214/500\n",
      " - 0s - loss: 0.8700 - accuracy: 0.9130\n",
      "Epoch 215/500\n",
      " - 0s - loss: 0.8703 - accuracy: 0.8696\n",
      "Epoch 216/500\n",
      " - 0s - loss: 0.8682 - accuracy: 0.8696\n",
      "Epoch 217/500\n",
      " - 0s - loss: 0.8643 - accuracy: 0.9565\n",
      "Epoch 218/500\n",
      " - 0s - loss: 0.8609 - accuracy: 0.8696\n",
      "Epoch 219/500\n",
      " - 0s - loss: 0.8502 - accuracy: 0.9130\n",
      "Epoch 220/500\n",
      " - 0s - loss: 0.8511 - accuracy: 0.8696\n",
      "Epoch 221/500\n",
      " - 0s - loss: 0.8439 - accuracy: 0.9130\n",
      "Epoch 222/500\n",
      " - 0s - loss: 0.8373 - accuracy: 0.9130\n",
      "Epoch 223/500\n",
      " - 0s - loss: 0.8372 - accuracy: 0.8696\n",
      "Epoch 224/500\n",
      " - 0s - loss: 0.8303 - accuracy: 0.9130\n",
      "Epoch 225/500\n",
      " - 0s - loss: 0.8327 - accuracy: 0.9130\n",
      "Epoch 226/500\n",
      " - 0s - loss: 0.8331 - accuracy: 0.8261\n",
      "Epoch 227/500\n",
      " - 0s - loss: 0.8302 - accuracy: 0.9130\n",
      "Epoch 228/500\n",
      " - 0s - loss: 0.8267 - accuracy: 0.9130\n",
      "Epoch 229/500\n",
      " - 0s - loss: 0.8243 - accuracy: 0.8696\n",
      "Epoch 230/500\n",
      " - 0s - loss: 0.8147 - accuracy: 0.9130\n",
      "Epoch 231/500\n",
      " - 0s - loss: 0.8106 - accuracy: 0.8261\n",
      "Epoch 232/500\n",
      " - 0s - loss: 0.8039 - accuracy: 0.9565\n",
      "Epoch 233/500\n",
      " - 0s - loss: 0.8042 - accuracy: 0.9130\n",
      "Epoch 234/500\n",
      " - 0s - loss: 0.7969 - accuracy: 0.9130\n",
      "Epoch 235/500\n",
      " - 0s - loss: 0.7955 - accuracy: 0.9130\n",
      "Epoch 236/500\n",
      " - 0s - loss: 0.7924 - accuracy: 0.9130\n",
      "Epoch 237/500\n",
      " - 0s - loss: 0.7853 - accuracy: 0.9565\n",
      "Epoch 238/500\n",
      " - 0s - loss: 0.7831 - accuracy: 0.9130\n",
      "Epoch 239/500\n",
      " - 0s - loss: 0.7834 - accuracy: 0.9130\n",
      "Epoch 240/500\n",
      " - 0s - loss: 0.7821 - accuracy: 0.8696\n",
      "Epoch 241/500\n",
      " - 0s - loss: 0.7793 - accuracy: 0.9565\n",
      "Epoch 242/500\n",
      " - 0s - loss: 0.7700 - accuracy: 0.9130\n",
      "Epoch 243/500\n",
      " - 0s - loss: 0.7640 - accuracy: 0.8696\n",
      "Epoch 244/500\n",
      " - 0s - loss: 0.7623 - accuracy: 0.9130\n",
      "Epoch 245/500\n",
      " - 0s - loss: 0.7548 - accuracy: 0.9130\n",
      "Epoch 246/500\n",
      " - 0s - loss: 0.7599 - accuracy: 0.9130\n",
      "Epoch 247/500\n",
      " - 0s - loss: 0.7496 - accuracy: 0.9565\n",
      "Epoch 248/500\n",
      " - 0s - loss: 0.7555 - accuracy: 0.8696\n",
      "Epoch 249/500\n",
      " - 0s - loss: 0.7466 - accuracy: 0.9565\n",
      "Epoch 250/500\n",
      " - 0s - loss: 0.7449 - accuracy: 0.9130\n",
      "Epoch 251/500\n",
      " - 0s - loss: 0.7363 - accuracy: 0.9130\n",
      "Epoch 252/500\n",
      " - 0s - loss: 0.7387 - accuracy: 0.9130\n",
      "Epoch 253/500\n",
      " - 0s - loss: 0.7303 - accuracy: 0.9565\n",
      "Epoch 254/500\n",
      " - 0s - loss: 0.7318 - accuracy: 0.8696\n",
      "Epoch 255/500\n",
      " - 0s - loss: 0.7199 - accuracy: 0.9130\n",
      "Epoch 256/500\n",
      " - 0s - loss: 0.7256 - accuracy: 0.9565\n",
      "Epoch 257/500\n",
      " - 0s - loss: 0.7225 - accuracy: 0.9130\n",
      "Epoch 258/500\n",
      " - 0s - loss: 0.7181 - accuracy: 0.8696\n",
      "Epoch 259/500\n",
      " - 0s - loss: 0.7145 - accuracy: 0.8696\n",
      "Epoch 260/500\n",
      " - 0s - loss: 0.7015 - accuracy: 0.9565\n",
      "Epoch 261/500\n",
      " - 0s - loss: 0.7037 - accuracy: 0.9565\n",
      "Epoch 262/500\n",
      " - 0s - loss: 0.6974 - accuracy: 0.9565\n",
      "Epoch 263/500\n",
      " - 0s - loss: 0.6945 - accuracy: 0.9130\n",
      "Epoch 264/500\n",
      " - 0s - loss: 0.7010 - accuracy: 0.9130\n",
      "Epoch 265/500\n",
      " - 0s - loss: 0.6939 - accuracy: 0.9130\n",
      "Epoch 266/500\n",
      " - 0s - loss: 0.6900 - accuracy: 0.9565\n",
      "Epoch 267/500\n",
      " - 0s - loss: 0.6896 - accuracy: 0.9565\n",
      "Epoch 268/500\n",
      " - 0s - loss: 0.6839 - accuracy: 0.9130\n",
      "Epoch 269/500\n",
      " - 0s - loss: 0.6816 - accuracy: 0.9130\n",
      "Epoch 270/500\n",
      " - 0s - loss: 0.6764 - accuracy: 0.9565\n",
      "Epoch 271/500\n",
      " - 0s - loss: 0.6744 - accuracy: 0.9130\n",
      "Epoch 272/500\n",
      " - 0s - loss: 0.6670 - accuracy: 0.9565\n",
      "Epoch 273/500\n",
      " - 0s - loss: 0.6642 - accuracy: 0.9130\n",
      "Epoch 274/500\n",
      " - 0s - loss: 0.6681 - accuracy: 0.9130\n",
      "Epoch 275/500\n",
      " - 0s - loss: 0.6573 - accuracy: 0.9565\n",
      "Epoch 276/500\n",
      " - 0s - loss: 0.6565 - accuracy: 0.9565\n",
      "Epoch 277/500\n",
      " - 0s - loss: 0.6525 - accuracy: 0.9130\n",
      "Epoch 278/500\n",
      " - 0s - loss: 0.6488 - accuracy: 0.9130\n",
      "Epoch 279/500\n",
      " - 0s - loss: 0.6502 - accuracy: 0.9565\n",
      "Epoch 280/500\n",
      " - 0s - loss: 0.6511 - accuracy: 0.9565\n",
      "Epoch 281/500\n",
      " - 0s - loss: 0.6456 - accuracy: 0.8696\n",
      "Epoch 282/500\n",
      " - 0s - loss: 0.6512 - accuracy: 0.9130\n",
      "Epoch 283/500\n",
      " - 0s - loss: 0.6444 - accuracy: 0.8696\n",
      "Epoch 284/500\n",
      " - 0s - loss: 0.6386 - accuracy: 0.9565\n",
      "Epoch 285/500\n",
      " - 0s - loss: 0.6337 - accuracy: 0.9565\n",
      "Epoch 286/500\n",
      " - 0s - loss: 0.6357 - accuracy: 0.9130\n",
      "Epoch 287/500\n",
      " - 0s - loss: 0.6279 - accuracy: 0.9130\n",
      "Epoch 288/500\n",
      " - 0s - loss: 0.6236 - accuracy: 0.9565\n",
      "Epoch 289/500\n",
      " - 0s - loss: 0.6131 - accuracy: 0.9565\n",
      "Epoch 290/500\n",
      " - 0s - loss: 0.6150 - accuracy: 0.9565\n",
      "Epoch 291/500\n",
      " - 0s - loss: 0.6122 - accuracy: 0.9565\n",
      "Epoch 292/500\n",
      " - 0s - loss: 0.6045 - accuracy: 0.9565\n",
      "Epoch 293/500\n",
      " - 0s - loss: 0.6070 - accuracy: 0.9130\n",
      "Epoch 294/500\n",
      " - 0s - loss: 0.6059 - accuracy: 0.9130\n",
      "Epoch 295/500\n",
      " - 0s - loss: 0.6048 - accuracy: 0.9130\n",
      "Epoch 296/500\n",
      " - 0s - loss: 0.6002 - accuracy: 0.9565\n",
      "Epoch 297/500\n",
      " - 0s - loss: 0.5993 - accuracy: 0.9565\n",
      "Epoch 298/500\n",
      " - 0s - loss: 0.5952 - accuracy: 0.9130\n",
      "Epoch 299/500\n",
      " - 0s - loss: 0.5926 - accuracy: 0.9565\n",
      "Epoch 300/500\n",
      " - 0s - loss: 0.5849 - accuracy: 0.9565\n",
      "Epoch 301/500\n",
      " - 0s - loss: 0.5908 - accuracy: 0.9565\n",
      "Epoch 302/500\n",
      " - 0s - loss: 0.5872 - accuracy: 0.9130\n",
      "Epoch 303/500\n",
      " - 0s - loss: 0.5839 - accuracy: 0.9130\n",
      "Epoch 304/500\n",
      " - 0s - loss: 0.5827 - accuracy: 0.9565\n",
      "Epoch 305/500\n",
      " - 0s - loss: 0.5875 - accuracy: 0.9565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/500\n",
      " - 0s - loss: 0.5748 - accuracy: 0.9565\n",
      "Epoch 307/500\n",
      " - 0s - loss: 0.5703 - accuracy: 0.9565\n",
      "Epoch 308/500\n",
      " - 0s - loss: 0.5731 - accuracy: 0.9565\n",
      "Epoch 309/500\n",
      " - 0s - loss: 0.5679 - accuracy: 0.9130\n",
      "Epoch 310/500\n",
      " - 0s - loss: 0.5673 - accuracy: 0.9565\n",
      "Epoch 311/500\n",
      " - 0s - loss: 0.5689 - accuracy: 1.0000\n",
      "Epoch 312/500\n",
      " - 0s - loss: 0.5600 - accuracy: 0.8696\n",
      "Epoch 313/500\n",
      " - 0s - loss: 0.5558 - accuracy: 0.9565\n",
      "Epoch 314/500\n",
      " - 0s - loss: 0.5498 - accuracy: 0.9130\n",
      "Epoch 315/500\n",
      " - 0s - loss: 0.5515 - accuracy: 0.9565\n",
      "Epoch 316/500\n",
      " - 0s - loss: 0.5503 - accuracy: 0.9565\n",
      "Epoch 317/500\n",
      " - 0s - loss: 0.5499 - accuracy: 0.9565\n",
      "Epoch 318/500\n",
      " - 0s - loss: 0.5496 - accuracy: 0.9565\n",
      "Epoch 319/500\n",
      " - 0s - loss: 0.5496 - accuracy: 0.9565\n",
      "Epoch 320/500\n",
      " - 0s - loss: 0.5393 - accuracy: 0.9130\n",
      "Epoch 321/500\n",
      " - 0s - loss: 0.5385 - accuracy: 0.9565\n",
      "Epoch 322/500\n",
      " - 0s - loss: 0.5358 - accuracy: 0.9565\n",
      "Epoch 323/500\n",
      " - 0s - loss: 0.5332 - accuracy: 0.9130\n",
      "Epoch 324/500\n",
      " - 0s - loss: 0.5299 - accuracy: 0.9130\n",
      "Epoch 325/500\n",
      " - 0s - loss: 0.5291 - accuracy: 0.9565\n",
      "Epoch 326/500\n",
      " - 0s - loss: 0.5334 - accuracy: 0.9565\n",
      "Epoch 327/500\n",
      " - 0s - loss: 0.5458 - accuracy: 0.9130\n",
      "Epoch 328/500\n",
      " - 0s - loss: 0.5272 - accuracy: 0.9130\n",
      "Epoch 329/500\n",
      " - 0s - loss: 0.5215 - accuracy: 0.8696\n",
      "Epoch 330/500\n",
      " - 0s - loss: 0.5163 - accuracy: 0.9565\n",
      "Epoch 331/500\n",
      " - 0s - loss: 0.5178 - accuracy: 0.9565\n",
      "Epoch 332/500\n",
      " - 0s - loss: 0.5102 - accuracy: 0.9565\n",
      "Epoch 333/500\n",
      " - 0s - loss: 0.5137 - accuracy: 0.9565\n",
      "Epoch 334/500\n",
      " - 0s - loss: 0.5131 - accuracy: 0.9130\n",
      "Epoch 335/500\n",
      " - 0s - loss: 0.5116 - accuracy: 1.0000\n",
      "Epoch 336/500\n",
      " - 0s - loss: 0.5020 - accuracy: 1.0000\n",
      "Epoch 337/500\n",
      " - 0s - loss: 0.4982 - accuracy: 0.9565\n",
      "Epoch 338/500\n",
      " - 0s - loss: 0.5013 - accuracy: 0.9565\n",
      "Epoch 339/500\n",
      " - 0s - loss: 0.5026 - accuracy: 0.9565\n",
      "Epoch 340/500\n",
      " - 0s - loss: 0.4966 - accuracy: 0.9130\n",
      "Epoch 341/500\n",
      " - 0s - loss: 0.4942 - accuracy: 0.9565\n",
      "Epoch 342/500\n",
      " - 0s - loss: 0.4887 - accuracy: 1.0000\n",
      "Epoch 343/500\n",
      " - 0s - loss: 0.4998 - accuracy: 0.9565\n",
      "Epoch 344/500\n",
      " - 0s - loss: 0.4896 - accuracy: 0.9130\n",
      "Epoch 345/500\n",
      " - 0s - loss: 0.4838 - accuracy: 0.9565\n",
      "Epoch 346/500\n",
      " - 0s - loss: 0.4836 - accuracy: 1.0000\n",
      "Epoch 347/500\n",
      " - 0s - loss: 0.5010 - accuracy: 0.8261\n",
      "Epoch 348/500\n",
      " - 0s - loss: 0.4826 - accuracy: 0.9565\n",
      "Epoch 349/500\n",
      " - 0s - loss: 0.4900 - accuracy: 0.9565\n",
      "Epoch 350/500\n",
      " - 0s - loss: 0.4748 - accuracy: 0.9565\n",
      "Epoch 351/500\n",
      " - 0s - loss: 0.4721 - accuracy: 1.0000\n",
      "Epoch 352/500\n",
      " - 0s - loss: 0.4783 - accuracy: 0.9565\n",
      "Epoch 353/500\n",
      " - 0s - loss: 0.4722 - accuracy: 0.9565\n",
      "Epoch 354/500\n",
      " - 0s - loss: 0.4664 - accuracy: 0.9565\n",
      "Epoch 355/500\n",
      " - 0s - loss: 0.4663 - accuracy: 0.9565\n",
      "Epoch 356/500\n",
      " - 0s - loss: 0.4629 - accuracy: 0.9565\n",
      "Epoch 357/500\n",
      " - 0s - loss: 0.4617 - accuracy: 1.0000\n",
      "Epoch 358/500\n",
      " - 0s - loss: 0.4627 - accuracy: 0.9130\n",
      "Epoch 359/500\n",
      " - 0s - loss: 0.4580 - accuracy: 1.0000\n",
      "Epoch 360/500\n",
      " - 0s - loss: 0.4513 - accuracy: 0.9130\n",
      "Epoch 361/500\n",
      " - 0s - loss: 0.4514 - accuracy: 0.9565\n",
      "Epoch 362/500\n",
      " - 0s - loss: 0.4572 - accuracy: 1.0000\n",
      "Epoch 363/500\n",
      " - 0s - loss: 0.4500 - accuracy: 1.0000\n",
      "Epoch 364/500\n",
      " - 0s - loss: 0.4469 - accuracy: 0.9565\n",
      "Epoch 365/500\n",
      " - 0s - loss: 0.4481 - accuracy: 0.9565\n",
      "Epoch 366/500\n",
      " - 0s - loss: 0.4467 - accuracy: 1.0000\n",
      "Epoch 367/500\n",
      " - 0s - loss: 0.4431 - accuracy: 0.9565\n",
      "Epoch 368/500\n",
      " - 0s - loss: 0.4395 - accuracy: 0.9565\n",
      "Epoch 369/500\n",
      " - 0s - loss: 0.4396 - accuracy: 1.0000\n",
      "Epoch 370/500\n",
      " - 0s - loss: 0.4409 - accuracy: 0.9565\n",
      "Epoch 371/500\n",
      " - 0s - loss: 0.4369 - accuracy: 1.0000\n",
      "Epoch 372/500\n",
      " - 0s - loss: 0.4374 - accuracy: 1.0000\n",
      "Epoch 373/500\n",
      " - 0s - loss: 0.4351 - accuracy: 1.0000\n",
      "Epoch 374/500\n",
      " - 0s - loss: 0.4307 - accuracy: 0.9565\n",
      "Epoch 375/500\n",
      " - 0s - loss: 0.4390 - accuracy: 0.9130\n",
      "Epoch 376/500\n",
      " - 0s - loss: 0.4325 - accuracy: 0.9565\n",
      "Epoch 377/500\n",
      " - 0s - loss: 0.4252 - accuracy: 1.0000\n",
      "Epoch 378/500\n",
      " - 0s - loss: 0.4248 - accuracy: 1.0000\n",
      "Epoch 379/500\n",
      " - 0s - loss: 0.4198 - accuracy: 1.0000\n",
      "Epoch 380/500\n",
      " - 0s - loss: 0.4216 - accuracy: 1.0000\n",
      "Epoch 381/500\n",
      " - 0s - loss: 0.4218 - accuracy: 1.0000\n",
      "Epoch 382/500\n",
      " - 0s - loss: 0.4225 - accuracy: 0.9565\n",
      "Epoch 383/500\n",
      " - 0s - loss: 0.4164 - accuracy: 1.0000\n",
      "Epoch 384/500\n",
      " - 0s - loss: 0.4127 - accuracy: 1.0000\n",
      "Epoch 385/500\n",
      " - 0s - loss: 0.4107 - accuracy: 1.0000\n",
      "Epoch 386/500\n",
      " - 0s - loss: 0.4174 - accuracy: 0.9565\n",
      "Epoch 387/500\n",
      " - 0s - loss: 0.4136 - accuracy: 0.9565\n",
      "Epoch 388/500\n",
      " - 0s - loss: 0.4081 - accuracy: 1.0000\n",
      "Epoch 389/500\n",
      " - 0s - loss: 0.4111 - accuracy: 0.9565\n",
      "Epoch 390/500\n",
      " - 0s - loss: 0.4103 - accuracy: 0.9565\n",
      "Epoch 391/500\n",
      " - 0s - loss: 0.4028 - accuracy: 1.0000\n",
      "Epoch 392/500\n",
      " - 0s - loss: 0.4030 - accuracy: 0.9565\n",
      "Epoch 393/500\n",
      " - 0s - loss: 0.4152 - accuracy: 0.9130\n",
      "Epoch 394/500\n",
      " - 0s - loss: 0.4025 - accuracy: 0.9565\n",
      "Epoch 395/500\n",
      " - 0s - loss: 0.4003 - accuracy: 1.0000\n",
      "Epoch 396/500\n",
      " - 0s - loss: 0.4016 - accuracy: 1.0000\n",
      "Epoch 397/500\n",
      " - 0s - loss: 0.4011 - accuracy: 1.0000\n",
      "Epoch 398/500\n",
      " - 0s - loss: 0.3923 - accuracy: 1.0000\n",
      "Epoch 399/500\n",
      " - 0s - loss: 0.3906 - accuracy: 1.0000\n",
      "Epoch 400/500\n",
      " - 0s - loss: 0.3902 - accuracy: 1.0000\n",
      "Epoch 401/500\n",
      " - 0s - loss: 0.3850 - accuracy: 1.0000\n",
      "Epoch 402/500\n",
      " - 0s - loss: 0.3809 - accuracy: 1.0000\n",
      "Epoch 403/500\n",
      " - 0s - loss: 0.3852 - accuracy: 0.9565\n",
      "Epoch 404/500\n",
      " - 0s - loss: 0.3848 - accuracy: 0.9565\n",
      "Epoch 405/500\n",
      " - 0s - loss: 0.3826 - accuracy: 1.0000\n",
      "Epoch 406/500\n",
      " - 0s - loss: 0.3849 - accuracy: 1.0000\n",
      "Epoch 407/500\n",
      " - 0s - loss: 0.3769 - accuracy: 1.0000\n",
      "Epoch 408/500\n",
      " - 0s - loss: 0.3747 - accuracy: 1.0000\n",
      "Epoch 409/500\n",
      " - 0s - loss: 0.3812 - accuracy: 0.9565\n",
      "Epoch 410/500\n",
      " - 0s - loss: 0.3802 - accuracy: 1.0000\n",
      "Epoch 411/500\n",
      " - 0s - loss: 0.3803 - accuracy: 1.0000\n",
      "Epoch 412/500\n",
      " - 0s - loss: 0.3739 - accuracy: 0.9565\n",
      "Epoch 413/500\n",
      " - 0s - loss: 0.3768 - accuracy: 1.0000\n",
      "Epoch 414/500\n",
      " - 0s - loss: 0.3828 - accuracy: 0.9565\n",
      "Epoch 415/500\n",
      " - 0s - loss: 0.3807 - accuracy: 0.9130\n",
      "Epoch 416/500\n",
      " - 0s - loss: 0.3759 - accuracy: 0.9565\n",
      "Epoch 417/500\n",
      " - 0s - loss: 0.3861 - accuracy: 0.9130\n",
      "Epoch 418/500\n",
      " - 0s - loss: 0.3673 - accuracy: 0.9565\n",
      "Epoch 419/500\n",
      " - 0s - loss: 0.3589 - accuracy: 1.0000\n",
      "Epoch 420/500\n",
      " - 0s - loss: 0.3646 - accuracy: 1.0000\n",
      "Epoch 421/500\n",
      " - 0s - loss: 0.3619 - accuracy: 1.0000\n",
      "Epoch 422/500\n",
      " - 0s - loss: 0.3565 - accuracy: 1.0000\n",
      "Epoch 423/500\n",
      " - 0s - loss: 0.3575 - accuracy: 1.0000\n",
      "Epoch 424/500\n",
      " - 0s - loss: 0.3558 - accuracy: 1.0000\n",
      "Epoch 425/500\n",
      " - 0s - loss: 0.3554 - accuracy: 1.0000\n",
      "Epoch 426/500\n",
      " - 0s - loss: 0.3483 - accuracy: 1.0000\n",
      "Epoch 427/500\n",
      " - 0s - loss: 0.3497 - accuracy: 0.9565\n",
      "Epoch 428/500\n",
      " - 0s - loss: 0.3515 - accuracy: 1.0000\n",
      "Epoch 429/500\n",
      " - 0s - loss: 0.3551 - accuracy: 1.0000\n",
      "Epoch 430/500\n",
      " - 0s - loss: 0.3527 - accuracy: 1.0000\n",
      "Epoch 431/500\n",
      " - 0s - loss: 0.3558 - accuracy: 1.0000\n",
      "Epoch 432/500\n",
      " - 0s - loss: 0.3582 - accuracy: 0.9565\n",
      "Epoch 433/500\n",
      " - 0s - loss: 0.3448 - accuracy: 0.9565\n",
      "Epoch 434/500\n",
      " - 0s - loss: 0.3388 - accuracy: 1.0000\n",
      "Epoch 435/500\n",
      " - 0s - loss: 0.3366 - accuracy: 1.0000\n",
      "Epoch 436/500\n",
      " - 0s - loss: 0.3350 - accuracy: 1.0000\n",
      "Epoch 437/500\n",
      " - 0s - loss: 0.3430 - accuracy: 1.0000\n",
      "Epoch 438/500\n",
      " - 0s - loss: 0.3374 - accuracy: 1.0000\n",
      "Epoch 439/500\n",
      " - 0s - loss: 0.3304 - accuracy: 1.0000\n",
      "Epoch 440/500\n",
      " - 0s - loss: 0.3348 - accuracy: 1.0000\n",
      "Epoch 441/500\n",
      " - 0s - loss: 0.3313 - accuracy: 1.0000\n",
      "Epoch 442/500\n",
      " - 0s - loss: 0.3376 - accuracy: 1.0000\n",
      "Epoch 443/500\n",
      " - 0s - loss: 0.3379 - accuracy: 1.0000\n",
      "Epoch 444/500\n",
      " - 0s - loss: 0.3320 - accuracy: 1.0000\n",
      "Epoch 445/500\n",
      " - 0s - loss: 0.3286 - accuracy: 1.0000\n",
      "Epoch 446/500\n",
      " - 0s - loss: 0.3308 - accuracy: 1.0000\n",
      "Epoch 447/500\n",
      " - 0s - loss: 0.3274 - accuracy: 1.0000\n",
      "Epoch 448/500\n",
      " - 0s - loss: 0.3229 - accuracy: 1.0000\n",
      "Epoch 449/500\n",
      " - 0s - loss: 0.3338 - accuracy: 1.0000\n",
      "Epoch 450/500\n",
      " - 0s - loss: 0.3254 - accuracy: 0.9565\n",
      "Epoch 451/500\n",
      " - 0s - loss: 0.3294 - accuracy: 1.0000\n",
      "Epoch 452/500\n",
      " - 0s - loss: 0.3388 - accuracy: 1.0000\n",
      "Epoch 453/500\n",
      " - 0s - loss: 0.3672 - accuracy: 0.9130\n",
      "Epoch 454/500\n",
      " - 0s - loss: 0.3448 - accuracy: 0.9565\n",
      "Epoch 455/500\n",
      " - 0s - loss: 0.3396 - accuracy: 1.0000\n",
      "Epoch 456/500\n",
      " - 0s - loss: 0.3382 - accuracy: 0.9565\n",
      "Epoch 457/500\n",
      " - 0s - loss: 0.3186 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 458/500\n",
      " - 0s - loss: 0.3205 - accuracy: 1.0000\n",
      "Epoch 459/500\n",
      " - 0s - loss: 0.3119 - accuracy: 1.0000\n",
      "Epoch 460/500\n",
      " - 0s - loss: 0.3101 - accuracy: 1.0000\n",
      "Epoch 461/500\n",
      " - 0s - loss: 0.3149 - accuracy: 1.0000\n",
      "Epoch 462/500\n",
      " - 0s - loss: 0.3118 - accuracy: 1.0000\n",
      "Epoch 463/500\n",
      " - 0s - loss: 0.3058 - accuracy: 1.0000\n",
      "Epoch 464/500\n",
      " - 0s - loss: 0.3081 - accuracy: 1.0000\n",
      "Epoch 465/500\n",
      " - 0s - loss: 0.3091 - accuracy: 1.0000\n",
      "Epoch 466/500\n",
      " - 0s - loss: 0.3057 - accuracy: 1.0000\n",
      "Epoch 467/500\n",
      " - 0s - loss: 0.3003 - accuracy: 1.0000\n",
      "Epoch 468/500\n",
      " - 0s - loss: 0.2979 - accuracy: 1.0000\n",
      "Epoch 469/500\n",
      " - 0s - loss: 0.2950 - accuracy: 1.0000\n",
      "Epoch 470/500\n",
      " - 0s - loss: 0.2970 - accuracy: 1.0000\n",
      "Epoch 471/500\n",
      " - 0s - loss: 0.2935 - accuracy: 1.0000\n",
      "Epoch 472/500\n",
      " - 0s - loss: 0.2907 - accuracy: 1.0000\n",
      "Epoch 473/500\n",
      " - 0s - loss: 0.2959 - accuracy: 1.0000\n",
      "Epoch 474/500\n",
      " - 0s - loss: 0.2938 - accuracy: 1.0000\n",
      "Epoch 475/500\n",
      " - 0s - loss: 0.3073 - accuracy: 1.0000\n",
      "Epoch 476/500\n",
      " - 0s - loss: 0.2968 - accuracy: 1.0000\n",
      "Epoch 477/500\n",
      " - 0s - loss: 0.2946 - accuracy: 1.0000\n",
      "Epoch 478/500\n",
      " - 0s - loss: 0.2863 - accuracy: 1.0000\n",
      "Epoch 479/500\n",
      " - 0s - loss: 0.2899 - accuracy: 1.0000\n",
      "Epoch 480/500\n",
      " - 0s - loss: 0.2880 - accuracy: 1.0000\n",
      "Epoch 481/500\n",
      " - 0s - loss: 0.2852 - accuracy: 1.0000\n",
      "Epoch 482/500\n",
      " - 0s - loss: 0.2858 - accuracy: 1.0000\n",
      "Epoch 483/500\n",
      " - 0s - loss: 0.2894 - accuracy: 1.0000\n",
      "Epoch 484/500\n",
      " - 0s - loss: 0.2791 - accuracy: 1.0000\n",
      "Epoch 485/500\n",
      " - 0s - loss: 0.2803 - accuracy: 1.0000\n",
      "Epoch 486/500\n",
      " - 0s - loss: 0.2833 - accuracy: 1.0000\n",
      "Epoch 487/500\n",
      " - 0s - loss: 0.2783 - accuracy: 1.0000\n",
      "Epoch 488/500\n",
      " - 0s - loss: 0.2870 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      " - 0s - loss: 0.2828 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      " - 0s - loss: 0.2790 - accuracy: 1.0000\n",
      "Epoch 491/500\n",
      " - 0s - loss: 0.2748 - accuracy: 1.0000\n",
      "Epoch 492/500\n",
      " - 0s - loss: 0.2759 - accuracy: 1.0000\n",
      "Epoch 493/500\n",
      " - 0s - loss: 0.2732 - accuracy: 1.0000\n",
      "Epoch 494/500\n",
      " - 0s - loss: 0.2731 - accuracy: 1.0000\n",
      "Epoch 495/500\n",
      " - 0s - loss: 0.2726 - accuracy: 1.0000\n",
      "Epoch 496/500\n",
      " - 0s - loss: 0.2741 - accuracy: 1.0000\n",
      "Epoch 497/500\n",
      " - 0s - loss: 0.2692 - accuracy: 1.0000\n",
      "Epoch 498/500\n",
      " - 0s - loss: 0.2775 - accuracy: 1.0000\n",
      "Epoch 499/500\n",
      " - 0s - loss: 0.2737 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      " - 0s - loss: 0.2695 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x16023f8f978>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the model\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n",
      "['A', 'B', 'C'] -> D\n",
      "['B', 'C', 'D'] -> E\n",
      "['C', 'D', 'E'] -> F\n",
      "['D', 'E', 'F'] -> G\n",
      "['E', 'F', 'G'] -> H\n",
      "['F', 'G', 'H'] -> I\n",
      "['G', 'H', 'I'] -> J\n",
      "['H', 'I', 'J'] -> K\n",
      "['I', 'J', 'K'] -> L\n",
      "['J', 'K', 'L'] -> M\n",
      "['K', 'L', 'M'] -> N\n",
      "['L', 'M', 'N'] -> O\n",
      "['M', 'N', 'O'] -> P\n",
      "['N', 'O', 'P'] -> Q\n",
      "['O', 'P', 'Q'] -> R\n",
      "['P', 'Q', 'R'] -> S\n",
      "['Q', 'R', 'S'] -> T\n",
      "['R', 'S', 'T'] -> U\n",
      "['S', 'T', 'U'] -> V\n",
      "['T', 'U', 'V'] -> W\n",
      "['U', 'V', 'W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['W', 'X', 'Y'] -> Z\n"
     ]
    }
   ],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "\tx = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "已经学会了用字母表中的三个字母来预测下一个字母的顺序。它可以显示字母表中的任意三个字母的随机序列，并预测下一个字母。\n",
    "\n",
    "LSTM网络是有状态的。它们应该能够学习整个字母表序列，但是在默认情况下，keras在每次训练之后重新设置网络状态。\n",
    "\n",
    "我们还没有展示出循环神经网络的强大之处，因为上面这个问题我们用多层感知器，足够多的神经元，足够多的迭代次数也可以很好的解决。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras实现的LSTM在每一个batch以后，都重置了LSTM的状态。\n",
    "\n",
    "这表明，如果我们的批处理大小足够容纳所有输入模式，如果所有输入模式都按顺序排序，LSTM就可以使用序列中的序列上下文来更好地学习序列。\n",
    "\n",
    "通过修改第一个示例来学习一对一映射，并将批处理大小从1增加到训练数据集的大小，我们可以很容易地演示这一点。\n",
    "\n",
    "此外，在每个epoch前，keras都重置了训练数据集。为了确保训练数据模式保持顺序，我们可以禁用这种洗牌。\n",
    "```py\n",
    "model.fit(X, y, epochs=500, batch_size=len(dataX), verbose=2, shuffle=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2833 - accuracy: 0.0400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2576 - accuracy: 0.0800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2425 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.2246 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1978 - accuracy: 0.1600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.1488 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0646 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9935 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9770 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 3.0761 - accuracy: 0.1600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9541 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.8619 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.7808 - accuracy: 0.2400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6971 - accuracy: 0.2000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5670 - accuracy: 0.2000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.5309 - accuracy: 0.2400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.4355 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3414 - accuracy: 0.2400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2224 - accuracy: 0.2800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2937 - accuracy: 0.1600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.3169 - accuracy: 0.2400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.9203 - accuracy: 0.1200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.6570 - accuracy: 0.2400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.2029 - accuracy: 0.2800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.1437 - accuracy: 0.2800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 2.0133 - accuracy: 0.4000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9398 - accuracy: 0.5200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.9026 - accuracy: 0.3600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8516 - accuracy: 0.4400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.8252 - accuracy: 0.4400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7743 - accuracy: 0.5200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7532 - accuracy: 0.4800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.7016 - accuracy: 0.6400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6978 - accuracy: 0.5200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6347 - accuracy: 0.6400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6670 - accuracy: 0.5200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5798 - accuracy: 0.6800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6494 - accuracy: 0.4800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5457 - accuracy: 0.6400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.6204 - accuracy: 0.4000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4957 - accuracy: 0.6400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.5498 - accuracy: 0.4800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4399 - accuracy: 0.6400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4648 - accuracy: 0.6000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3915 - accuracy: 0.7200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.4017 - accuracy: 0.6000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3485 - accuracy: 0.7200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3526 - accuracy: 0.7600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3082 - accuracy: 0.8000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.3077 - accuracy: 0.8000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2691 - accuracy: 0.8400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2687 - accuracy: 0.8400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2306 - accuracy: 0.8800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.2300 - accuracy: 0.8400\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1929 - accuracy: 0.8800\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1934 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1566 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1578 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1209 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.1198 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0868 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0832 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0539 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0451 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0224 - accuracy: 0.9200\n",
      "Epoch 1/1\n",
      " - 0s - loss: 1.0099 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9918 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9763 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9605 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9446 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9296 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.9140 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8989 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8837 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8688 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8540 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8392 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8245 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.8100 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7956 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7814 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7673 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7535 - accuracy: 0.9600\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7400 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7267 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7137 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.7008 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6883 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6759 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6637 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6517 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6399 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6282 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6168 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.6052 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5936 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5823 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5712 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5604 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5499 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5395 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5294 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5194 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5096 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.5001 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4907 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4815 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4724 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4636 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4549 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4464 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4380 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4298 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4217 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4138 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.4060 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3984 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3908 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3835 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3762 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3690 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3619 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3550 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3481 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3414 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3347 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3281 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3216 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3153 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3090 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.3027 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2966 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2906 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2846 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2787 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2729 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2671 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2614 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2558 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2502 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2447 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2393 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2339 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2287 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2235 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2185 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2135 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2087 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.2040 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1993 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1948 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1904 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1861 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1818 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1777 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1737 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1697 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1659 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1622 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1585 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1549 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      " - 0s - loss: 0.1515 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1481 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1448 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1415 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1384 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1353 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1323 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1294 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1265 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1237 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1209 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1182 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1156 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1131 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1106 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1081 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1058 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1035 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.1012 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0990 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0969 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0948 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0927 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0908 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0888 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0869 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0851 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0833 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0816 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0799 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0782 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0766 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0750 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0735 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0719 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0705 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0690 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0676 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0663 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0649 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0636 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0624 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0611 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0599 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0587 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0576 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0554 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0543 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0532 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0522 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0512 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0502 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0492 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0482 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0473 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0464 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0455 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0446 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0438 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0429 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0413 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0405 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0390 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0383 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0376 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0368 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0362 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0355 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0348 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0335 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0317 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0305 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0289 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0278 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0273 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0268 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0263 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0258 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0254 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0249 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0245 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0240 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0236 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0231 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0223 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0219 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0208 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0204 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0200 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0197 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0183 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0177 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0173 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0167 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0161 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0159 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0139 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 1/1\n",
      " - 0s - loss: 0.0103 - accuracy: 1.0000\n",
      "Model Accuracy: 100.00%\n",
      "A -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n",
      "F -> G\n",
      "G -> H\n",
      "H -> I\n",
      "I -> J\n",
      "J -> K\n",
      "K -> L\n",
      "L -> M\n",
      "M -> N\n",
      "N -> O\n",
      "O -> P\n",
      "P -> Q\n",
      "Q -> R\n",
      "R -> S\n",
      "S -> T\n",
      "T -> U\n",
      "U -> V\n",
      "V -> W\n",
      "W -> X\n",
      "X -> Y\n",
      "Y -> Z\n",
      "New start:  K\n",
      "K -> B\n",
      "B -> C\n",
      "C -> D\n",
      "D -> E\n",
      "E -> F\n"
     ]
    }
   ],
   "source": [
    "# Stateful LSTM to learn one-char to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 1\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "\tseq_in = alphabet[i:i + seq_length]\n",
    "\tseq_out = alphabet[i + seq_length]\n",
    "\tdataX.append([char_to_int[char] for char in seq_in])\n",
    "\tdataY.append(char_to_int[seq_out])\n",
    "\tprint(seq_in, '->', seq_out)\n",
    "    \n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (len(dataX), seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "for i in range(300):\n",
    "\tmodel.fit(X, y, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "\tmodel.reset_states()\n",
    "    \n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, batch_size=batch_size, verbose=0)\n",
    "model.reset_states()\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "seed = [char_to_int[alphabet[0]]]\n",
    "for i in range(0, len(alphabet)-1):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()\n",
    "\n",
    "# demonstrate a random starting point\n",
    "letter = \"K\"\n",
    "seed = [char_to_int[letter]]\n",
    "print(\"New start: \", letter)\n",
    "for i in range(0, 5):\n",
    "\tx = numpy.reshape(seed, (1, len(seed), 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tprint(int_to_char[seed[0]], \"->\", int_to_char[index])\n",
    "\tseed = [index]\n",
    "model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，网络已经完美地记住了整个字母表。它使用了样本的上下文，并学习了预测序列中下一个字符所需要的依赖关系。\n",
    "\n",
    "我们还可以看到，如果我们用第一个字母输入网络，它就能正确地对字母表的其他部分进行正确的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可变长度输入——单字符输出的LSTM\n",
    "\n",
    "在上一节中，我们发现keras的“有状态的”LSTM实际上只是重新播放第一个n序列的一个快捷方式，并没有真正学习一个通用的字母表模型。\n",
    "\n",
    "在这一节中，我们将探索一个“无状态”LSTM的变体，它学习了字母表中的随机子序列，并可以根据任意字母或字母序列去预测字母表中的下一个字母。\n",
    "\n",
    "首先，我们改变问题的框架。为了简化，我们定义一个最大的输入序列长度(maximum input sequence length)，并将其设置为5这样的小值来加速训练。这就定义了(用于训练的字母表的)子序列的最大长度。在扩展中，如果我们允许循环回到序列的开始，这就可以设置为完整的字母表(26)或更长。\n",
    "\n",
    "我们还需要定义要创建的随机序列的数量，在本例中为1000。这也可能是更多或更少。我希望实际需要的模式更少。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PQRST -> U\n",
      "W -> X\n",
      "O -> P\n",
      "OPQ -> R\n",
      "IJKLM -> N\n",
      "QRSTU -> V\n",
      "ABCD -> E\n",
      "X -> Y\n",
      "GHIJ -> K\n",
      "M -> N\n",
      "XY -> Z\n",
      "QRST -> U\n",
      "ABC -> D\n",
      "JKLMN -> O\n",
      "OP -> Q\n",
      "XY -> Z\n",
      "D -> E\n",
      "T -> U\n",
      "B -> C\n",
      "QRSTU -> V\n",
      "HIJ -> K\n",
      "JKLM -> N\n",
      "ABCDE -> F\n",
      "X -> Y\n",
      "V -> W\n",
      "DE -> F\n",
      "DEFG -> H\n",
      "BCDE -> F\n",
      "EFGH -> I\n",
      "BCDE -> F\n",
      "FG -> H\n",
      "RST -> U\n",
      "TUV -> W\n",
      "STUV -> W\n",
      "LMN -> O\n",
      "P -> Q\n",
      "MNOP -> Q\n",
      "JK -> L\n",
      "MNOP -> Q\n",
      "OPQRS -> T\n",
      "UVWXY -> Z\n",
      "PQRS -> T\n",
      "D -> E\n",
      "EFGH -> I\n",
      "IJK -> L\n",
      "WX -> Y\n",
      "STUV -> W\n",
      "MNOPQ -> R\n",
      "P -> Q\n",
      "WXY -> Z\n",
      "VWX -> Y\n",
      "V -> W\n",
      "HI -> J\n",
      "KLMNO -> P\n",
      "UV -> W\n",
      "JKL -> M\n",
      "ABCDE -> F\n",
      "WXY -> Z\n",
      "M -> N\n",
      "CDEF -> G\n",
      "KLMNO -> P\n",
      "RST -> U\n",
      "RS -> T\n",
      "W -> X\n",
      "J -> K\n",
      "WX -> Y\n",
      "JKLMN -> O\n",
      "MN -> O\n",
      "L -> M\n",
      "BCDE -> F\n",
      "TU -> V\n",
      "MNOPQ -> R\n",
      "NOPQR -> S\n",
      "HIJ -> K\n",
      "JKLM -> N\n",
      "STUVW -> X\n",
      "QRST -> U\n",
      "N -> O\n",
      "VWXY -> Z\n",
      "B -> C\n",
      "UVWX -> Y\n",
      "OP -> Q\n",
      "K -> L\n",
      "C -> D\n",
      "X -> Y\n",
      "ST -> U\n",
      "JKLM -> N\n",
      "B -> C\n",
      "QR -> S\n",
      "RS -> T\n",
      "VWXY -> Z\n",
      "S -> T\n",
      "NOP -> Q\n",
      "KLMNO -> P\n",
      "IJ -> K\n",
      "EF -> G\n",
      "MNOP -> Q\n",
      "WXY -> Z\n",
      "HI -> J\n",
      "P -> Q\n",
      "STUVW -> X\n",
      "Q -> R\n",
      "MN -> O\n",
      "O -> P\n",
      "C -> D\n",
      "L -> M\n",
      "JKLM -> N\n",
      "K -> L\n",
      "IJKLM -> N\n",
      "FGHIJ -> K\n",
      "LM -> N\n",
      "OPQ -> R\n",
      "U -> V\n",
      "HIJKL -> M\n",
      "PQR -> S\n",
      "S -> T\n",
      "OPQR -> S\n",
      "J -> K\n",
      "DE -> F\n",
      "K -> L\n",
      "BCDE -> F\n",
      "EFGH -> I\n",
      "RSTUV -> W\n",
      "LMNOP -> Q\n",
      "QR -> S\n",
      "ABCDE -> F\n",
      "LM -> N\n",
      "IJKLM -> N\n",
      "B -> C\n",
      "VWX -> Y\n",
      "MNOPQ -> R\n",
      "MNOPQ -> R\n",
      "LM -> N\n",
      "S -> T\n",
      "VWX -> Y\n",
      "WXY -> Z\n",
      "F -> G\n",
      "KLMNO -> P\n",
      "OPQ -> R\n",
      "M -> N\n",
      "X -> Y\n",
      "OPQRS -> T\n",
      "F -> G\n",
      "JKLMN -> O\n",
      "XY -> Z\n",
      "OPQ -> R\n",
      "FG -> H\n",
      "OP -> Q\n",
      "DEFGH -> I\n",
      "ABCD -> E\n",
      "VWX -> Y\n",
      "U -> V\n",
      "UV -> W\n",
      "VWX -> Y\n",
      "LMNO -> P\n",
      "E -> F\n",
      "NOPQ -> R\n",
      "HIJK -> L\n",
      "HIJ -> K\n",
      "DE -> F\n",
      "B -> C\n",
      "UVW -> X\n",
      "STUV -> W\n",
      "RST -> U\n",
      "H -> I\n",
      "I -> J\n",
      "MN -> O\n",
      "CDEF -> G\n",
      "ABC -> D\n",
      "RSTU -> V\n",
      "B -> C\n",
      "JKLM -> N\n",
      "TUVW -> X\n",
      "STUVW -> X\n",
      "C -> D\n",
      "UV -> W\n",
      "QRS -> T\n",
      "ABC -> D\n",
      "NOP -> Q\n",
      "W -> X\n",
      "DE -> F\n",
      "VWXY -> Z\n",
      "UV -> W\n",
      "JK -> L\n",
      "E -> F\n",
      "MNO -> P\n",
      "EFGH -> I\n",
      "PQRS -> T\n",
      "FGH -> I\n",
      "WXY -> Z\n",
      "OPQRS -> T\n",
      "TUV -> W\n",
      "MN -> O\n",
      "O -> P\n",
      "LMN -> O\n",
      "VWX -> Y\n",
      "QR -> S\n",
      "TUV -> W\n",
      "STU -> V\n",
      "EFGH -> I\n",
      "E -> F\n",
      "HIJ -> K\n",
      "QRS -> T\n",
      "H -> I\n",
      "K -> L\n",
      "E -> F\n",
      "UV -> W\n",
      "X -> Y\n",
      "QR -> S\n",
      "QRS -> T\n",
      "WXY -> Z\n",
      "S -> T\n",
      "CDEFG -> H\n",
      "PQRST -> U\n",
      "RST -> U\n",
      "A -> B\n",
      "CDEF -> G\n",
      "X -> Y\n",
      "JKLM -> N\n",
      "VWX -> Y\n",
      "N -> O\n",
      "W -> X\n",
      "TUVW -> X\n",
      "LMNOP -> Q\n",
      "EFG -> H\n",
      "HI -> J\n",
      "WXY -> Z\n",
      "IJK -> L\n",
      "R -> S\n",
      "H -> I\n",
      "V -> W\n",
      "OPQR -> S\n",
      "QRSTU -> V\n",
      "MNOPQ -> R\n",
      "Q -> R\n",
      "VWXY -> Z\n",
      "ABCDE -> F\n",
      "HIJK -> L\n",
      "FGHIJ -> K\n",
      "BC -> D\n",
      "UV -> W\n",
      "WXY -> Z\n",
      "VWX -> Y\n",
      "L -> M\n",
      "FG -> H\n",
      "E -> F\n",
      "WXY -> Z\n",
      "KLMN -> O\n",
      "B -> C\n",
      "QRSTU -> V\n",
      "X -> Y\n",
      "ST -> U\n",
      "GH -> I\n",
      "CDE -> F\n",
      "IJKLM -> N\n",
      "JKL -> M\n",
      "HIJ -> K\n",
      "UVWXY -> Z\n",
      "PQ -> R\n",
      "AB -> C\n",
      "HIJ -> K\n",
      "EFG -> H\n",
      "PQRS -> T\n",
      "BCDEF -> G\n",
      "IJKL -> M\n",
      "DEFGH -> I\n",
      "VW -> X\n",
      "XY -> Z\n",
      "OPQ -> R\n",
      "MN -> O\n",
      "OP -> Q\n",
      "WXY -> Z\n",
      "STU -> V\n",
      "LM -> N\n",
      "UV -> W\n",
      "EF -> G\n",
      "LMN -> O\n",
      "D -> E\n",
      "H -> I\n",
      "KLMNO -> P\n",
      "PQRST -> U\n",
      "V -> W\n",
      "M -> N\n",
      "UVW -> X\n",
      "ABCD -> E\n",
      "LM -> N\n",
      "A -> B\n",
      "DEFGH -> I\n",
      "IJK -> L\n",
      "OP -> Q\n",
      "WXY -> Z\n",
      "CDEFG -> H\n",
      "UVW -> X\n",
      "RS -> T\n",
      "FGHIJ -> K\n",
      "RST -> U\n",
      "NO -> P\n",
      "X -> Y\n",
      "RST -> U\n",
      "I -> J\n",
      "TUV -> W\n",
      "B -> C\n",
      "UVWX -> Y\n",
      "HIJKL -> M\n",
      "MNOPQ -> R\n",
      "ABC -> D\n",
      "PQ -> R\n",
      "WX -> Y\n",
      "XY -> Z\n",
      "UVW -> X\n",
      "IJKL -> M\n",
      "XY -> Z\n",
      "DEFG -> H\n",
      "H -> I\n",
      "Q -> R\n",
      "CDEFG -> H\n",
      "C -> D\n",
      "ABCD -> E\n",
      "LMN -> O\n",
      "PQRST -> U\n",
      "VWX -> Y\n",
      "M -> N\n",
      "KLMN -> O\n",
      "AB -> C\n",
      "NOPQ -> R\n",
      "F -> G\n",
      "NO -> P\n",
      "KLM -> N\n",
      "TUVWX -> Y\n",
      "U -> V\n",
      "CDEFG -> H\n",
      "FGHI -> J\n",
      "STUVW -> X\n",
      "JKLM -> N\n",
      "ABC -> D\n",
      "JKLMN -> O\n",
      "TUVWX -> Y\n",
      "D -> E\n",
      "EFGH -> I\n",
      "IJ -> K\n",
      "UVW -> X\n",
      "OPQR -> S\n",
      "N -> O\n",
      "VWXY -> Z\n",
      "ABC -> D\n",
      "J -> K\n",
      "RS -> T\n",
      "LMNOP -> Q\n",
      "BC -> D\n",
      "OPQ -> R\n",
      "JKLM -> N\n",
      "WX -> Y\n",
      "BCD -> E\n",
      "RSTU -> V\n",
      "GHI -> J\n",
      "O -> P\n",
      "R -> S\n",
      "QR -> S\n",
      "HIJKL -> M\n",
      "UVWXY -> Z\n",
      "CDEFG -> H\n",
      "OP -> Q\n",
      "HIJK -> L\n",
      "A -> B\n",
      "RST -> U\n",
      "QR -> S\n",
      "ABCD -> E\n",
      "LMN -> O\n",
      "TUV -> W\n",
      "MNO -> P\n",
      "AB -> C\n",
      "M -> N\n",
      "OPQR -> S\n",
      "STU -> V\n",
      "TUV -> W\n",
      "PQRST -> U\n",
      "LM -> N\n",
      "A -> B\n",
      "A -> B\n",
      "OPQ -> R\n",
      "HIJK -> L\n",
      "TU -> V\n",
      "QRS -> T\n",
      "WX -> Y\n",
      "BCD -> E\n",
      "ST -> U\n",
      "X -> Y\n",
      "EFGHI -> J\n",
      "E -> F\n",
      "FGHIJ -> K\n",
      "HI -> J\n",
      "ABC -> D\n",
      "NOPQ -> R\n",
      "HIJK -> L\n",
      "B -> C\n",
      "U -> V\n",
      "GH -> I\n",
      "TUVWX -> Y\n",
      "S -> T\n",
      "BCDEF -> G\n",
      "KLM -> N\n",
      "Q -> R\n",
      "CD -> E\n",
      "PQ -> R\n",
      "GH -> I\n",
      "U -> V\n",
      "RST -> U\n",
      "JKLM -> N\n",
      "FGH -> I\n",
      "IJ -> K\n",
      "O -> P\n",
      "X -> Y\n",
      "H -> I\n",
      "DEF -> G\n",
      "QRSTU -> V\n",
      "ABCD -> E\n",
      "IJK -> L\n",
      "GHI -> J\n",
      "QR -> S\n",
      "NOPQR -> S\n",
      "EF -> G\n",
      "PQRST -> U\n",
      "RST -> U\n",
      "X -> Y\n",
      "QR -> S\n",
      "HIJ -> K\n",
      "D -> E\n",
      "AB -> C\n",
      "N -> O\n",
      "QR -> S\n",
      "BCDEF -> G\n",
      "QRS -> T\n",
      "DEF -> G\n",
      "TUV -> W\n",
      "A -> B\n",
      "GHIJ -> K\n",
      "W -> X\n",
      "VWXY -> Z\n",
      "LM -> N\n",
      "OPQ -> R\n",
      "XY -> Z\n",
      "KLM -> N\n",
      "RST -> U\n",
      "OP -> Q\n",
      "VWX -> Y\n",
      "OPQ -> R\n",
      "N -> O\n",
      "M -> N\n",
      "JKL -> M\n",
      "OP -> Q\n",
      "DEF -> G\n",
      "BCD -> E\n",
      "K -> L\n",
      "MN -> O\n",
      "IJKL -> M\n",
      "QR -> S\n",
      "IJKLM -> N\n",
      "U -> V\n",
      "FGH -> I\n",
      "MNOPQ -> R\n",
      "TUVW -> X\n",
      "MN -> O\n",
      "RSTUV -> W\n",
      "VWX -> Y\n",
      "Q -> R\n",
      "DEFGH -> I\n",
      "NO -> P\n",
      "T -> U\n",
      "V -> W\n",
      "ST -> U\n",
      "DEFG -> H\n",
      "RS -> T\n",
      "NOPQ -> R\n",
      "GHIJK -> L\n",
      "QRSTU -> V\n",
      "LMNO -> P\n",
      "IJK -> L\n",
      "PQRST -> U\n",
      "IJK -> L\n",
      "DE -> F\n",
      "CD -> E\n",
      "JKLM -> N\n",
      "WX -> Y\n",
      "UV -> W\n",
      "W -> X\n",
      "KLM -> N\n",
      "PQ -> R\n",
      "W -> X\n",
      "WXY -> Z\n",
      "EFGHI -> J\n",
      "E -> F\n",
      "NOP -> Q\n",
      "VW -> X\n",
      "EFGHI -> J\n",
      "NO -> P\n",
      "HIJKL -> M\n",
      "UVWXY -> Z\n",
      "OPQ -> R\n",
      "P -> Q\n",
      "H -> I\n",
      "O -> P\n",
      "GHIJK -> L\n",
      "S -> T\n",
      "E -> F\n",
      "KLMN -> O\n",
      "TUVW -> X\n",
      "E -> F\n",
      "CDE -> F\n",
      "I -> J\n",
      "CDEF -> G\n",
      "F -> G\n",
      "ABCD -> E\n",
      "H -> I\n",
      "LMNOP -> Q\n",
      "V -> W\n",
      "W -> X\n",
      "BCD -> E\n",
      "TU -> V\n",
      "VWXY -> Z\n",
      "UVWX -> Y\n",
      "JKL -> M\n",
      "VW -> X\n",
      "CDEF -> G\n",
      "DEF -> G\n",
      "ABCDE -> F\n",
      "MNO -> P\n",
      "EFGH -> I\n",
      "JKLM -> N\n",
      "QR -> S\n",
      "ABCDE -> F\n",
      "OPQR -> S\n",
      "DEF -> G\n",
      "Q -> R\n",
      "TU -> V\n",
      "CDEFG -> H\n",
      "KLMN -> O\n",
      "VW -> X\n",
      "HIJKL -> M\n",
      "DE -> F\n",
      "OP -> Q\n",
      "I -> J\n",
      "GHIJK -> L\n",
      "HIJKL -> M\n",
      "I -> J\n",
      "AB -> C\n",
      "DE -> F\n",
      "I -> J\n",
      "O -> P\n",
      "HIJK -> L\n",
      "QR -> S\n",
      "MN -> O\n",
      "I -> J\n",
      "LM -> N\n",
      "VWXY -> Z\n",
      "JKLMN -> O\n",
      "BC -> D\n",
      "MN -> O\n",
      "GHIJ -> K\n",
      "KL -> M\n",
      "TU -> V\n",
      "QRST -> U\n",
      "ABCDE -> F\n",
      "GH -> I\n",
      "Q -> R\n",
      "NO -> P\n",
      "RST -> U\n",
      "BCDE -> F\n",
      "T -> U\n",
      "TUV -> W\n",
      "FGHIJ -> K\n",
      "T -> U\n",
      "BCD -> E\n",
      "NO -> P\n",
      "JK -> L\n",
      "BCD -> E\n",
      "G -> H\n",
      "A -> B\n",
      "GHIJK -> L\n",
      "QRSTU -> V\n",
      "AB -> C\n",
      "VW -> X\n",
      "HIJKL -> M\n",
      "FGHIJ -> K\n",
      "PQ -> R\n",
      "UV -> W\n",
      "F -> G\n",
      "A -> B\n",
      "Q -> R\n",
      "MNOP -> Q\n",
      "UVWXY -> Z\n",
      "GHIJK -> L\n",
      "GHIJK -> L\n",
      "BCDE -> F\n",
      "QRS -> T\n",
      "PQRS -> T\n",
      "PQ -> R\n",
      "HI -> J\n",
      "PQRST -> U\n",
      "OPQR -> S\n",
      "QRST -> U\n",
      "IJKLM -> N\n",
      "Q -> R\n",
      "F -> G\n",
      "QRST -> U\n",
      "ST -> U\n",
      "MN -> O\n",
      "CD -> E\n",
      "EFG -> H\n",
      "FGH -> I\n",
      "R -> S\n",
      "C -> D\n",
      "RSTUV -> W\n",
      "KL -> M\n",
      "HIJK -> L\n",
      "CD -> E\n",
      "FGHI -> J\n",
      "VW -> X\n",
      "P -> Q\n",
      "C -> D\n",
      "DE -> F\n",
      "DE -> F\n",
      "I -> J\n",
      "LMNOP -> Q\n",
      "KLMNO -> P\n",
      "QRS -> T\n",
      "F -> G\n",
      "UVWXY -> Z\n",
      "QRS -> T\n",
      "BCD -> E\n",
      "FG -> H\n",
      "ABCDE -> F\n",
      "U -> V\n",
      "M -> N\n",
      "KLMN -> O\n",
      "RST -> U\n",
      "UVWX -> Y\n",
      "X -> Y\n",
      "XY -> Z\n",
      "I -> J\n",
      "KLMN -> O\n",
      "X -> Y\n",
      "W -> X\n",
      "RSTUV -> W\n",
      "VW -> X\n",
      "XY -> Z\n",
      "T -> U\n",
      "CDE -> F\n",
      "FGHI -> J\n",
      "PQ -> R\n",
      "OPQRS -> T\n",
      "D -> E\n",
      "E -> F\n",
      "EFGH -> I\n",
      "GHIJK -> L\n",
      "L -> M\n",
      "KLMN -> O\n",
      "STU -> V\n",
      "EF -> G\n",
      "UV -> W\n",
      "K -> L\n",
      "QRS -> T\n",
      "QRSTU -> V\n",
      "DEF -> G\n",
      "UV -> W\n",
      "D -> E\n",
      "BC -> D\n",
      "OPQRS -> T\n",
      "EFGH -> I\n",
      "QRST -> U\n",
      "EF -> G\n",
      "RST -> U\n",
      "JKL -> M\n",
      "STU -> V\n",
      "UVWX -> Y\n",
      "EFGHI -> J\n",
      "JKLMN -> O\n",
      "P -> Q\n",
      "BCD -> E\n",
      "TU -> V\n",
      "O -> P\n",
      "RST -> U\n",
      "D -> E\n",
      "VWXY -> Z\n",
      "R -> S\n",
      "P -> Q\n",
      "CDE -> F\n",
      "X -> Y\n",
      "UVWXY -> Z\n",
      "DEFGH -> I\n",
      "NOP -> Q\n",
      "ABCD -> E\n",
      "B -> C\n",
      "BC -> D\n",
      "VW -> X\n",
      "E -> F\n",
      "TUVW -> X\n",
      "JKL -> M\n",
      "XY -> Z\n",
      "LM -> N\n",
      "PQRS -> T\n",
      "O -> P\n",
      "KLMN -> O\n",
      "STUV -> W\n",
      "K -> L\n",
      "UVWX -> Y\n",
      "U -> V\n",
      "HIJ -> K\n",
      "W -> X\n",
      "VWXY -> Z\n",
      "WX -> Y\n",
      "HIJ -> K\n",
      "O -> P\n",
      "QR -> S\n",
      "VWXY -> Z\n",
      "CD -> E\n",
      "KL -> M\n",
      "DEFGH -> I\n",
      "LMN -> O\n",
      "QRS -> T\n",
      "JKLMN -> O\n",
      "QR -> S\n",
      "CD -> E\n",
      "QRST -> U\n",
      "BCDEF -> G\n",
      "CDE -> F\n",
      "LMN -> O\n",
      "DEF -> G\n",
      "BCD -> E\n",
      "UV -> W\n",
      "STUVW -> X\n",
      "RS -> T\n",
      "ABCD -> E\n",
      "BCDEF -> G\n",
      "Q -> R\n",
      "UVWXY -> Z\n",
      "VW -> X\n",
      "VW -> X\n",
      "WXY -> Z\n",
      "NOPQR -> S\n",
      "V -> W\n",
      "LM -> N\n",
      "B -> C\n",
      "JKL -> M\n",
      "DE -> F\n",
      "K -> L\n",
      "ABC -> D\n",
      "E -> F\n",
      "STU -> V\n",
      "TU -> V\n",
      "G -> H\n",
      "AB -> C\n",
      "J -> K\n",
      "FGH -> I\n",
      "MNOP -> Q\n",
      "VW -> X\n",
      "CD -> E\n",
      "TUVWX -> Y\n",
      "F -> G\n",
      "VWX -> Y\n",
      "LMNO -> P\n",
      "GHIJ -> K\n",
      "TUVWX -> Y\n",
      "JKL -> M\n",
      "LM -> N\n",
      "EFGHI -> J\n",
      "MNO -> P\n",
      "H -> I\n",
      "M -> N\n",
      "S -> T\n",
      "STU -> V\n",
      "QRST -> U\n",
      "PQR -> S\n",
      "RSTUV -> W\n",
      "ST -> U\n",
      "RSTUV -> W\n",
      "JKLM -> N\n",
      "T -> U\n",
      "CDE -> F\n",
      "HIJ -> K\n",
      "NOPQ -> R\n",
      "OPQ -> R\n",
      "EF -> G\n",
      "AB -> C\n",
      "CD -> E\n",
      "RST -> U\n",
      "STU -> V\n",
      "L -> M\n",
      "WXY -> Z\n",
      "STUVW -> X\n",
      "QRST -> U\n",
      "W -> X\n",
      "S -> T\n",
      "M -> N\n",
      "GH -> I\n",
      "QRST -> U\n",
      "FGH -> I\n",
      "PQRS -> T\n",
      "GH -> I\n",
      "DE -> F\n",
      "DE -> F\n",
      "GHIJK -> L\n",
      "Q -> R\n",
      "WX -> Y\n",
      "WX -> Y\n",
      "KLM -> N\n",
      "DE -> F\n",
      "EF -> G\n",
      "UVW -> X\n",
      "IJK -> L\n",
      "NO -> P\n",
      "QR -> S\n",
      "TU -> V\n",
      "RST -> U\n",
      "VW -> X\n",
      "A -> B\n",
      "DE -> F\n",
      "WXY -> Z\n",
      "CD -> E\n",
      "IJK -> L\n",
      "STUV -> W\n",
      "LMNOP -> Q\n",
      "X -> Y\n",
      "FGH -> I\n",
      "F -> G\n",
      "IJK -> L\n",
      "EFG -> H\n",
      "DEFG -> H\n",
      "NOP -> Q\n",
      "FG -> H\n",
      "RSTU -> V\n",
      "E -> F\n",
      "WXY -> Z\n",
      "GH -> I\n",
      "CD -> E\n",
      "IJ -> K\n",
      "TUVWX -> Y\n",
      "EFGH -> I\n",
      "DEFGH -> I\n",
      "BCDE -> F\n",
      "STUV -> W\n",
      "HI -> J\n",
      "GH -> I\n",
      "STUVW -> X\n",
      "ABC -> D\n",
      "S -> T\n",
      "LMNOP -> Q\n",
      "UVWX -> Y\n",
      "PQ -> R\n",
      "CDEF -> G\n",
      "E -> F\n",
      "TU -> V\n",
      "TUVWX -> Y\n",
      "GHIJ -> K\n",
      "JK -> L\n",
      "IJK -> L\n",
      "G -> H\n",
      "EFG -> H\n",
      "TU -> V\n",
      "FGHI -> J\n",
      "W -> X\n",
      "T -> U\n",
      "CDE -> F\n",
      "XY -> Z\n",
      "XY -> Z\n",
      "CDE -> F\n",
      "N -> O\n",
      "QRST -> U\n",
      "FGHIJ -> K\n",
      "PQ -> R\n",
      "I -> J\n",
      "GH -> I\n",
      "F -> G\n",
      "VWX -> Y\n",
      "ABC -> D\n",
      "GH -> I\n",
      "KLMN -> O\n",
      "X -> Y\n",
      "Q -> R\n",
      "NOPQR -> S\n",
      "HIJ -> K\n",
      "IJ -> K\n",
      "C -> D\n",
      "FG -> H\n",
      "JKLMN -> O\n",
      "TU -> V\n",
      "NOPQR -> S\n",
      "O -> P\n",
      "TU -> V\n",
      "MNOPQ -> R\n",
      "PQ -> R\n",
      "S -> T\n",
      "VWXY -> Z\n",
      "VWXY -> Z\n",
      "CD -> E\n",
      "BCDEF -> G\n",
      "OPQ -> R\n",
      "LMNO -> P\n",
      "HIJKL -> M\n",
      "STU -> V\n",
      "GHI -> J\n",
      "UVWX -> Y\n",
      "NOPQ -> R\n",
      "HIJK -> L\n",
      "NOP -> Q\n",
      "Q -> R\n",
      "HIJ -> K\n",
      "W -> X\n",
      "QR -> S\n",
      "UVWX -> Y\n",
      "H -> I\n",
      "ABC -> D\n",
      "RSTUV -> W\n",
      "VW -> X\n",
      "OP -> Q\n",
      "RSTUV -> W\n",
      "ABC -> D\n",
      "ABC -> D\n",
      "GHIJ -> K\n",
      "WXY -> Z\n",
      "BCDE -> F\n",
      "N -> O\n",
      "JK -> L\n",
      "X -> Y\n",
      "TUV -> W\n",
      "L -> M\n",
      "F -> G\n",
      "MN -> O\n",
      "JKLMN -> O\n",
      "G -> H\n",
      "BCDEF -> G\n",
      "LMN -> O\n",
      "N -> O\n",
      "V -> W\n",
      "BCDEF -> G\n",
      "KLM -> N\n",
      "ST -> U\n",
      "TUV -> W\n",
      "MN -> O\n",
      "JKLM -> N\n",
      "LM -> N\n",
      "U -> V\n",
      "FGH -> I\n",
      "TUV -> W\n",
      "C -> D\n",
      "HIJK -> L\n",
      "UVWX -> Y\n",
      "W -> X\n",
      "QR -> S\n",
      "PQR -> S\n",
      "STUVW -> X\n",
      "RSTU -> V\n",
      "TU -> V\n",
      "RSTU -> V\n",
      "JKL -> M\n",
      "JKL -> M\n",
      "RSTUV -> W\n",
      "GHI -> J\n",
      "V -> W\n",
      "CD -> E\n",
      "QRSTU -> V\n",
      "M -> N\n",
      "BCDE -> F\n",
      "WX -> Y\n",
      "K -> L\n",
      "VW -> X\n",
      "GHI -> J\n",
      "CD -> E\n",
      "XY -> Z\n",
      "HI -> J\n",
      "C -> D\n",
      "IJK -> L\n",
      "DEFG -> H\n",
      "UV -> W\n",
      "LM -> N\n",
      "X -> Y\n",
      "UV -> W\n",
      "I -> J\n",
      "NO -> P\n",
      "ABCD -> E\n",
      "K -> L\n",
      "IJK -> L\n",
      "JKL -> M\n",
      "EFGHI -> J\n",
      "JK -> L\n",
      "TU -> V\n",
      "IJ -> K\n",
      "MNOPQ -> R\n",
      "C -> D\n",
      "IJKLM -> N\n",
      "VW -> X\n",
      "CDE -> F\n",
      "E -> F\n",
      "NOP -> Q\n",
      "OPQRS -> T\n",
      "FGHI -> J\n",
      "STUV -> W\n",
      "IJKLM -> N\n",
      "STUV -> W\n",
      "TUVWX -> Y\n",
      "RSTU -> V\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      " - 3s - loss: 3.0761 - accuracy: 0.0640\n",
      "Epoch 2/500\n",
      " - 2s - loss: 2.7741 - accuracy: 0.1300\n",
      "Epoch 3/500\n",
      " - 2s - loss: 2.4514 - accuracy: 0.2000\n",
      "Epoch 4/500\n",
      " - 2s - loss: 2.2189 - accuracy: 0.2580\n",
      "Epoch 5/500\n",
      " - 2s - loss: 2.0732 - accuracy: 0.3000\n",
      "Epoch 6/500\n",
      " - 2s - loss: 1.9519 - accuracy: 0.3240\n",
      "Epoch 7/500\n",
      " - 2s - loss: 1.8520 - accuracy: 0.3350\n",
      "Epoch 8/500\n",
      " - 2s - loss: 1.7668 - accuracy: 0.3630\n",
      "Epoch 9/500\n",
      " - 2s - loss: 1.6841 - accuracy: 0.4120\n",
      "Epoch 10/500\n",
      " - 2s - loss: 1.6135 - accuracy: 0.4360\n",
      "Epoch 11/500\n",
      " - 2s - loss: 1.5414 - accuracy: 0.4680\n",
      "Epoch 12/500\n",
      " - 2s - loss: 1.4839 - accuracy: 0.4930\n",
      "Epoch 13/500\n",
      " - 2s - loss: 1.4277 - accuracy: 0.5070\n",
      "Epoch 14/500\n",
      " - 2s - loss: 1.3793 - accuracy: 0.5360\n",
      "Epoch 15/500\n",
      " - 2s - loss: 1.3342 - accuracy: 0.5420\n",
      "Epoch 16/500\n",
      " - 2s - loss: 1.2852 - accuracy: 0.5710\n",
      "Epoch 17/500\n",
      " - 2s - loss: 1.2447 - accuracy: 0.5810\n",
      "Epoch 18/500\n",
      " - 2s - loss: 1.2081 - accuracy: 0.6120\n",
      "Epoch 19/500\n",
      " - 2s - loss: 1.1620 - accuracy: 0.6100\n",
      "Epoch 20/500\n",
      " - 2s - loss: 1.1325 - accuracy: 0.6380\n",
      "Epoch 21/500\n",
      " - 2s - loss: 1.1021 - accuracy: 0.6400\n",
      "Epoch 22/500\n",
      " - 2s - loss: 1.0808 - accuracy: 0.6540\n",
      "Epoch 23/500\n",
      " - 2s - loss: 1.0308 - accuracy: 0.6860\n",
      "Epoch 24/500\n",
      " - 2s - loss: 1.0153 - accuracy: 0.6880\n",
      "Epoch 25/500\n",
      " - 2s - loss: 0.9830 - accuracy: 0.6950\n",
      "Epoch 26/500\n",
      " - 2s - loss: 0.9600 - accuracy: 0.6900\n",
      "Epoch 27/500\n",
      " - 2s - loss: 0.9336 - accuracy: 0.7140\n",
      "Epoch 28/500\n",
      " - 2s - loss: 0.9061 - accuracy: 0.7380\n",
      "Epoch 29/500\n",
      " - 2s - loss: 0.8981 - accuracy: 0.7230\n",
      "Epoch 30/500\n",
      " - 2s - loss: 0.8710 - accuracy: 0.7310\n",
      "Epoch 31/500\n",
      " - 2s - loss: 0.8593 - accuracy: 0.7280\n",
      "Epoch 32/500\n",
      " - 2s - loss: 0.8194 - accuracy: 0.7700\n",
      "Epoch 33/500\n",
      " - 2s - loss: 0.8292 - accuracy: 0.7400\n",
      "Epoch 34/500\n",
      " - 2s - loss: 0.7994 - accuracy: 0.7430\n",
      "Epoch 35/500\n",
      " - 2s - loss: 0.7894 - accuracy: 0.7420\n",
      "Epoch 36/500\n",
      " - 2s - loss: 0.7654 - accuracy: 0.7680\n",
      "Epoch 37/500\n",
      " - 2s - loss: 0.7711 - accuracy: 0.7610\n",
      "Epoch 38/500\n",
      " - 2s - loss: 0.7266 - accuracy: 0.7910\n",
      "Epoch 39/500\n",
      " - 2s - loss: 0.7601 - accuracy: 0.7640\n",
      "Epoch 40/500\n",
      " - 2s - loss: 0.7250 - accuracy: 0.7720\n",
      "Epoch 41/500\n",
      " - 2s - loss: 0.7080 - accuracy: 0.7730\n",
      "Epoch 42/500\n",
      " - 2s - loss: 0.6889 - accuracy: 0.7900\n",
      "Epoch 43/500\n",
      " - 2s - loss: 0.6906 - accuracy: 0.7920\n",
      "Epoch 44/500\n",
      " - 2s - loss: 0.6669 - accuracy: 0.7950\n",
      "Epoch 45/500\n",
      " - 2s - loss: 0.6955 - accuracy: 0.7610\n",
      "Epoch 46/500\n",
      " - 2s - loss: 0.6489 - accuracy: 0.8030\n",
      "Epoch 47/500\n",
      " - 2s - loss: 0.6422 - accuracy: 0.7900\n",
      "Epoch 48/500\n",
      " - 2s - loss: 0.6390 - accuracy: 0.7960\n",
      "Epoch 49/500\n",
      " - 2s - loss: 0.6360 - accuracy: 0.7890\n",
      "Epoch 50/500\n",
      " - 2s - loss: 0.6157 - accuracy: 0.7970\n",
      "Epoch 51/500\n",
      " - 2s - loss: 0.6190 - accuracy: 0.8060\n",
      "Epoch 52/500\n",
      " - 2s - loss: 0.6049 - accuracy: 0.8010\n",
      "Epoch 53/500\n",
      " - 2s - loss: 0.5994 - accuracy: 0.8020\n",
      "Epoch 54/500\n",
      " - 2s - loss: 0.5843 - accuracy: 0.8170\n",
      "Epoch 55/500\n",
      " - 2s - loss: 0.5972 - accuracy: 0.7950\n",
      "Epoch 56/500\n",
      " - 2s - loss: 0.5856 - accuracy: 0.8030\n",
      "Epoch 57/500\n",
      " - 2s - loss: 0.5743 - accuracy: 0.8240\n",
      "Epoch 58/500\n",
      " - 2s - loss: 0.5547 - accuracy: 0.8230\n",
      "Epoch 59/500\n",
      " - 2s - loss: 0.5780 - accuracy: 0.8070\n",
      "Epoch 60/500\n",
      " - 2s - loss: 0.5461 - accuracy: 0.8290\n",
      "Epoch 61/500\n",
      " - 2s - loss: 0.5542 - accuracy: 0.8120\n",
      "Epoch 62/500\n",
      " - 2s - loss: 0.5476 - accuracy: 0.8250\n",
      "Epoch 63/500\n",
      " - 2s - loss: 0.5381 - accuracy: 0.8230\n",
      "Epoch 64/500\n",
      " - 2s - loss: 0.5291 - accuracy: 0.8260\n",
      "Epoch 65/500\n",
      " - 2s - loss: 0.5242 - accuracy: 0.8270\n",
      "Epoch 66/500\n",
      " - 2s - loss: 0.5392 - accuracy: 0.8150\n",
      "Epoch 67/500\n",
      " - 2s - loss: 0.5046 - accuracy: 0.8480\n",
      "Epoch 68/500\n",
      " - 2s - loss: 0.5036 - accuracy: 0.8390\n",
      "Epoch 69/500\n",
      " - 2s - loss: 0.5128 - accuracy: 0.8330\n",
      "Epoch 70/500\n",
      " - 2s - loss: 0.5218 - accuracy: 0.8150\n",
      "Epoch 71/500\n",
      " - 2s - loss: 0.4899 - accuracy: 0.8400\n",
      "Epoch 72/500\n",
      " - 2s - loss: 0.4860 - accuracy: 0.8380\n",
      "Epoch 73/500\n",
      " - 2s - loss: 0.4935 - accuracy: 0.8330\n",
      "Epoch 74/500\n",
      " - 2s - loss: 0.4920 - accuracy: 0.8390\n",
      "Epoch 75/500\n",
      " - 2s - loss: 0.4999 - accuracy: 0.8400\n",
      "Epoch 76/500\n",
      " - 2s - loss: 0.4635 - accuracy: 0.8550\n",
      "Epoch 77/500\n",
      " - 2s - loss: 0.4639 - accuracy: 0.8530\n",
      "Epoch 78/500\n",
      " - 2s - loss: 0.5205 - accuracy: 0.8210\n",
      "Epoch 79/500\n",
      " - 2s - loss: 0.4596 - accuracy: 0.8540\n",
      "Epoch 80/500\n",
      " - 2s - loss: 0.5334 - accuracy: 0.8200\n",
      "Epoch 81/500\n",
      " - 2s - loss: 0.4435 - accuracy: 0.8630\n",
      "Epoch 82/500\n",
      " - 2s - loss: 0.4406 - accuracy: 0.8560\n",
      "Epoch 83/500\n",
      " - 2s - loss: 0.4666 - accuracy: 0.8460\n",
      "Epoch 84/500\n",
      " - 2s - loss: 0.4420 - accuracy: 0.8570\n",
      "Epoch 85/500\n",
      " - 2s - loss: 0.4406 - accuracy: 0.8590\n",
      "Epoch 86/500\n",
      " - 2s - loss: 0.4543 - accuracy: 0.8480\n",
      "Epoch 87/500\n",
      " - 2s - loss: 0.4527 - accuracy: 0.8590\n",
      "Epoch 88/500\n",
      " - 2s - loss: 0.4256 - accuracy: 0.8640\n",
      "Epoch 89/500\n",
      " - 2s - loss: 0.4412 - accuracy: 0.8580\n",
      "Epoch 90/500\n",
      " - 2s - loss: 0.5319 - accuracy: 0.8350\n",
      "Epoch 91/500\n",
      " - 2s - loss: 0.4126 - accuracy: 0.8780\n",
      "Epoch 92/500\n",
      " - 2s - loss: 0.4366 - accuracy: 0.8730\n",
      "Epoch 93/500\n",
      " - 2s - loss: 0.4180 - accuracy: 0.8630\n",
      "Epoch 94/500\n",
      " - 2s - loss: 0.4238 - accuracy: 0.8620\n",
      "Epoch 95/500\n",
      " - 2s - loss: 0.4067 - accuracy: 0.8690\n",
      "Epoch 96/500\n",
      " - 2s - loss: 0.4136 - accuracy: 0.8690\n",
      "Epoch 97/500\n",
      " - 2s - loss: 0.4376 - accuracy: 0.8590\n",
      "Epoch 98/500\n",
      " - 2s - loss: 0.4270 - accuracy: 0.8610\n",
      "Epoch 99/500\n",
      " - 2s - loss: 0.3975 - accuracy: 0.8720\n",
      "Epoch 100/500\n",
      " - 2s - loss: 0.4030 - accuracy: 0.8570\n",
      "Epoch 101/500\n",
      " - 2s - loss: 0.4197 - accuracy: 0.8630\n",
      "Epoch 102/500\n",
      " - 2s - loss: 0.4724 - accuracy: 0.8400\n",
      "Epoch 103/500\n",
      " - 2s - loss: 0.3856 - accuracy: 0.8870\n",
      "Epoch 104/500\n",
      " - 2s - loss: 0.3893 - accuracy: 0.8730\n",
      "Epoch 105/500\n",
      " - 2s - loss: 0.4171 - accuracy: 0.8620\n",
      "Epoch 106/500\n",
      " - 2s - loss: 0.3755 - accuracy: 0.8880\n",
      "Epoch 107/500\n",
      " - 2s - loss: 0.4593 - accuracy: 0.8490\n",
      "Epoch 108/500\n",
      " - 2s - loss: 0.3706 - accuracy: 0.8880\n",
      "Epoch 109/500\n",
      " - 2s - loss: 0.3760 - accuracy: 0.8750\n",
      "Epoch 110/500\n",
      " - 2s - loss: 0.3816 - accuracy: 0.8700\n",
      "Epoch 111/500\n",
      " - 2s - loss: 0.4061 - accuracy: 0.8680\n",
      "Epoch 112/500\n",
      " - 2s - loss: 0.3753 - accuracy: 0.8840\n",
      "Epoch 113/500\n",
      " - 2s - loss: 0.3799 - accuracy: 0.8800\n",
      "Epoch 114/500\n",
      " - 2s - loss: 0.3647 - accuracy: 0.8930\n",
      "Epoch 115/500\n",
      " - 2s - loss: 0.3721 - accuracy: 0.8820\n",
      "Epoch 116/500\n",
      " - 2s - loss: 0.4082 - accuracy: 0.8650\n",
      "Epoch 117/500\n",
      " - 2s - loss: 0.3618 - accuracy: 0.8880\n",
      "Epoch 118/500\n",
      " - 2s - loss: 0.3637 - accuracy: 0.8840\n",
      "Epoch 119/500\n",
      " - 2s - loss: 0.3579 - accuracy: 0.8930\n",
      "Epoch 120/500\n",
      " - 2s - loss: 0.4408 - accuracy: 0.8670\n",
      "Epoch 121/500\n",
      " - 2s - loss: 0.3448 - accuracy: 0.8950\n",
      "Epoch 122/500\n",
      " - 2s - loss: 0.3494 - accuracy: 0.8970\n",
      "Epoch 123/500\n",
      " - 2s - loss: 0.3757 - accuracy: 0.8770\n",
      "Epoch 124/500\n",
      " - 2s - loss: 0.4145 - accuracy: 0.8820\n",
      "Epoch 125/500\n",
      " - 2s - loss: 0.3390 - accuracy: 0.8980\n",
      "Epoch 126/500\n",
      " - 2s - loss: 0.3488 - accuracy: 0.8840\n",
      "Epoch 127/500\n",
      " - 2s - loss: 0.3466 - accuracy: 0.8910\n",
      "Epoch 128/500\n",
      " - 2s - loss: 0.3944 - accuracy: 0.8750\n",
      "Epoch 129/500\n",
      " - 2s - loss: 0.3413 - accuracy: 0.8950\n",
      "Epoch 130/500\n",
      " - 2s - loss: 0.3533 - accuracy: 0.8870\n",
      "Epoch 131/500\n",
      " - 2s - loss: 0.3724 - accuracy: 0.8830\n",
      "Epoch 132/500\n",
      " - 2s - loss: 0.3264 - accuracy: 0.9080\n",
      "Epoch 133/500\n",
      " - 2s - loss: 0.3389 - accuracy: 0.8840\n",
      "Epoch 134/500\n",
      " - 2s - loss: 0.3389 - accuracy: 0.9060\n",
      "Epoch 135/500\n",
      " - 2s - loss: 0.3490 - accuracy: 0.8960\n",
      "Epoch 136/500\n",
      " - 2s - loss: 0.3766 - accuracy: 0.8820\n",
      "Epoch 137/500\n",
      " - 2s - loss: 0.3206 - accuracy: 0.9000\n",
      "Epoch 138/500\n",
      " - 2s - loss: 0.3242 - accuracy: 0.9030\n",
      "Epoch 139/500\n",
      " - 2s - loss: 0.3260 - accuracy: 0.9070\n",
      "Epoch 140/500\n",
      " - 2s - loss: 0.3982 - accuracy: 0.8720\n",
      "Epoch 141/500\n",
      " - 2s - loss: 0.3167 - accuracy: 0.9030\n",
      "Epoch 142/500\n",
      " - 2s - loss: 0.3324 - accuracy: 0.8900\n",
      "Epoch 143/500\n",
      " - 2s - loss: 0.3110 - accuracy: 0.9160\n",
      "Epoch 144/500\n",
      " - 2s - loss: 0.3232 - accuracy: 0.8960\n",
      "Epoch 145/500\n",
      " - 2s - loss: 0.4186 - accuracy: 0.8630\n",
      "Epoch 146/500\n",
      " - 2s - loss: 0.3112 - accuracy: 0.9080\n",
      "Epoch 147/500\n",
      " - 2s - loss: 0.3083 - accuracy: 0.9030\n",
      "Epoch 148/500\n",
      " - 2s - loss: 0.3111 - accuracy: 0.8960\n",
      "Epoch 149/500\n",
      " - 2s - loss: 0.3122 - accuracy: 0.9000\n",
      "Epoch 150/500\n",
      " - 2s - loss: 0.3726 - accuracy: 0.8800\n",
      "Epoch 151/500\n",
      " - 2s - loss: 0.3010 - accuracy: 0.9130\n",
      "Epoch 152/500\n",
      " - 2s - loss: 0.3058 - accuracy: 0.9010\n",
      "Epoch 153/500\n",
      " - 2s - loss: 0.3084 - accuracy: 0.9010\n",
      "Epoch 154/500\n",
      " - 2s - loss: 0.3057 - accuracy: 0.9030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/500\n",
      " - 2s - loss: 0.4370 - accuracy: 0.8730\n",
      "Epoch 156/500\n",
      " - 2s - loss: 0.3119 - accuracy: 0.9130\n",
      "Epoch 157/500\n",
      " - 2s - loss: 0.2931 - accuracy: 0.9070\n",
      "Epoch 158/500\n",
      " - 2s - loss: 0.3044 - accuracy: 0.9010\n",
      "Epoch 159/500\n",
      " - 2s - loss: 0.2999 - accuracy: 0.8990\n",
      "Epoch 160/500\n",
      " - 2s - loss: 0.3469 - accuracy: 0.9010\n",
      "Epoch 161/500\n",
      " - 2s - loss: 0.2963 - accuracy: 0.9040\n",
      "Epoch 162/500\n",
      " - 2s - loss: 0.2878 - accuracy: 0.9150\n",
      "Epoch 163/500\n",
      " - 2s - loss: 0.3779 - accuracy: 0.8900\n",
      "Epoch 164/500\n",
      " - 2s - loss: 0.2881 - accuracy: 0.9100\n",
      "Epoch 165/500\n",
      " - 2s - loss: 0.2869 - accuracy: 0.9130\n",
      "Epoch 166/500\n",
      " - 2s - loss: 0.3054 - accuracy: 0.9050\n",
      "Epoch 167/500\n",
      " - 2s - loss: 0.2918 - accuracy: 0.9070\n",
      "Epoch 168/500\n",
      " - 2s - loss: 0.4181 - accuracy: 0.8800\n",
      "Epoch 169/500\n",
      " - 2s - loss: 0.2796 - accuracy: 0.9170\n",
      "Epoch 170/500\n",
      " - 2s - loss: 0.2823 - accuracy: 0.9130\n",
      "Epoch 171/500\n",
      " - 2s - loss: 0.2827 - accuracy: 0.9100\n",
      "Epoch 172/500\n",
      " - 2s - loss: 0.3052 - accuracy: 0.8990\n",
      "Epoch 173/500\n",
      " - 2s - loss: 0.2798 - accuracy: 0.9070\n",
      "Epoch 174/500\n",
      " - 2s - loss: 0.2813 - accuracy: 0.9100\n",
      "Epoch 175/500\n",
      " - 2s - loss: 0.3571 - accuracy: 0.8890\n",
      "Epoch 176/500\n",
      " - 2s - loss: 0.2744 - accuracy: 0.9210\n",
      "Epoch 177/500\n",
      " - 2s - loss: 0.2750 - accuracy: 0.9140\n",
      "Epoch 178/500\n",
      " - 2s - loss: 0.2742 - accuracy: 0.9220\n",
      "Epoch 179/500\n",
      " - 2s - loss: 0.2846 - accuracy: 0.9110\n",
      "Epoch 180/500\n",
      " - 2s - loss: 0.2740 - accuracy: 0.9070\n",
      "Epoch 181/500\n",
      " - 2s - loss: 0.2772 - accuracy: 0.9040\n",
      "Epoch 182/500\n",
      " - 2s - loss: 0.3819 - accuracy: 0.8900\n",
      "Epoch 183/500\n",
      " - 2s - loss: 0.2673 - accuracy: 0.9160\n",
      "Epoch 184/500\n",
      " - 2s - loss: 0.2691 - accuracy: 0.9200\n",
      "Epoch 185/500\n",
      " - 2s - loss: 0.2700 - accuracy: 0.9050\n",
      "Epoch 186/500\n",
      " - 2s - loss: 0.2850 - accuracy: 0.9100\n",
      "Epoch 187/500\n",
      " - 2s - loss: 0.2645 - accuracy: 0.9230\n",
      "Epoch 188/500\n",
      " - 2s - loss: 0.2758 - accuracy: 0.9130\n",
      "Epoch 189/500\n",
      " - 2s - loss: 0.2661 - accuracy: 0.9140\n",
      "Epoch 190/500\n",
      " - 2s - loss: 0.2681 - accuracy: 0.9230\n",
      "Epoch 191/500\n",
      " - 2s - loss: 0.3388 - accuracy: 0.9040\n",
      "Epoch 192/500\n",
      " - 2s - loss: 0.2591 - accuracy: 0.9200\n",
      "Epoch 193/500\n",
      " - 2s - loss: 0.2591 - accuracy: 0.9200\n",
      "Epoch 194/500\n",
      " - 2s - loss: 0.2631 - accuracy: 0.9110\n",
      "Epoch 195/500\n",
      " - 2s - loss: 0.2614 - accuracy: 0.9300\n",
      "Epoch 196/500\n",
      " - 2s - loss: 0.2615 - accuracy: 0.9160\n",
      "Epoch 197/500\n",
      " - 2s - loss: 0.3786 - accuracy: 0.8950\n",
      "Epoch 198/500\n",
      " - 2s - loss: 0.2524 - accuracy: 0.9240\n",
      "Epoch 199/500\n",
      " - 2s - loss: 0.2522 - accuracy: 0.9230\n",
      "Epoch 200/500\n",
      " - 2s - loss: 0.2521 - accuracy: 0.9200\n",
      "Epoch 201/500\n",
      " - 2s - loss: 0.2515 - accuracy: 0.9150\n",
      "Epoch 202/500\n",
      " - 2s - loss: 0.3660 - accuracy: 0.8950\n",
      "Epoch 203/500\n",
      " - 2s - loss: 0.2459 - accuracy: 0.9340\n",
      "Epoch 204/500\n",
      " - 2s - loss: 0.2491 - accuracy: 0.9360\n",
      "Epoch 205/500\n",
      " - 2s - loss: 0.2557 - accuracy: 0.9260\n",
      "Epoch 206/500\n",
      " - 2s - loss: 0.2535 - accuracy: 0.9290\n",
      "Epoch 207/500\n",
      " - 2s - loss: 0.2467 - accuracy: 0.9310\n",
      "Epoch 208/500\n",
      " - 2s - loss: 0.3831 - accuracy: 0.8970\n",
      "Epoch 209/500\n",
      " - 2s - loss: 0.2642 - accuracy: 0.9320\n",
      "Epoch 210/500\n",
      " - 2s - loss: 0.2426 - accuracy: 0.9220\n",
      "Epoch 211/500\n",
      " - 2s - loss: 0.2442 - accuracy: 0.9280\n",
      "Epoch 212/500\n",
      " - 2s - loss: 0.2454 - accuracy: 0.9290\n",
      "Epoch 213/500\n",
      " - 2s - loss: 0.2718 - accuracy: 0.9110\n",
      "Epoch 214/500\n",
      " - 2s - loss: 0.3171 - accuracy: 0.9170\n",
      "Epoch 215/500\n",
      " - 2s - loss: 0.2394 - accuracy: 0.9380\n",
      "Epoch 216/500\n",
      " - 2s - loss: 0.2394 - accuracy: 0.9360\n",
      "Epoch 217/500\n",
      " - 2s - loss: 0.2406 - accuracy: 0.9310\n",
      "Epoch 218/500\n",
      " - 2s - loss: 0.2484 - accuracy: 0.9280\n",
      "Epoch 219/500\n",
      " - 2s - loss: 0.2400 - accuracy: 0.9310\n",
      "Epoch 220/500\n",
      " - 2s - loss: 0.2443 - accuracy: 0.9290\n",
      "Epoch 221/500\n",
      " - 2s - loss: 0.2377 - accuracy: 0.9320\n",
      "Epoch 222/500\n",
      " - 2s - loss: 0.3174 - accuracy: 0.9170\n",
      "Epoch 223/500\n",
      " - 2s - loss: 0.2537 - accuracy: 0.9360\n",
      "Epoch 224/500\n",
      " - 2s - loss: 0.2324 - accuracy: 0.9340\n",
      "Epoch 225/500\n",
      " - 2s - loss: 0.2334 - accuracy: 0.9360\n",
      "Epoch 226/500\n",
      " - 2s - loss: 0.2357 - accuracy: 0.9270\n",
      "Epoch 227/500\n",
      " - 2s - loss: 0.2932 - accuracy: 0.9050\n",
      "Epoch 228/500\n",
      " - 2s - loss: 0.2295 - accuracy: 0.9410\n",
      "Epoch 229/500\n",
      " - 2s - loss: 0.2254 - accuracy: 0.9380\n",
      "Epoch 230/500\n",
      " - 2s - loss: 0.2303 - accuracy: 0.9320\n",
      "Epoch 231/500\n",
      " - 2s - loss: 0.2296 - accuracy: 0.9340\n",
      "Epoch 232/500\n",
      " - 2s - loss: 0.2502 - accuracy: 0.9290\n",
      "Epoch 233/500\n",
      " - 2s - loss: 0.2276 - accuracy: 0.9350\n",
      "Epoch 234/500\n",
      " - 2s - loss: 0.2285 - accuracy: 0.9300\n",
      "Epoch 235/500\n",
      " - 2s - loss: 0.2262 - accuracy: 0.9310\n",
      "Epoch 236/500\n",
      " - 2s - loss: 0.2958 - accuracy: 0.9160\n",
      "Epoch 237/500\n",
      " - 2s - loss: 0.2205 - accuracy: 0.9430\n",
      "Epoch 238/500\n",
      " - 2s - loss: 0.2233 - accuracy: 0.9350\n",
      "Epoch 239/500\n",
      " - 2s - loss: 0.2179 - accuracy: 0.9360\n",
      "Epoch 240/500\n",
      " - 2s - loss: 0.2241 - accuracy: 0.9300\n",
      "Epoch 241/500\n",
      " - 2s - loss: 0.2212 - accuracy: 0.9350\n",
      "Epoch 242/500\n",
      " - 2s - loss: 0.3308 - accuracy: 0.9110\n",
      "Epoch 243/500\n",
      " - 2s - loss: 0.2156 - accuracy: 0.9460\n",
      "Epoch 244/500\n",
      " - 2s - loss: 0.2155 - accuracy: 0.9410\n",
      "Epoch 245/500\n",
      " - 2s - loss: 0.2205 - accuracy: 0.9340\n",
      "Epoch 246/500\n",
      " - 2s - loss: 0.2162 - accuracy: 0.9340\n",
      "Epoch 247/500\n",
      " - 2s - loss: 0.2787 - accuracy: 0.9310\n",
      "Epoch 248/500\n",
      " - 2s - loss: 0.2187 - accuracy: 0.9420\n",
      "Epoch 249/500\n",
      " - 2s - loss: 0.2112 - accuracy: 0.9420\n",
      "Epoch 250/500\n",
      " - 2s - loss: 0.2122 - accuracy: 0.9400\n",
      "Epoch 251/500\n",
      " - 2s - loss: 0.2147 - accuracy: 0.9400\n",
      "Epoch 252/500\n",
      " - 2s - loss: 0.2132 - accuracy: 0.9370\n",
      "Epoch 253/500\n",
      " - 2s - loss: 0.3188 - accuracy: 0.9160\n",
      "Epoch 254/500\n",
      " - 2s - loss: 0.2065 - accuracy: 0.9450\n",
      "Epoch 255/500\n",
      " - 2s - loss: 0.2066 - accuracy: 0.9500\n",
      "Epoch 256/500\n",
      " - 2s - loss: 0.2091 - accuracy: 0.9440\n",
      "Epoch 257/500\n",
      " - 2s - loss: 0.2150 - accuracy: 0.9360\n",
      "Epoch 258/500\n",
      " - 2s - loss: 0.2115 - accuracy: 0.9440\n",
      "Epoch 259/500\n",
      " - 2s - loss: 0.2844 - accuracy: 0.9190\n",
      "Epoch 260/500\n",
      " - 2s - loss: 0.2122 - accuracy: 0.9470\n",
      "Epoch 261/500\n",
      " - 2s - loss: 0.2051 - accuracy: 0.9420\n",
      "Epoch 262/500\n",
      " - 2s - loss: 0.2035 - accuracy: 0.9400\n",
      "Epoch 263/500\n",
      " - 2s - loss: 0.2062 - accuracy: 0.9420\n",
      "Epoch 264/500\n",
      " - 2s - loss: 0.2043 - accuracy: 0.9380\n",
      "Epoch 265/500\n",
      " - 2s - loss: 0.3405 - accuracy: 0.9180\n",
      "Epoch 266/500\n",
      " - 2s - loss: 0.2400 - accuracy: 0.9470\n",
      "Epoch 267/500\n",
      " - 2s - loss: 0.1945 - accuracy: 0.9460\n",
      "Epoch 268/500\n",
      " - 2s - loss: 0.1974 - accuracy: 0.9480\n",
      "Epoch 269/500\n",
      " - 2s - loss: 0.2003 - accuracy: 0.9380\n",
      "Epoch 270/500\n",
      " - 2s - loss: 0.2002 - accuracy: 0.9430\n",
      "Epoch 271/500\n",
      " - 2s - loss: 0.2020 - accuracy: 0.9420\n",
      "Epoch 272/500\n",
      " - 2s - loss: 0.2609 - accuracy: 0.9230\n",
      "Epoch 273/500\n",
      " - 2s - loss: 0.1965 - accuracy: 0.9470\n",
      "Epoch 274/500\n",
      " - 2s - loss: 0.1968 - accuracy: 0.9490\n",
      "Epoch 275/500\n",
      " - 2s - loss: 0.1984 - accuracy: 0.9450\n",
      "Epoch 276/500\n",
      " - 2s - loss: 0.1988 - accuracy: 0.9480\n",
      "Epoch 277/500\n",
      " - 2s - loss: 0.2372 - accuracy: 0.9330\n",
      "Epoch 278/500\n",
      " - 2s - loss: 0.1937 - accuracy: 0.9470\n",
      "Epoch 279/500\n",
      " - 2s - loss: 0.1956 - accuracy: 0.9480\n",
      "Epoch 280/500\n",
      " - 2s - loss: 0.1970 - accuracy: 0.9370\n",
      "Epoch 281/500\n",
      " - 2s - loss: 0.1960 - accuracy: 0.9510\n",
      "Epoch 282/500\n",
      " - 2s - loss: 0.2960 - accuracy: 0.9260\n",
      "Epoch 283/500\n",
      " - 2s - loss: 0.1901 - accuracy: 0.9480\n",
      "Epoch 284/500\n",
      " - 2s - loss: 0.1915 - accuracy: 0.9530\n",
      "Epoch 285/500\n",
      " - 2s - loss: 0.1925 - accuracy: 0.9500\n",
      "Epoch 286/500\n",
      " - 2s - loss: 0.1946 - accuracy: 0.9500\n",
      "Epoch 287/500\n",
      " - 2s - loss: 0.1915 - accuracy: 0.9490\n",
      "Epoch 288/500\n",
      " - 2s - loss: 0.1937 - accuracy: 0.9430\n",
      "Epoch 289/500\n",
      " - 2s - loss: 0.2609 - accuracy: 0.9320\n",
      "Epoch 290/500\n",
      " - 2s - loss: 0.1807 - accuracy: 0.9540\n",
      "Epoch 291/500\n",
      " - 2s - loss: 0.1883 - accuracy: 0.9480\n",
      "Epoch 292/500\n",
      " - 2s - loss: 0.1880 - accuracy: 0.9430\n",
      "Epoch 293/500\n",
      " - 2s - loss: 0.1905 - accuracy: 0.9500\n",
      "Epoch 294/500\n",
      " - 2s - loss: 0.3534 - accuracy: 0.9180\n",
      "Epoch 295/500\n",
      " - 2s - loss: 0.1801 - accuracy: 0.9540\n",
      "Epoch 296/500\n",
      " - 2s - loss: 0.1852 - accuracy: 0.9510\n",
      "Epoch 297/500\n",
      " - 2s - loss: 0.1802 - accuracy: 0.9520\n",
      "Epoch 298/500\n",
      " - 2s - loss: 0.1858 - accuracy: 0.9550\n",
      "Epoch 299/500\n",
      " - 2s - loss: 0.1845 - accuracy: 0.9480\n",
      "Epoch 300/500\n",
      " - 2s - loss: 0.1841 - accuracy: 0.9460\n",
      "Epoch 301/500\n",
      " - 2s - loss: 0.1829 - accuracy: 0.9500\n",
      "Epoch 302/500\n",
      " - 2s - loss: 0.1802 - accuracy: 0.9490\n",
      "Epoch 303/500\n",
      " - 2s - loss: 0.2517 - accuracy: 0.9440\n",
      "Epoch 304/500\n",
      " - 2s - loss: 0.1896 - accuracy: 0.9480\n",
      "Epoch 305/500\n",
      " - 2s - loss: 0.1793 - accuracy: 0.9500\n",
      "Epoch 306/500\n",
      " - 2s - loss: 0.1797 - accuracy: 0.9520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 307/500\n",
      " - 2s - loss: 0.1768 - accuracy: 0.9510\n",
      "Epoch 308/500\n",
      " - 2s - loss: 0.1786 - accuracy: 0.9540\n",
      "Epoch 309/500\n",
      " - 2s - loss: 0.2592 - accuracy: 0.9280\n",
      "Epoch 310/500\n",
      " - 2s - loss: 0.1728 - accuracy: 0.9580\n",
      "Epoch 311/500\n",
      " - 2s - loss: 0.1713 - accuracy: 0.9570\n",
      "Epoch 312/500\n",
      " - 2s - loss: 0.1763 - accuracy: 0.9490\n",
      "Epoch 313/500\n",
      " - 2s - loss: 0.1760 - accuracy: 0.9500\n",
      "Epoch 314/500\n",
      " - 2s - loss: 0.1762 - accuracy: 0.9540\n",
      "Epoch 315/500\n",
      " - 2s - loss: 0.1742 - accuracy: 0.9490\n",
      "Epoch 316/500\n",
      " - 2s - loss: 0.2729 - accuracy: 0.9340\n",
      "Epoch 317/500\n",
      " - 2s - loss: 0.1686 - accuracy: 0.9600\n",
      "Epoch 318/500\n",
      " - 2s - loss: 0.1687 - accuracy: 0.9590\n",
      "Epoch 319/500\n",
      " - 2s - loss: 0.1708 - accuracy: 0.9590\n",
      "Epoch 320/500\n",
      " - 2s - loss: 0.1718 - accuracy: 0.9490\n",
      "Epoch 321/500\n",
      " - 2s - loss: 0.1747 - accuracy: 0.9490\n",
      "Epoch 322/500\n",
      " - 2s - loss: 0.3392 - accuracy: 0.9330\n",
      "Epoch 323/500\n",
      " - 2s - loss: 0.1895 - accuracy: 0.9590\n",
      "Epoch 324/500\n",
      " - 2s - loss: 0.1625 - accuracy: 0.9630\n",
      "Epoch 325/500\n",
      " - 2s - loss: 0.1633 - accuracy: 0.9620\n",
      "Epoch 326/500\n",
      " - 2s - loss: 0.1657 - accuracy: 0.9620\n",
      "Epoch 327/500\n",
      " - 2s - loss: 0.1685 - accuracy: 0.9540\n",
      "Epoch 328/500\n",
      " - 2s - loss: 0.1684 - accuracy: 0.9600\n",
      "Epoch 329/500\n",
      " - 2s - loss: 0.1716 - accuracy: 0.9520\n",
      "Epoch 330/500\n",
      " - 2s - loss: 0.1666 - accuracy: 0.9540\n",
      "Epoch 331/500\n",
      " - 2s - loss: 0.1685 - accuracy: 0.9510\n",
      "Epoch 332/500\n",
      " - 2s - loss: 0.2926 - accuracy: 0.9310\n",
      "Epoch 333/500\n",
      " - 2s - loss: 0.1598 - accuracy: 0.9600\n",
      "Epoch 334/500\n",
      " - 2s - loss: 0.1620 - accuracy: 0.9570\n",
      "Epoch 335/500\n",
      " - 2s - loss: 0.1632 - accuracy: 0.9590\n",
      "Epoch 336/500\n",
      " - 2s - loss: 0.1643 - accuracy: 0.9570\n",
      "Epoch 337/500\n",
      " - 2s - loss: 0.1629 - accuracy: 0.9550\n",
      "Epoch 338/500\n",
      " - 2s - loss: 0.1638 - accuracy: 0.9550\n",
      "Epoch 339/500\n",
      " - 2s - loss: 0.1615 - accuracy: 0.9550\n",
      "Epoch 340/500\n",
      " - 2s - loss: 0.1615 - accuracy: 0.9570\n",
      "Epoch 341/500\n",
      " - 2s - loss: 0.2704 - accuracy: 0.9300\n",
      "Epoch 342/500\n",
      " - 2s - loss: 0.1565 - accuracy: 0.9640\n",
      "Epoch 343/500\n",
      " - 2s - loss: 0.1535 - accuracy: 0.9690\n",
      "Epoch 344/500\n",
      " - 2s - loss: 0.1591 - accuracy: 0.9580\n",
      "Epoch 345/500\n",
      " - 2s - loss: 0.1576 - accuracy: 0.9680\n",
      "Epoch 346/500\n",
      " - 2s - loss: 0.1586 - accuracy: 0.9580\n",
      "Epoch 347/500\n",
      " - 2s - loss: 0.1584 - accuracy: 0.9600\n",
      "Epoch 348/500\n",
      " - 2s - loss: 0.1605 - accuracy: 0.9550\n",
      "Epoch 349/500\n",
      " - 2s - loss: 0.2161 - accuracy: 0.9460\n",
      "Epoch 350/500\n",
      " - 2s - loss: 0.2100 - accuracy: 0.9480\n",
      "Epoch 351/500\n",
      " - 2s - loss: 0.1540 - accuracy: 0.9630\n",
      "Epoch 352/500\n",
      " - 2s - loss: 0.1527 - accuracy: 0.9590\n",
      "Epoch 353/500\n",
      " - 2s - loss: 0.1541 - accuracy: 0.9590\n",
      "Epoch 354/500\n",
      " - 2s - loss: 0.1521 - accuracy: 0.9620\n",
      "Epoch 355/500\n",
      " - 2s - loss: 0.1567 - accuracy: 0.9550\n",
      "Epoch 356/500\n",
      " - 2s - loss: 0.2375 - accuracy: 0.9430\n",
      "Epoch 357/500\n",
      " - 2s - loss: 0.1492 - accuracy: 0.9630\n",
      "Epoch 358/500\n",
      " - 2s - loss: 0.1528 - accuracy: 0.9610\n",
      "Epoch 359/500\n",
      " - 2s - loss: 0.1519 - accuracy: 0.9600\n",
      "Epoch 360/500\n",
      " - 2s - loss: 0.1526 - accuracy: 0.9590\n",
      "Epoch 361/500\n",
      " - 2s - loss: 0.1526 - accuracy: 0.9590\n",
      "Epoch 362/500\n",
      " - 2s - loss: 0.1504 - accuracy: 0.9610\n",
      "Epoch 363/500\n",
      " - 2s - loss: 0.2382 - accuracy: 0.9390\n",
      "Epoch 364/500\n",
      " - 2s - loss: 0.1468 - accuracy: 0.9670\n",
      "Epoch 365/500\n",
      " - 2s - loss: 0.1486 - accuracy: 0.9610\n",
      "Epoch 366/500\n",
      " - 2s - loss: 0.1492 - accuracy: 0.9590\n",
      "Epoch 367/500\n",
      " - 2s - loss: 0.1481 - accuracy: 0.9580\n",
      "Epoch 368/500\n",
      " - 2s - loss: 0.1498 - accuracy: 0.9620\n",
      "Epoch 369/500\n",
      " - 2s - loss: 0.2099 - accuracy: 0.9470\n",
      "Epoch 370/500\n",
      " - 2s - loss: 0.1425 - accuracy: 0.9730\n",
      "Epoch 371/500\n",
      " - 2s - loss: 0.1438 - accuracy: 0.9680\n",
      "Epoch 372/500\n",
      " - 2s - loss: 0.1430 - accuracy: 0.9650\n",
      "Epoch 373/500\n",
      " - 2s - loss: 0.1493 - accuracy: 0.9630\n",
      "Epoch 374/500\n",
      " - 2s - loss: 0.2435 - accuracy: 0.9400\n",
      "Epoch 375/500\n",
      " - 2s - loss: 0.1402 - accuracy: 0.9740\n",
      "Epoch 376/500\n",
      " - 2s - loss: 0.1416 - accuracy: 0.9660\n",
      "Epoch 377/500\n",
      " - 2s - loss: 0.1454 - accuracy: 0.9630\n",
      "Epoch 378/500\n",
      " - 2s - loss: 0.1430 - accuracy: 0.9630\n",
      "Epoch 379/500\n",
      " - 2s - loss: 0.1436 - accuracy: 0.9610\n",
      "Epoch 380/500\n",
      " - 2s - loss: 0.1467 - accuracy: 0.9630\n",
      "Epoch 381/500\n",
      " - 2s - loss: 0.1424 - accuracy: 0.9540\n",
      "Epoch 382/500\n",
      " - 2s - loss: 0.1431 - accuracy: 0.9630\n",
      "Epoch 383/500\n",
      " - 2s - loss: 0.1439 - accuracy: 0.9630\n",
      "Epoch 384/500\n",
      " - 2s - loss: 0.2273 - accuracy: 0.9420\n",
      "Epoch 385/500\n",
      " - 2s - loss: 0.1446 - accuracy: 0.9680\n",
      "Epoch 386/500\n",
      " - 2s - loss: 0.1397 - accuracy: 0.9660\n",
      "Epoch 387/500\n",
      " - 2s - loss: 0.1378 - accuracy: 0.9710\n",
      "Epoch 388/500\n",
      " - 2s - loss: 0.1392 - accuracy: 0.9600\n",
      "Epoch 389/500\n",
      " - 2s - loss: 0.1413 - accuracy: 0.9660\n",
      "Epoch 390/500\n",
      " - 2s - loss: 0.1404 - accuracy: 0.9670\n",
      "Epoch 391/500\n",
      " - 2s - loss: 0.1376 - accuracy: 0.9670\n",
      "Epoch 392/500\n",
      " - 2s - loss: 0.2046 - accuracy: 0.9450\n",
      "Epoch 393/500\n",
      " - 2s - loss: 0.1562 - accuracy: 0.9710\n",
      "Epoch 394/500\n",
      " - 2s - loss: 0.1331 - accuracy: 0.9690\n",
      "Epoch 395/500\n",
      " - 2s - loss: 0.1350 - accuracy: 0.9670\n",
      "Epoch 396/500\n",
      " - 2s - loss: 0.1394 - accuracy: 0.9610\n",
      "Epoch 397/500\n",
      " - 2s - loss: 0.1362 - accuracy: 0.9670\n",
      "Epoch 398/500\n",
      " - 2s - loss: 0.1386 - accuracy: 0.9610\n",
      "Epoch 399/500\n",
      " - 2s - loss: 0.1375 - accuracy: 0.9710\n",
      "Epoch 400/500\n",
      " - 2s - loss: 0.1356 - accuracy: 0.9620\n",
      "Epoch 401/500\n",
      " - 2s - loss: 0.1349 - accuracy: 0.9660\n",
      "Epoch 402/500\n",
      " - 2s - loss: 0.2429 - accuracy: 0.9400\n",
      "Epoch 403/500\n",
      " - 2s - loss: 0.1293 - accuracy: 0.9700\n",
      "Epoch 404/500\n",
      " - 2s - loss: 0.1298 - accuracy: 0.9760\n",
      "Epoch 405/500\n",
      " - 2s - loss: 0.1305 - accuracy: 0.9670\n",
      "Epoch 406/500\n",
      " - 2s - loss: 0.1317 - accuracy: 0.9700\n",
      "Epoch 407/500\n",
      " - 2s - loss: 0.2160 - accuracy: 0.9500\n",
      "Epoch 408/500\n",
      " - 2s - loss: 0.1316 - accuracy: 0.9690\n",
      "Epoch 409/500\n",
      " - 2s - loss: 0.1305 - accuracy: 0.9670\n",
      "Epoch 410/500\n",
      " - 2s - loss: 0.1306 - accuracy: 0.9680\n",
      "Epoch 411/500\n",
      " - 2s - loss: 0.1293 - accuracy: 0.9690\n",
      "Epoch 412/500\n",
      " - 2s - loss: 0.1312 - accuracy: 0.9640\n",
      "Epoch 413/500\n",
      " - 2s - loss: 0.1629 - accuracy: 0.9660\n",
      "Epoch 414/500\n",
      " - 2s - loss: 0.1296 - accuracy: 0.9640\n",
      "Epoch 415/500\n",
      " - 2s - loss: 0.1298 - accuracy: 0.9680\n",
      "Epoch 416/500\n",
      " - 2s - loss: 0.1860 - accuracy: 0.9590\n",
      "Epoch 417/500\n",
      " - 2s - loss: 0.1562 - accuracy: 0.9680\n",
      "Epoch 418/500\n",
      " - 2s - loss: 0.1258 - accuracy: 0.9760\n",
      "Epoch 419/500\n",
      " - 2s - loss: 0.1274 - accuracy: 0.9730\n",
      "Epoch 420/500\n",
      " - 2s - loss: 0.1256 - accuracy: 0.9680\n",
      "Epoch 421/500\n",
      " - 2s - loss: 0.1281 - accuracy: 0.9690\n",
      "Epoch 422/500\n",
      " - 2s - loss: 0.1257 - accuracy: 0.9720\n",
      "Epoch 423/500\n",
      " - 2s - loss: 0.1281 - accuracy: 0.9650\n",
      "Epoch 424/500\n",
      " - 2s - loss: 0.1272 - accuracy: 0.9640\n",
      "Epoch 425/500\n",
      " - 2s - loss: 0.2145 - accuracy: 0.9500\n",
      "Epoch 426/500\n",
      " - 2s - loss: 0.1236 - accuracy: 0.9720\n",
      "Epoch 427/500\n",
      " - 2s - loss: 0.1217 - accuracy: 0.9800\n",
      "Epoch 428/500\n",
      " - 2s - loss: 0.1226 - accuracy: 0.9720\n",
      "Epoch 429/500\n",
      " - 2s - loss: 0.1235 - accuracy: 0.9710\n",
      "Epoch 430/500\n",
      " - 2s - loss: 0.1236 - accuracy: 0.9660\n",
      "Epoch 431/500\n",
      " - 2s - loss: 0.1241 - accuracy: 0.9680\n",
      "Epoch 432/500\n",
      " - 2s - loss: 0.1249 - accuracy: 0.9670\n",
      "Epoch 433/500\n",
      " - 2s - loss: 0.1225 - accuracy: 0.9680\n",
      "Epoch 434/500\n",
      " - 2s - loss: 0.2022 - accuracy: 0.9550\n",
      "Epoch 435/500\n",
      " - 2s - loss: 0.1182 - accuracy: 0.9770\n",
      "Epoch 436/500\n",
      " - 2s - loss: 0.1197 - accuracy: 0.9700\n",
      "Epoch 437/500\n",
      " - 2s - loss: 0.1195 - accuracy: 0.9760\n",
      "Epoch 438/500\n",
      " - 2s - loss: 0.1192 - accuracy: 0.9720\n",
      "Epoch 439/500\n",
      " - 2s - loss: 0.1226 - accuracy: 0.9720\n",
      "Epoch 440/500\n",
      " - 2s - loss: 0.1196 - accuracy: 0.9730\n",
      "Epoch 441/500\n",
      " - 2s - loss: 0.1241 - accuracy: 0.9690\n",
      "Epoch 442/500\n",
      " - 2s - loss: 0.1218 - accuracy: 0.9680\n",
      "Epoch 443/500\n",
      " - 2s - loss: 0.1196 - accuracy: 0.9740\n",
      "Epoch 444/500\n",
      " - 2s - loss: 0.2096 - accuracy: 0.9570\n",
      "Epoch 445/500\n",
      " - 2s - loss: 0.1145 - accuracy: 0.9790\n",
      "Epoch 446/500\n",
      " - 2s - loss: 0.1153 - accuracy: 0.9750\n",
      "Epoch 447/500\n",
      " - 2s - loss: 0.1156 - accuracy: 0.9720\n",
      "Epoch 448/500\n",
      " - 2s - loss: 0.1199 - accuracy: 0.9670\n",
      "Epoch 449/500\n",
      " - 2s - loss: 0.1170 - accuracy: 0.9700\n",
      "Epoch 450/500\n",
      " - 2s - loss: 0.1178 - accuracy: 0.9710\n",
      "Epoch 451/500\n",
      " - 2s - loss: 0.1190 - accuracy: 0.9720\n",
      "Epoch 452/500\n",
      " - 2s - loss: 0.1173 - accuracy: 0.9770\n",
      "Epoch 453/500\n",
      " - 2s - loss: 0.1677 - accuracy: 0.9520\n",
      "Epoch 454/500\n",
      " - 2s - loss: 0.1716 - accuracy: 0.9710\n",
      "Epoch 455/500\n",
      " - 2s - loss: 0.1126 - accuracy: 0.9790\n",
      "Epoch 456/500\n",
      " - 2s - loss: 0.1139 - accuracy: 0.9740\n",
      "Epoch 457/500\n",
      " - 2s - loss: 0.1121 - accuracy: 0.9750\n",
      "Epoch 458/500\n",
      " - 2s - loss: 0.1160 - accuracy: 0.9670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 459/500\n",
      " - 2s - loss: 0.1147 - accuracy: 0.9760\n",
      "Epoch 460/500\n",
      " - 2s - loss: 0.1150 - accuracy: 0.9750\n",
      "Epoch 461/500\n",
      " - 2s - loss: 0.1151 - accuracy: 0.9740\n",
      "Epoch 462/500\n",
      " - 2s - loss: 0.2599 - accuracy: 0.9460\n",
      "Epoch 463/500\n",
      " - 2s - loss: 0.1094 - accuracy: 0.9760\n",
      "Epoch 464/500\n",
      " - 2s - loss: 0.1104 - accuracy: 0.9780\n",
      "Epoch 465/500\n",
      " - 2s - loss: 0.1098 - accuracy: 0.9770\n",
      "Epoch 466/500\n",
      " - 2s - loss: 0.1115 - accuracy: 0.9730\n",
      "Epoch 467/500\n",
      " - 2s - loss: 0.1112 - accuracy: 0.9740\n",
      "Epoch 468/500\n",
      " - 2s - loss: 0.1114 - accuracy: 0.9700\n",
      "Epoch 469/500\n",
      " - 2s - loss: 0.1135 - accuracy: 0.9710\n",
      "Epoch 470/500\n",
      " - 2s - loss: 0.1295 - accuracy: 0.9770\n",
      "Epoch 471/500\n",
      " - 2s - loss: 0.1096 - accuracy: 0.9720\n",
      "Epoch 472/500\n",
      " - 2s - loss: 0.1095 - accuracy: 0.9710\n",
      "Epoch 473/500\n",
      " - 2s - loss: 0.1121 - accuracy: 0.9770\n",
      "Epoch 474/500\n",
      " - 2s - loss: 0.1106 - accuracy: 0.9710\n",
      "Epoch 475/500\n",
      " - 2s - loss: 0.1081 - accuracy: 0.9780\n",
      "Epoch 476/500\n",
      " - 2s - loss: 0.2026 - accuracy: 0.9580\n",
      "Epoch 477/500\n",
      " - 2s - loss: 0.1369 - accuracy: 0.9720\n",
      "Epoch 478/500\n",
      " - 2s - loss: 0.1061 - accuracy: 0.9780\n",
      "Epoch 479/500\n",
      " - 2s - loss: 0.1077 - accuracy: 0.9780\n",
      "Epoch 480/500\n",
      " - 2s - loss: 0.1072 - accuracy: 0.9830\n",
      "Epoch 481/500\n",
      " - 2s - loss: 0.1059 - accuracy: 0.9770\n",
      "Epoch 482/500\n",
      " - 2s - loss: 0.1085 - accuracy: 0.9710\n",
      "Epoch 483/500\n",
      " - 2s - loss: 0.1084 - accuracy: 0.9720\n",
      "Epoch 484/500\n",
      " - 2s - loss: 0.1060 - accuracy: 0.9780\n",
      "Epoch 485/500\n",
      " - 2s - loss: 0.1066 - accuracy: 0.9780\n",
      "Epoch 486/500\n",
      " - 2s - loss: 0.1058 - accuracy: 0.9780\n",
      "Epoch 487/500\n",
      " - 2s - loss: 0.1638 - accuracy: 0.9620\n",
      "Epoch 488/500\n",
      " - 2s - loss: 0.1030 - accuracy: 0.9780\n",
      "Epoch 489/500\n",
      " - 2s - loss: 0.1034 - accuracy: 0.9790\n",
      "Epoch 490/500\n",
      " - 2s - loss: 0.1049 - accuracy: 0.9810\n",
      "Epoch 491/500\n",
      " - 2s - loss: 0.1611 - accuracy: 0.9630\n",
      "Epoch 492/500\n",
      " - 2s - loss: 0.1054 - accuracy: 0.9850\n",
      "Epoch 493/500\n",
      " - 2s - loss: 0.1018 - accuracy: 0.9790\n",
      "Epoch 494/500\n",
      " - 2s - loss: 0.1016 - accuracy: 0.9810\n",
      "Epoch 495/500\n",
      " - 2s - loss: 0.1035 - accuracy: 0.9800\n",
      "Epoch 496/500\n",
      " - 2s - loss: 0.1021 - accuracy: 0.9770\n",
      "Epoch 497/500\n",
      " - 2s - loss: 0.1025 - accuracy: 0.9790\n",
      "Epoch 498/500\n",
      " - 2s - loss: 0.1016 - accuracy: 0.9800\n",
      "Epoch 499/500\n",
      " - 2s - loss: 0.1035 - accuracy: 0.9750\n",
      "Epoch 500/500\n",
      " - 2s - loss: 0.1028 - accuracy: 0.9760\n",
      "Model Accuracy: 98.10%\n",
      "['T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['V', 'W', 'X', 'Y'] -> Z\n",
      "['A', 'B', 'C', 'D'] -> E\n",
      "['C'] -> D\n",
      "['K', 'L', 'M', 'N'] -> O\n",
      "['B'] -> C\n",
      "['C', 'D', 'E', 'F', 'G'] -> H\n",
      "['Q', 'R'] -> S\n",
      "['T', 'U', 'V', 'W', 'X'] -> Y\n",
      "['D', 'E', 'F', 'G', 'H'] -> I\n",
      "['B', 'C', 'D', 'E', 'F'] -> G\n",
      "['C', 'D', 'E', 'F'] -> G\n",
      "['C'] -> D\n",
      "['K', 'L', 'M'] -> N\n",
      "['B', 'C', 'D', 'E'] -> F\n",
      "['N', 'O'] -> P\n",
      "['P'] -> Q\n",
      "['W'] -> X\n",
      "['V', 'W', 'X'] -> Y\n",
      "['C'] -> D\n"
     ]
    }
   ],
   "source": [
    "# LSTM with Variable Length Input Sequences to One Character Output\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "num_inputs = 1000\n",
    "max_len = 5\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(num_inputs):\n",
    "\tstart = numpy.random.randint(len(alphabet)-2)\n",
    "\tend = numpy.random.randint(start, min(start+max_len,len(alphabet)-1))\n",
    "\tsequence_in = alphabet[start:end+1]\n",
    "\tsequence_out = alphabet[end + 1]\n",
    "\tdataX.append([char_to_int[char] for char in sequence_in])\n",
    "\tdataY.append(char_to_int[sequence_out])\n",
    "\tprint(sequence_in, '->', sequence_out)\n",
    "# convert list of lists to array and pad sequences if needed\n",
    "# 输入序列的长度在1和maxlen之间变化，因此需要zero padding(零填充)。\n",
    "# 使用了left-hand-side (prefix) padding和 keras自带的pad_sequences()函数。\n",
    "X = pad_sequences(dataX, maxlen=max_len, dtype='float32')\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(X, (X.shape[0], max_len, 1))\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "# create and fit the model\n",
    "batch_size = 1\n",
    "model = Sequential()\n",
    "model.add(LSTM(32, input_shape=(X.shape[1], 1)))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=500, batch_size=batch_size, verbose=2)\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "# demonstrate some model predictions\n",
    "for i in range(20):\n",
    "\tpattern_index = numpy.random.randint(len(dataX))\n",
    "\tpattern = dataX[pattern_index]\n",
    "\tx = pad_sequences([pattern], maxlen=max_len, dtype='float32')\n",
    "\tx = numpy.reshape(x, (1, max_len, 1))\n",
    "\tx = x / float(len(alphabet))\n",
    "\tprediction = model.predict(x, verbose=0)\n",
    "\tindex = numpy.argmax(prediction)\n",
    "\tresult = int_to_char[index]\n",
    "\tseq_in = [int_to_char[value] for value in pattern]\n",
    "\tprint(seq_in, \"->\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，尽管这个模型没有从随机生成的子序列中完美地学习字母表，但它做得很好。该模型没有进行调整，可能需要更多的训练或更大的网络，或者两者都需要(为读者提供一个练习)。\n",
    "\n",
    "这是一个很好的自然扩展，对于“每个批处理中的所有顺序输入示例”，都可以在上面学到，它可以处理特殊的查询，但是这一次是任意的序列长度(最多的是最大长度)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小结\n",
    "* 如何开发一个简单的LSTM网络，一个字符到一个字符的预测。\n",
    "* 如何配置一个简单的LSTM，以在一个示例中跨时间步骤学习一个序列。\n",
    "* 如何配置LSTM来通过手动管理状态来学习跨示例的序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 在IMDB数据上应用LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 52s 3ms/step - loss: 0.5698 - acc: 0.7120 - val_loss: 0.4507 - val_acc: 0.8058\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.4047 - acc: 0.8287 - val_loss: 0.4112 - val_acc: 0.8212\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 52s 3ms/step - loss: 0.3661 - acc: 0.8433 - val_loss: 0.3523 - val_acc: 0.8536\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 52s 3ms/step - loss: 0.3502 - acc: 0.8547 - val_loss: 0.3497 - val_acc: 0.8564\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 51s 3ms/step - loss: 0.3394 - acc: 0.8619 - val_loss: 0.3617 - val_acc: 0.8490\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features, 32))  #将每个词原来是整数，转换为 32位的向量\n",
    "model2.add(LSTM(32))                       #LSTM\n",
    "model2.add(Dense(1, activation='sigmoid'))  #最后一层用于分类\n",
    "\n",
    "model2.compile(optimizer='rmsprop',\n",
    "             loss='binary_crossentropy',\n",
    "             metrics=['acc'])\n",
    "history2 = model2.fit(input_train, y_train,\n",
    "                   epochs=5,\n",
    "                   batch_size=128,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history2.history['acc']\n",
    "val_acc = history2.history['val_acc']\n",
    "loss = history2.history['loss']\n",
    "val_loss = history2.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 39s 2ms/step\n",
      "[0.3530502226638794, 0.8510800004005432]\n"
     ]
    }
   ],
   "source": [
    "#验证模型\n",
    "#Returns the loss value & metrics values for the model in test mode\n",
    "\n",
    "results2 = model2.evaluate(input_test, y_test)\n",
    "print(results2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.IMDB电影评论分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM由于其设计的特点，非常适合用于对时序数据的建模，如文本数据。将词的表示组合成句子的表示，可以采用相加的方法，即将所有词的表示进行加和，或者取平均等方法，但是这些方法没有考虑到词语在句子中前后顺序。如句子“我不觉得他好”。“不”字是对后面“好”的否定，即该句子的情感极性是贬义。使用LSTM模型可以更好的捕捉到较长距离的依赖关系。因为LSTM通过训练过程可以学到记忆哪些信息和遗忘哪些信息。\n",
    "\n",
    "基于Keras框架，采用LSTM实现文本分类。文本采用imdb影评分类语料，共25,000条影评，label标记为正面/负面两种评价。影评已被预处理为词下标构成的序列。方便起见，单词的下标基于它在数据集中出现的频率标定，例如整数3所编码的词为数据集中第3常出现的词。这样的组织方法使得用户可以快速完成诸如“只考虑最常出现的10,000个词，但不考虑最常出现的20个词”这样的操作。词向量没有采用预训练好的向量，训练中生成，采用的网络结构如图所示：\n",
    "\n",
    "![image](images/14.png)\n",
    "\n",
    "具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "[1, 14, 22, 16, 43]\n",
      "(25000,)\n",
      "1\n",
      "(25000, 20)\n",
      "[ 65  16  38   2  88  12  16 283   5  16   2 113 103  32  15  16   2  19\n",
      " 178  32]\n",
      "(25000, 2)\n",
      "[0. 1.]\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 20, 8)             8000      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 20, 256)           271360    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 20, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 805,186\n",
      "Trainable params: 805,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/20\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.6606 - acc: 0.5933 - val_loss: 0.5991 - val_acc: 0.6843\n",
      "Epoch 2/20\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.5493 - acc: 0.7190 - val_loss: 0.5263 - val_acc: 0.7338\n",
      "Epoch 3/20\n",
      "25000/25000 [==============================] - 38s 2ms/step - loss: 0.5106 - acc: 0.7458 - val_loss: 0.5208 - val_acc: 0.7374\n",
      "Epoch 4/20\n",
      "25000/25000 [==============================] - 41s 2ms/step - loss: 0.5016 - acc: 0.7529 - val_loss: 0.5185 - val_acc: 0.7357\n",
      "Epoch 5/20\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.4951 - acc: 0.7559 - val_loss: 0.5163 - val_acc: 0.7380\n",
      "Epoch 6/20\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.4942 - acc: 0.7571 - val_loss: 0.5252 - val_acc: 0.7305\n",
      "Epoch 7/20\n",
      "25000/25000 [==============================] - 39s 2ms/step - loss: 0.4933 - acc: 0.7563 - val_loss: 0.5194 - val_acc: 0.7388\n",
      "Epoch 8/20\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4925 - acc: 0.7579 - val_loss: 0.5176 - val_acc: 0.7390\n",
      "Epoch 9/20\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4898 - acc: 0.7555 - val_loss: 0.5185 - val_acc: 0.7389\n",
      "Epoch 10/20\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4859 - acc: 0.7606 - val_loss: 0.5213 - val_acc: 0.7305\n",
      "Epoch 11/20\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4866 - acc: 0.7583 - val_loss: 0.5244 - val_acc: 0.7333\n",
      "Epoch 12/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4818 - acc: 0.7632 - val_loss: 0.5186 - val_acc: 0.7349\n",
      "Epoch 13/20\n",
      "25000/25000 [==============================] - 44s 2ms/step - loss: 0.4778 - acc: 0.7645 - val_loss: 0.5224 - val_acc: 0.7348\n",
      "Epoch 14/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4759 - acc: 0.7667 - val_loss: 0.5221 - val_acc: 0.7365\n",
      "Epoch 15/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4719 - acc: 0.7699 - val_loss: 0.5280 - val_acc: 0.7336\n",
      "Epoch 16/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4671 - acc: 0.7722 - val_loss: 0.5180 - val_acc: 0.7343\n",
      "Epoch 17/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4635 - acc: 0.7716 - val_loss: 0.5259 - val_acc: 0.7362\n",
      "Epoch 18/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4597 - acc: 0.7742 - val_loss: 0.5302 - val_acc: 0.7294\n",
      "Epoch 19/20\n",
      "25000/25000 [==============================] - 46s 2ms/step - loss: 0.4558 - acc: 0.7756 - val_loss: 0.5250 - val_acc: 0.7356\n",
      "Epoch 20/20\n",
      "25000/25000 [==============================] - 45s 2ms/step - loss: 0.4501 - acc: 0.7796 - val_loss: 0.5264 - val_acc: 0.7354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1603c338f60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### load packages ####################\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "###################### load data ####################\n",
    "######### 只考虑最常见的1000个词 ########\n",
    "num_words = 1000\n",
    "\n",
    "######### 导入数据 #########\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=num_words)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0][:5])\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "###################### preprocess data ####################\n",
    "######## 句子长度最长设置为20 ########\n",
    "max_len = 20\n",
    "\n",
    "######## 对文本进行填充，将文本转成相同长度 ########\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0])\n",
    "\n",
    "######## 对label做one-hot处理 ########\n",
    "num_class = 2\n",
    "y_train = to_categorical(y_train, num_class)\n",
    "y_test = to_categorical(y_test, num_class)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_train[0])\n",
    "\n",
    "\n",
    "###################### build network ####################\n",
    "######## word dim 词向量维度 ########\n",
    "word_dim = 8\n",
    "\n",
    "######## network structure ########\n",
    "model = Sequential()\n",
    "\n",
    "#### Embedding层 ####\n",
    "model.add(Embedding(input_dim=1000, output_dim=word_dim, input_length=max_len))\n",
    "\n",
    "#### 两层LSTM，第一层，设置return_sequences参数为True ####\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "\n",
    "#### dropout ####\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#### 两层LSTM，第二层，设置return_sequences参数为False ####\n",
    "model.add(LSTM(256, return_sequences=False))\n",
    "\n",
    "#### dropout ####\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#### 输出层 ####\n",
    "model.add(Dense(num_class, activation='softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "######## optimization and train ########\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.fit(x_train, y_train, batch_size=512, epochs=20, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Any Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
